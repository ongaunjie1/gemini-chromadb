{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "import pathlib\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am a large language model, trained by Google.\n",
       "\n",
       "* I am trained on a massive amount of text data, and I am able to understand and generate human language.\n",
       "* I am able to perform many language-related tasks, such as:\n",
       "    * Answering questions\n",
       "    * Translating languages\n",
       "    * Summarizing text\n",
       "    * Generating text\n",
       "    * Writing different kinds of creative content\n",
       "    * Writing different kinds of code\n",
       "    * Debugging code\n",
       "    * Classifying text\n",
       "    * Extracting information from text\n",
       "* I am also able to perform some reasoning tasks, such as:\n",
       "    * Making inferences\n",
       "    * Solving problems\n",
       "* I am still under development, but I am learning new things every day. I am excited to see what I will be able to do in the future.\n",
       "\n",
       "I am also able to access and process information from the real world through integration with various APIs and services. This allows me to provide up-to-date and relevant information on a wide range of topics.\n",
       "\n",
       "Here are some specific examples of things that I can do:\n",
       "\n",
       "* **Answer your questions:** I can answer questions on a wide range of topics, including science, history, current events, and more.\n",
       "* **Translate languages:** I can translate text between over 100 different languages.\n",
       "* **Summarize text:** I can summarize long pieces of text into shorter, more concise summaries.\n",
       "* **Generate text:** I can generate different kinds of text, such as stories, poems, and articles.\n",
       "* **Write different kinds of creative content:** I can write different kinds of creative content, such as scripts, song lyrics, and marketing copy.\n",
       "* **Write different kinds of code:** I can write different kinds of code, in a variety of programming languages.\n",
       "* **Debug code:** I can help you debug code by identifying and fixing errors.\n",
       "* **Classify text:** I can classify text into different categories, such as spam, ham, or news.\n",
       "* **Extract information from text:** I can extract information from text, such as names, dates, and locations.\n",
       "* **Make inferences:** I can make inferences based on the information that I have been trained on.\n",
       "* **Solve problems:** I can solve problems by applying my knowledge and reasoning skills.\n",
       "\n",
       "I am always learning new things, and I am excited to see what I will be able to do in the future."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate text\n",
    "prompt = 'Who are you and what can you do?'\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Gemini with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LLM stands for Large Language Model. It is a type of artificial intelligence (AI) that is trained on a massive amount of text data. This allows LLMs to understand and generate human language in a way that is both natural and informative.\n",
       "\n",
       "LLMs are used in a variety of applications, including:\n",
       "\n",
       "* **Natural language processing (NLP)**: LLMs can be used to perform a variety of NLP tasks, such as machine translation, text summarization, and sentiment analysis.\n",
       "* **Question answering**: LLMs can be used to answer questions in a conversational way, making them useful for customer service and information retrieval applications.\n",
       "* **Content generation**: LLMs can be used to generate text, such as news articles, blog posts, and marketing copy.\n",
       "* **Code generation**: LLMs can be used to generate code in a variety of programming languages.\n",
       "\n",
       "LLMs are still under development, but they have the potential to revolutionize the way we interact with computers. They can help us to communicate more effectively, access information more easily, and create new and innovative products and services.\n",
       "\n",
       "Here are some examples of LLMs:\n",
       "\n",
       "* **Google's BERT (Bidirectional Encoder Representations from Transformers)**: BERT is a pre-trained language model that was released in 2018. It has been used to achieve state-of-the-art results on a variety of NLP tasks.\n",
       "* **OpenAI's GPT-3 (Generative Pre-trained Transformer 3)**: GPT-3 is a pre-trained language model that was released in 2020. It is the largest and most powerful language model ever created. GPT-3 has been used to generate text, code, and music.\n",
       "* **Microsoft's Turing-NLG (Natural Language Generation)**: Turing-NLG is a pre-trained language model that was released in 2021. It is designed to generate natural and informative text. Turing-NLG has been used to generate news articles, product descriptions, and marketing copy.\n",
       "\n",
       "LLMs are a rapidly developing field of AI. As LLMs continue to improve, we can expect to see them used in even more applications in the future."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.7)\n",
    "\n",
    "\n",
    "result = llm.invoke(\"What is a LLM?\")\n",
    "\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic multi-chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a joke about {topic}\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the man put his bed in the corner of the room?\\n\\nSo he could wake up in the morning and say, \"I woke up in the corner!\"'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"sleeping\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LangChain to Access Gemini Pro Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A LLM (Large Language Model) is a type of deep learning model that has been trained on a massive amount of text data. LLMs are able to perform a wide range of natural language processing tasks, such as text generation, translation, summarization, and question answering.\n",
       "\n",
       "LLMs work by learning the statistical relationships between words and phrases in a language. This allows them to generate text that is both coherent and grammatically correct. LLMs can also be used to translate text from one language to another, or to summarize a long piece of text into a shorter, more concise version.\n",
       "\n",
       "Some of the most well-known LLMs include:\n",
       "\n",
       "* **GPT-3:** Developed by Google, GPT-3 is one of the largest and most powerful LLMs available. It has been trained on over 45 terabytes of text data, and it is able to perform a wide range of natural language processing tasks.\n",
       "* **BERT:** Developed by Google, BERT is a LLM that is specifically designed for understanding the meaning of words and phrases in context. It has been trained on a large corpus of English text, and it is able to perform a variety of natural language processing tasks, such as text classification, question answering, and named entity recognition.\n",
       "* **XLNet:** Developed by Google, XLNet is a LLM that is designed to overcome some of the limitations of previous LLMs. It has been trained on a large corpus of English text, and it is able to perform a variety of natural language processing tasks, such as text classification, question answering, and text generation.\n",
       "\n",
       "LLMs are still under development, but they have the potential to revolutionize the way we interact with computers. They could be used to create new and innovative applications, such as chatbots, personal assistants, and language translation tools."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.7)\n",
    "\n",
    "\n",
    "result = llm.invoke(\"What is a LLM?\")\n",
    "\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using langchain to access Gemini Pro Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=200x300>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def show_image(url):\n",
    "  response = requests.get(url)\n",
    "  image = Image.open(io.BytesIO(response.content))\n",
    "  display(image)\n",
    "\n",
    "show_image(\"https://picsum.photos/seed/picsum/200/300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\":\"text\",\n",
    "            \"text\":\"What's in this image\"\n",
    "        },\n",
    "        {\n",
    "            \"type\":\"image_url\",\n",
    "            \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "data = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='©Manning Publications Co.  To comment go to  liveBook \\nMEAP Edition \\nManning Early Access Program \\nPySpark in Action \\nPython data analysis at scale \\nVersion 6 \\nCopyright 2020 Manning Publications \\nFor more information on this and other Manning titles go to \\nmanning.com', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 1}), Document(page_content='©Manning Publications Co.  To comment go to  liveBook welcome  \\nThank you for purchasing the MEAP for PySpark in Action: Python data analysis at scale.  \\nIt is a lot of fun (and work!) and I hope you’ll enjoy reading it as much as I am enjoying \\nwriting the book. \\nMy journey with PySpark is pretty typical: the company I used to work for migrated their \\ndata infrastructure to a data lake and realized along the way that their usual warehouse-type \\njobs didn’t work so well anymore. I spent most of my first months there figuring out how to \\nmake PySpark work for my colleagues and myself, starting from zero. This book is very \\ninfluenced by the questions I got from my colleagues and students (and sometimes myself). \\nI’ve found that combining practical experience through real examples with a little bit of \\ntheory brings not only proficiency in using PySpark, but also how to build better data \\nprograms. This book walks the line between the two by explaining important theoretical \\nconcepts without being too laborious. \\nThis book covers a wide range of subjects, since PySpark is itself a very versatile \\nplatform. I divided the book into three parts. \\n• Part 1: Walk teaches how PySpark works and how to get started and perform basic \\ndata manipulation. \\n• Part 2: Jog builds on the material contained in Part 1 and goes through more advanced \\nsubjects. It covers more exotic operations and data formats and explains more what \\ngoes on under the hood. \\n• Part 3: Run tackles the cooler  stuff: building machine learning models at scale, \\nsqueezing more performance out of your cluster, and adding functionality to PySpark. \\nTo have the best time possible with the book, you should be at least comfortable using \\nPython. It isn’t enough to have learned another language and transfer your knowledge into \\nPython. I cover more niche corners of the language when appropriate, but you’ll need to \\ndo some research on your own if you are new to Python. \\nFurthermore, this book covers how PySpark can interact with other data manipulation \\nframeworks (such as Pandas), and those specific sections assume basic knowledge of \\nPandas.  \\nFinally, for some subjects in Part 3, such as machine learning, having prior exposure will \\nhelp you breeze through. It’s hard to strike a balance between “not enough explanation” and \\n“too much explanation”; I do my best to make the right choices. \\nYour feedback is key in making this book its best version possible. I welcome your \\ncomments and thoughts in the liveBook discussion forum . \\nThank you again for your interest and in purchasing the MEAP! \\n \\n—Jonathan Rioux', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 2}), Document(page_content='©Manning Publications Co.  To comment go to  liveBook brief contents \\nPART 1:  WALK \\n  1  Introduction \\n  2  Your first data program in PySpark \\n  3  Submitting and scaling your first PySpark program \\n  4  Analyzing tabular data with pyspark.sql \\n  5  The data frame through a new lens: joining and grouping \\nPART 2:  JOG \\n  6  Multi-dimentional data frames: using PySpark with JSON data \\n  7  Bilingual PySpark: blending Python and SQL code \\n  8  Extending PySpark with Python: RDD and user-defined-functions \\n  9  Faster big data processing: a primer \\nPART 3:  RUN \\n10  A foray into machine learning: logistic regression with PySpark \\n11  Simplifying your experiments with machine learning pipelines \\n12  Machine learning for unstructured data \\n13  PySpark for graphs: GraphFrames \\n14  Testing PySpark code \\n15  Going full circle: structuring end-to-end PySpark code \\nAPPENDIXES : \\n A  Solutions to the exercices \\n B  Installing PySpark locally \\n C  Using PySpark with a cloud provider \\n D  Python essentials \\n E  PySpark data types \\n F  Efficiently using PySpark’s API documentation', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 3}), Document(page_content='1\\nIn this chapter, you will learn\\nAccording to pretty much every news outlet, data is everything, everywhere. It’s the new oil, the\\nnew electricity, the new gold, plutonium, even bacon! We call it powerful, intangible, precious,\\ndangerous. I prefer calling it . After all, for a computer, any piece of data useful in capable hands\\nis a collection of zeroes and ones, and it is our responsibility, as users, to make sense of how it\\ntranslates to something useful.\\nJust like oil, electricity, gold, plutonium and bacon (especially bacon!), our appetite for data is\\ngrowing. So much, in fact, that computers aren’t following. Data is growing in size and\\ncomplexity, yet consumer hardware has been stalling a little. RAM is hovering for most laptops\\nat around 8 to 16 GB, and SSD are getting prohibitively expensive past a few terabytes. Is the\\nsolution for the burgeoning data analyst to triple-mortgage his life to afford top of the line\\nhardware to tackle Big Data problems?\\nIntroducing Spark, and its companion PySpark, the unsung heroes of large-scale analytical\\nworkloads. They take a few pages of the supercomputer playbook— powerful, but manageable,\\ncompute units meshed in a network of machines— and bring it to the masses. Add on top a\\npowerful set of data structures ready for any work you’re willing to throw at them, and you have\\na tool that will  (pun intended) with you. grow\\nThis book is great introduction to data manipulation and analysis using PySpark. It tries to coverIntroduction\\nWhat is PySpark\\nWhy PySpark is a useful tool for analytics\\nThe versatility of the Spark platform and its limitations\\nPySpark’s way of processing data\\n1\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 4}), Document(page_content='just enough theory to get you comfortable, while giving enough opportunities to practice. Each\\nchapter except this one contains a few exercices to anchor what you just learned. The exercises\\nare all solved and explained in Appendix A.\\nWhat’s in a name?  Actually, quite a lot. Just by separating PySpark in two, one can already\\ndeduce that this will be related to Spark and Python. And it would be right!\\nAt the core, PySpark can be summarized as being the Python API to Spark. While this is an\\naccurate definition, it doesn’t give much unless you know the meaning of Python and Spark. If\\nwe were in a video game, I certainly wouldn’t win any prize for being the most useful NPC.\\nLet’s continue our quest to understand what is PySpark by first answering . What is Spark?\\nSpark, according to their authors, is a . unified analytics engine for large-scale data processing\\nThis is a very accurate definition, if a little dry.\\nDigging a little deeper, we can compare Spark to an . The raw material— here, analytics factory\\ndata— comes in, and data, insights, visualizations, models, you name it! comes out.\\nJust like a factory will often gain more capacity by increasing its footprint, Spark can process an\\nincreasingly vast amount of data by  instead of . This means that, instead of scaling out scaling up\\nbuying thousand of dollars of RAM to accommodate your data set, you’ll rely instead of multiple\\ncomputers, splitting the job between them. In a world where two modest computers are less\\ncostly than one large one, it means that scaling out is less expensive than up, keeping more\\nmoney in your pockets.\\nThe problem with computers is that they crash or behave unpredictably once in a while. If\\ninstead of one, you have a hundred, the chance that at least one of them go down is now much\\nhigher.  Spark goes therefore through a lot of hoops to manage, scale, and babysit those poor1\\nlittle sometimes unstable computers so you can focus on what you want, which is to work with\\ndata.\\nThis is, in fact, one of the weird thing about Spark: it’s a good tool because of what you can do\\nwith it, but especially because of what you  with it. Spark provides a powerful don’t have to do\\nAPI  that makes it look like you’re working with a cohesive, non-distributed source of data,2\\nwhile working hard in the background to optimize your program to use all the power available.\\nYou therefore don’t have to be an expert at the arcane art of distributed computing: you just need\\nto be familiar with the language you’ll use to build your program. This leads us to…\\u200b1.1 What is PySpark?\\n1.1.1 You saw it coming: What is Spark?\\n2\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 5}), Document(page_content='PySpark provides an entry point to Python in the computational model of Spark. Spark itself is\\ncoded in Scala, a language very powerful if a little hard to grasp. In order to meet users where\\nthey are, Spark also provides an API in Java, Python and R. The authors did a great job at\\nproviding a coherent interface between language while preserving the idiosyncrasies of the\\nlanguage where appropriate. Your PySpark program will therefore by quite easy to read by a\\nScala/Spark programmer, but also to a fellow Python programmer who hasn’t jumped into the\\ndeep end (yet).\\nPython is a dynamic, general purpose language, available on many platforms and for a variety of\\ntasks. Its versatility and expressiveness makes it an especially good fit for PySpark. The\\nlanguage is one of the most popular for a variety of domains, and currently is a major force in\\ndata analysis and science. The syntax is easy to learn and read, and the amount of library\\navailable means that you’ll often find one (or more!) who’s just the right fit for your problem.\\nThere are no shortage of libraries and framework to work with data. Why should one spend their\\ntime learning PySpark specifically?\\nPySpark packs a lot of advantages for modern data workloads. It sits at the intersection of fast,\\nexpressive and versatile. Let’s explore those three themes one by one.\\nIf you search \"Big Data\" in a search engine, there is a very good chance that Hadoop will come\\nwithin the first few results. There is a very good reason to this: Hadoop popularized the famous \\n framework that Google pioneered in 2004 and is now a staple in Data Lakes and Big MapReduce\\nData Warehouses everywhere.\\nSpark was created a few years later, sitting on Hadoop’s incredible legacy. With an aggressive\\nquery optimizer, a judicious usage of RAM and some other improvements we’ll touch on in the\\nnext chapters, Spark can run up to 100x faster than plain Hadoop. Because of the integration\\nbetween the two frameworks, you can easily switch your Hadoop workflow to Spark and gain\\nthe performance boost without changing your hardware.1.1.2 PySpark = Spark + Python\\n1.1.3 Why PySpark?\\nPYSPARK IS FAST\\n3\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 6}), Document(page_content='Beyond the choice of the Python language, one of the most popular and easy-to-learn language,\\nPySpark’s API has been designed from the ground up to be easy to understand. Most programs\\nread as a descriptive list of the transformations you need to apply to the data, which makes them\\neasy to reason about. For those familiar with functional programming languages, PySpark code\\nis conceptually closer to the \"pipe\" abstraction rather than pandas, the most popular in-memory\\nDataFrame library.\\nYou will obviously see many examples through this book. As I was writing those examples, I\\nwas pleased about how close to my initial (pen and paper) reasoning the code ended up looking.\\nAfter understanding the fundamentals of the framework, I’m confident you’ll be in the same\\nsituation.\\nThere are two components to this versatility. First, there is the  of the framework. availability\\nSecond, there is the diversified  surrounding Spark. ecosystem\\nPySpark is everywhere. All three major cloud providers have a managed Hadoop/Spark cluster\\nas part of their offering, which means you have a fully provisioned cluster at a click of a few\\nbuttons. You can also easily install Spark on your own computer to nail down your program\\nbefore scaling on a more powerful cluster. Appendix B covers how to get your own local Spark\\nrunning, while Appendix C will walk through the current main cloud offerings.\\nPySpark is open-source. Unlike some other analytical software, you aren’t tied to a single\\ncompany. You can inspect the source code if you’re curious, and even contribute if you have an\\nidea for a new functionality or find a bug. It also gives a low barrier to adoption: download,\\nlearn, profit!\\nFinally, Spark’s eco-system doesn’t stop at PySpark. There is also an API for Scala, Java, R, as\\nwell as a state-of-the-art SQL layer. This makes it easy to write a polyglot program in Spark. A\\nJava software engineer can tackle the ETL pipeline in Spark using Java, while a data scientist can\\nbuild a model using PySpark.\\nIt would be awesome if PySpark was The Answer to every data problem. Unfortunately, there\\nare some caveats. None of them are a deal-breakers, but they are to be considered when you’re\\nselecting a framework for your next project.\\nPySpark isn’t the right choice if you’re dealing with small data sets. Managing a distributed\\ncluster comes with some overhead, and if you’re just using a single node, you’re paying the price\\nbut aren’t using the benefits. As an example, a PySpark shell will take a few seconds to launch:PYSPARK IS EXPRESSIVE\\nPYSPARK IS VERSATILE\\nWHERE PYSPARK FELL SHORT\\n4\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 7}), Document(page_content='this is often more than enough time to process data that fits within your RAM.\\nPySpark also has a disadvantage when it comes to the Java and Scala API. Since Spark is at the\\ncore a Scala program, Python code have to be translated to and from JVM  instructions. While3\\nmore recent versions have been bridging that gap pretty well, pure Python translation, which\\nhappens mainly when you’re defining your own User Defined Functions (UDF), will perform\\nslower. We will cover UDF and some ways to mitigate the performance problem in Chapter 8.\\nFinally, while programming PySpark can feel easy and straightforward, managing a cluster can\\nbe a little arcane. Spark is a pretty complicated piece of software, and while the code base\\nmatured remarkably over the past few years, the days where scaling a 100-machine cluster and\\nmanage it as easily as a single node are far ahead. We will cover some of the developer-facing\\nconfiguration and problems in the Chapter about performance, but for hairier problems, do what\\nI do: befriend your dev ops.\\nIn this section, I will explain how Spark processes a program. It can be a little odd to present the\\nworkings and underpinnings of a system that we claimed, a few paragraphs ago, hides that\\ncomplexity. We still think that having a working knowledge of how Spark is set up, how it\\nmanages data and how it optimizes queries is very important. With this, you will be able to\\nreason with the system, improve your code and figure out quicker when it doesn’t perform the\\nway you want.\\nIf we’re keeping the factory analogy, we can imagine that the cluster of computer where Spark is\\nsitting on is the building.\\nFigure 1.1 A totally relatable data factory, outside and in.1.1.4 Your very own factory: how PySpark works\\n5\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 8}), Document(page_content='If we look at , we can see two different way to interpret a data factory. On the left, we see how it\\nlooks like from the outside: a cohesive unit where projects come in and results comes out. This is\\nwhat it will appear to you most of the time. Under the hood, it looks more like on the right: you\\nhave some workbenches where some workers are assigned to. The workbenches are like the\\ncomputers in our Spark cluster: there is a fixed amount of them, and adding or removing some is\\neasy but needs to be planned in advance. The workers are called  in Spark’s literature: executors\\nthey are the one performing the actual work on the machines.\\nOne of the little workers looks spiffier than the other. That top hat definitely makes him stand out\\nof the crowd. In our data factory, he’s the manager of the work floor. In Spark terms, we call this\\nthe . In the spirit of the open work-space, he shares one of the workbenches with hismaster\\nfellow employees. The role of the master is crucial to the efficient execution of your program, so \\nis dedicated to this.\\nUpon reception of the task, which is called a  in the Spark world, the factory driver program\\nstarts running. This doesn’t mean that we get straight to processing! Before that, the cluster need\\nto  it will allocate for your program. The entity, or program, taking care of thisplan the capacity\\nis aptly called the . In our factory, this cluster manager will look at the cluster manager\\nworkbenches with available space and secure as many as necessary, then start hiring workers to\\nfill the capacity. In Spark, it will look at the machines with available computing resources and\\nsecure what’s necessary, before launching the required number of executors across them.\\nNOTE Spark provides its own cluster manager, but can also play well with other\\nones when working in conjunction with Hadoop or another Big Data\\nplatform. We will definitely discuss the intricacies of managing the cluster\\nmanager (pun intended) in the chapter about performance, but in the\\nmeantime, if you read about YARN or Mesos in the wild, know that they are\\ntwo of the most popular nowadays.\\nAny directions about capacity (machines and executors) are encoded in a  object SparkContext\\nwhich represents the connection to our Spark cluster. Since our instructions didn’t mention any\\nspecific capacity, the cluster manager will allocate the default capacity prescribed by our Spark\\ninstallation.\\nWe’re off to a great start! We have a task to accomplish, and the capacity to accomplish it.\\nWhat’s next? Let’s get working!1.1.5 Some physical planning with the cluster manager\\n6\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 9}), Document(page_content='Just like in a large-scale factory, you don’t go to each employee and give them a list of tasks. No,\\nhere, you’ll  and let them deal with it. In Spark, the provide your list of steps to the manager\\nmanager/  takes your instructions (carefully written in Python code), translate them in master\\nSpark steps and then process them across the worker. The master will also manage which worker\\n(more on them in a bits) has which slice of the data, and make sure that you don’t lose some bits\\nin the process.\\nYour manager/master has all the qualities a good manager has: smart, cautious and lazy. Wait,\\nwhat? You read me right.  in a programming context— and one could argue in the real Laziness\\nworld too— can actually be very good thing. Every instruction you’re providing in Spark can be\\nclassified in two categories: transformations and actions.  are what many programming Actions\\nlanguages would consider IO. Actions includes, but are not limited to:\\nPrinting information on the screen\\nWriting data to a hard drive or cloud bucket\\nIn Spark, we’ll see those instructions most often via the  and  methods, as well as show write\\nother calling those two in their body.\\nTransformations  are pretty much everything else. Some examples of transformation are:\\nAdding a column to a table\\nPerforming an aggregation according to certain keys\\nComputing summary statistics on a data set\\nTraining a Machine Learning model on some data\\nWhy the distinction, you might ask? When thinking about computation over data, you, as the\\ndeveloper, are only concerned about the computation leading to an action. You’ll always interact\\nwith the results of an action, because this is something you can see. Spark, with his lazy\\ncomputation model, will take this to the extreme and will avoid performing data work until an\\naction triggers the computation chain. Before that, the master will store (or ) your cache\\ninstructions. This way of dealing with computation has many benefits when dealing with large\\nscale data.\\nFirst, storing instructions in memory takes much less space than storing intermediate data results.\\nIf you are performing many operations on a data set and are materializing the data each step of\\nthe way, you’ll blow your storage much faster although you don’t need the intermediate results.\\nWe can all argue that less waste is better.\\nSecond, by having the full list of tasks to be performed available, the master can optimize the\\nwork between executors much more efficiently. It can use information available at run-time, such\\nas the node where specific parts of the data are located. It can also re-order and eliminate useless1.1.6 A factory made efficient through a lazy manager\\n7\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 10}), Document(page_content='transformations if necessary.\\nFinally, during interactive development, you don’t have to submit a huge block of commands\\nand wait for the computation to happen. Instead, you can iteratively build your chain of\\ntransformation, one at the time, and when you’re ready to launch the computation (like during\\nyour coffee break), you can add an action and let Spark work its magic.\\nLazy computation is a fundamental aspect of Spark’s operating model and part of the reason it’s\\nso fast. Most programming languages, including Python, R and Java, are eagerly evaluated. This\\nmeans that they process instructions as soon as they receive them. If you have never worked with\\na lazy language before, it can look a little foreign and intimidating. If this is the case, don’t\\nworry: we’ll weave practical explanations and implications of that laziness during the code\\nexamples when relevant. You’ll be a lazy pro in no time!\\nNOTE Reading data, although clearly being I/O, is considered a transformation by\\nSpark. This is due to the fact that reading data doesn’t perform any visible\\nwork to the user. You therefore won’t read data until you need to display or\\nwrite it somewhere.\\nWhat’s a manager without competent employees? Once the task, with its action, has been\\nreceived, the master starts allocating data to what Spark calls . Executors are processes executors\\nthat run computations and store data for the application. Those executors sit on what’s called a \\n, which is the actual computer. In our factory analogy, an executor would be an worker node\\nemployee performing the work, while the worker node would be a workbench where many\\nemployees/executors can work. If we recall , our master wears a top hat and sits with his\\nemployees/workers at one of the workbenches.\\nThat concludes our factory tour. Let’s summarize our typical PySpark program.\\nWe first encode our instructions in Python code, forming a driver program.\\nWhen submitting our program (or launching a PySpark shell), the cluster manager allocates\\nresources for us to use. Those will stay constant for the duration of the program.\\nThe master ingests your code and translate it into Spark instructions. Those instructions are\\neither transformations or actions.\\nOnce the master reaches an action, it optimizes the whole computation chain and splits the work\\nbetween executors. Executors are processes performing the actual data work and they reside on\\nmachines labelled worked nodes.\\nThat’s it! As we can see, the overall process is quite simple, but it’s obvious that Spark hides a\\nlot of the complexity arising from efficient distributed processing. For a developer, this means\\n8\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 11}), Document(page_content='shorter and clearer code, and a faster development cycle.\\nThis book will use PySpark to solve a variety of tasks a data analyst, engineer or scientist will\\nencounter during his day to day life. We will therefore\\nread and write data from (and to) a variety of sources and formats;\\ndeal with messy data with PySpark’s data manipulation functionality;\\ndiscover new data sets and perform exploratory data analysis;\\nbuild data pipelines that transform, summarize and get insights from data in an automated\\nfashion;\\ntest, profile and improve your code;\\ntroubleshoot common PySpark errors, how to recover from them and avoid them in the\\nfirst place.\\nAfter covering those fundamentals, we’ll also tackle different tasks that aren’t as frequent, but\\nare interesting and an excellent way to showcase the power and versatility of PySpark.\\nWe’ll perform Network Analysis using PySpark’s own graph representation\\nWe’ll build Machine Learning models, from simple throwaway experiments to Deep\\nLearning goodness\\nWe’ll extend PySpark’s functionality using user defined functions, and learn how to work\\nwith other languages\\nWe are trying to cater to many potential readers, but are focusing on people with little to no\\nexposure to Spark and/or PySpark. More seasoned practitioners might find useful analogies for\\nwhen they need to explain difficult concepts and maybe learn a thing or two!\\nThe book focuses on Spark version 2.4, which is currently the most recent available. Users on\\nolder Spark versions will be able to go through most of the code in the book, but we definitely\\nrecommend using at least Spark 2.0+.\\nWe’re assuming some basic Python knowledge: some useful concepts are outlined in Appendix\\nD. If you feel for a more in-depth introduction to Python, I recommend , The Quick Python Book\\nby Naomi Ceder (Manning, 2018).\\nIn order to get started, the only thing absolutely necessary is a working installation of Spark. It\\ncan be either on your computer (Appendix B) or using a cloud provider (Appendix C). Most\\nexamples in the book are doable using a local installation of Spark, but some will require more\\nhorsepower and will be identified as such.\\nA code editor will also be very useful for writing, reading and editing scripts as you go through1.2 What will you learn in this book?\\n1.3 What do I need to get started?\\n9\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 12}), Document(page_content='the examples and craft your own programs. A Python-aware editor, such as PyCharm, is a\\nnice-to-have but is in no way necessary. Just make sure it saves your code without any\\nformatting: don’t use Microsoft Word to write your programs!\\nThe book’s code examples are available on GitHub, so Git will be a useful piece of software to\\nhave. If you don’t know git, or don’t have it handy, GitHub provides a way to download all the\\nbook’s code in a Zip file. Make sure you check regularly for updates!\\nFinally, I recommend that you have an analog way of drafting your code and schema. I am a\\ncompulsive note-taker and doodler, and even if my drawing are very basic and crude, I find that\\nworking through a new piece of software via drawings helps in clarifying my thoughts. This\\nmeans less code re-writing, and a happier programmer! Nothing spiffy, some scrap paper and a\\npencil will do wonders.\\nPySpark is the Python API for Spark, a distributed framework for large-scale data\\nanalysis. It provides the expressiveness and dynamism of the Python programming\\nlanguage to Spark.\\nPySpark provides a full-stack analytics workbench. It has an API for data manipulation,\\ngraph analysis, streaming data as well as machine learning.\\nSpark is fast: it owes its speed to a judicious usage of the RAM available and an\\naggressive and lazy query optimizer.\\nSpark provides bindings for Python, Scala, Java, and R. You can also use SQL for data\\nmanipulation.\\nSpark uses a  which processes the instructions and orchestrates the work. The master\\n receive the instructions from the master and perform the work. executors\\nAll instructions in PySpark are either transformations or actions. Spark being lazy, only\\nactions will trigger the computation of a chain of instructions.1.4 Summary\\n10\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 13}), Document(page_content='2\\n1.  \\n2.  \\n3.  This chapter covers:\\nData-driven applications, no matter how complex, all boils down to what I like to call three \\n, which are easy to distinguish in a program. meta-steps\\nWe start by  or reading the data we wish to work with. ingesting\\nWe  the data, either via a few simple instructions or a very complex machinetransform\\nlearning model\\nWe then  the resulting data, either into a file to be fed into an app or byexport\\nsummarizing our findings into a visualization.\\nThe next two chapters will introduce a basic workflow with PySpark via the creation of a simple\\nETL ( , which is a more business-speak way of saying Extract, Transform and Load Ingest,\\n). We will spend most of our time at the  shell, interactively Transform and Export pyspark\\nbuilding our program one step at a time. Just like normal Python development, using the shell or\\nREPL (I’ll use the terms interchangeably) provides rapid feedback and quick progression. Once\\nwe are comfortable with the results, we will wrap our program so we can submit it in batch\\nmode.\\nData manipulation is the most basic and important aspect of any data-driven program and\\nPySpark puts a lot of focus on this. It serves as the foundation of any reporting, any machineYour first data program in PySpark\\nLaunching and using the  shell for interactive development pyspark\\nReading and ingesting data into a data frame\\nExploring data using the  structure DataFrame\\nSelecting columns using the  method select()\\nFiltering columns using the  method where()\\nApplying simple functions to your columns to modify the data they contain\\nReshaping singly-nested data into distinct records using explode()\\n11\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 14}), Document(page_content='learning or data science exercise we wish to perform. This section will give you the tools to not\\nonly use PySpark to manipulate data at scale, but also how to  in terms of data think\\ntransformation. We obviously can’t cover every function provided in PySpark, but I’ll provide a\\ngood explanation of the ones we use. I’ll also introduce how to use the shell as a friendly\\nreminder for those cases when you forget how something works.\\nSince this is your first end-to-end program in PySpark, we’ll get our feet wet with a simple\\nproblem to solve: counting the most popular word being used in the English language. Now,\\nsince collecting all the material ever produced in the English language would be a massive\\nundertaking, we’ll start with a very small sample:  by Jane Austen. We’ll Pride and Prejudice\\nmake our program work with this small sample and then scale it to ingest a larger corpus of text.\\nSince this is our first program, and I need to introduce many new concepts, this Chapter will\\nfocus on the data manipulation part of the program. Chapter 3 will cover the final computation as\\nwell as wrapping our program and then scaling it.\\nTIP The book repository, containing the code and data used for the examples\\nand exercises, is available at https://github.com/jonesberg/PySparkInAction\\n.\\nPySpark provides a REPL ( ) for interactive development. Python and other Read, eval, print loop\\nprogramming language families, such as Lisp, also provides one, so there is a good chance that\\nyou already worked with one in the past. It speeds up your development process by giving\\ninstantaneous feedback the moment you submit an instruction, instead of forcing you to compile\\nyour program and submit it as one big monolithic block. I’ll even say that using a REPL is even\\nmore useful in PySpark, since every operation can take a fair amount of time. Having a program\\ncrash mid-way is always frustrating, but it’s even worse when you’ve been running a data\\nintensive job for a few hours.\\nFor this chapter (and the rest of the book), I assume that you have access to a working\\ninstallation of Spark, either locally or in the cloud. If you want to perform the installation\\nyourself, Appendix B contains step-by-step instructions for Linux, OsX and Windows. If you\\ncan’t install it on your computer, or prefer not to, Appendix C provides a few cloud-powered\\noptions as well as additional instructions to upload your data and make it visible to Spark.\\nOnce everything is set up, you can launch the  shell by inputting  into your pyspark pyspark\\nterminal. You should see an ASCII-art version of the Spark logo, as well as some useful\\ninformation. shows what happens on my local machine.2.1 Setting up the pyspark shell\\n12\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 15}), Document(page_content='Listing 2.1 Launching  on a local machine pyspark\\nWhen using PySpark locally, you most often won’t have a full Hadoop cluster\\npre-configured. For learning purposes, this is perfectly fine.\\nSpark is indicating the level of details it’ll provide to you. We will see how to\\nconfigure this in .\\nWe are using Spark version 2.4.3\\nPySpark is using the Python available on your path.\\nThe  shell provides an entry point for you through the variable .pyspark spark\\nMore on this in .\\nThe REPL is now ready for your input!\\nNOTE I highly recommend you using IPython when using PySpark in interactive\\nmode. IPython is a better front-end to the Python shell containing many\\nuseful functionalities, such as friendlier copy-and-paste and syntax\\nhighlighting. The installation instructions in Appendix B includes configuring\\nPySpark to use the IPython shell.\\nWhile all the information provided in is useful, two elements are worth expanding on: the \\n entry point and the log level. SparkSession\\nIn we saw that, upon launching the PySpark shell creates a  variable that refers to spark\\n entry point. I will discuss about this entry point in this section as it provides the SparkSession\\nfunctionality for us to read data into PySpark .4$ pyspark\\nPython 3.7.3 (default, Mar 27 2019, 16:54:48)\\nType \\'copyright\\', \\'credits\\' or \\'license\\' for more information\\nIPython 7.4.0 -- An enhanced Interactive Python. Type \\'?\\' for help.\\n19/09/07 12:16:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable \\nUsing Spark\\'s default log4j profile: org/apache/spark/log4j-defaults.properties\\nSetting default log level to \"WARN\".\\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). \\nWelcome to\\n      ____              __\\n     / __/__  ___ _____/ /__\\n    _\\\\ \\\\/ _ \\\\/ _ `/ __/  \\'_/\\n   /__ / .__/\\\\_,_/_/ /_/\\\\_\\\\   version 2.4.3 \\n      /_/\\nUsing Python version 3.7.3 (default, Mar 27 2019 16:54:48) \\nSparkSession available as \\'spark\\'. \\nIn [1]: \\n2.1.1 The  entry-pointSparkSession\\n13\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 16}), Document(page_content='In , we spoke briefly about the Spark entry point called . Chapter 1 SparkContext SparkSession\\nis a super-set of that. It wraps the  and provides functionality for interacting with SparkContext\\ndata in a distributed fashion.Just to prove our point, see how easy it is to get to the \\n from our  object: just call the  attribute from SparkContext SparkSession sparkContext\\n.spark\\nThe  object is a recent addition to the PySpark API, making its way in versionSparkSession\\n2.0. This is due to the API evolving in a way that makes more room for the faster, more versatile\\ndata frame as the main data structure over the lower level RDD. Before that time, you had to use\\nanother object (called the ) in order to use the data frame. It’s much easier to have SQLContext\\neverything under a single umbrella.\\nThis book will focus mostly on the data frame as our main data structure. I’ll discuss about the\\nRDD in Chapter 8, when we discuss about lower-level PySpark programming and how to embed\\nour own Python functions in our programs.\\nSIDEBAR Reading older PySpark code\\nWhile this book shows modern PySpark programming, we are not living in\\na vacuum. If you go on the web, you might face older PySpark code that\\nuses the former /  combo. You’ll also see the SparkContext sqlContext sc\\nvariable mapped to the  entry-point. With that we know about SparkContext\\n and , we can reason about old PySpark code SparkSession SparkContext\\nby using the following variable assignments.\\nYou’ll see traces of  in the API documentation for backwards SQLContext\\ncompatibility. I recommend avoiding using this as the new SparkSession\\napproach is cleaner, simpler and more future-proof.\\nMonitoring your PySpark jobs is an important part of developing a robust program. PySpark\\nprovides many levels of logging, from nothing at all to a full description of everything happening\\non the cluster. By default, the  shell defaults on , that can be a little chatty when pyspark WARN\\nwe’re learning. Fortunately, we can change the settings for your session by using the code in .\\nListing 2.2 Deciding on how chatty you want PySpark to be.$ spark.sparkContext\\n# <SparkContext master=local[*] appName=PySparkShell>\\nsc = spark.sparkContext\\nsqlContext = spark\\n2.1.2 Configuring how chatty spark is: the log level\\nspark.sparkContext.setLogLevel(KEYWORD)\\n14\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 17}), Document(page_content='1.  \\n2.  \\n3.  \\n4.  \\n5.  lists the available keywords you can pass to . Each subsequent keyword contains setLogLevel\\nall the previous ones, with the obvious exception of  that doesn’t show anything. OFF\\nNOTE When using the  shell, anything chattier than  might appear pyspark WARN\\nwhen you’re typing a command, which makes it quite hard to input\\ncommands into the shell. You’re welcome to play with the log levels as you\\nplease, but we won’t show any output unless it’s valuable for the task at\\nhand.\\nSetting the log level to  is a  good way to annoy oblivious ALL very\\nco-workers if they don’t lock their computers. You haven’t heard it from me.\\nYou now have the REPL fired-up and ready for your input.\\nIn the Chapter introduction, we introduced our problem statement: what are the most popular\\n Before even hammering code in the REPL, we can start by words in the English language?\\nmapping the major steps our program will need to perform.\\nRead: Read the input data (we’re assuming a plain text file)\\nToken: Tokenize each word\\nClean: Remove any punctuation and/or tokens that aren’t words.\\nCount: Count the frequency of each word present in the text\\nAnswer: Return the top 10 (or 20, 50, 100)\\nVisually, a simplified flow of our program would look like 2.2 Mapping our programTable 2.1 log level keywords m\\nKeyword Signification\\nOFF No logging at all (not recommended).\\nFATAL Only fatal errors\\nERROR My personal favorite, will show  as well as other FATAL\\nuseful (but recoverable) errors.\\nWARN Add warnings (and there is quite a lot of them).\\nINFO Will give you runtime information\\nDEBUG Will provide debug information on your jobs.\\nTRACE Will trace your jobs (more verbose debug logs). Can be\\nquite pedagogic, but very annoying.\\nALL Everything that PySpark can spit, it will spit.\\n15\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 18}), Document(page_content='1.  \\n2.  \\nFigure 2.1 A simplified flow of our program, illustrating the 5 steps.\\nOur goal is quite lofty: the English language produced through history an unfathomable amount\\nof written material. Since we are learning, we’ll start with a relatively small source, get our\\nprogram working, and then scale it to accommodate a larger body of text. For this, I chose to use\\nJane Austen’s , since it’s already in plain text and freely available. Pride and Prejudice\\nSIDEBAR Data analysis and Pareto’s principle\\nPareto’s principle, also known commonly as the 80/20 rules, is often\\nsummarized as  In data 20% of the efforts will yield 80% of the results.\\nanalysis, we can consider that 20% to be analysis, visualization, machine\\nlearning models, anything that provides tangible value to the recipient.\\nThe remainder is what I call : ingesting the data, cleaning invisible work\\nit, figuring its meaning and shaping it into a usable form. If you look at your\\nsimple steps, Step 1 to 3 can be considered invisible work: we’re ingesting\\ndata and getting it ready for the counting process. Step 4 and 5 are really\\nthe visible ones that are answering our question (one could argue that only\\nStep 5 is performing visible work, but let’s not split hairs here). Steps 1 to 3\\nare there because the data requires processing to be usable for our\\nproblem. They aren’t core to our problem, but we can’t do without them.\\nWhen building your own project, this will be the part that will be the\\nmost time consuming and you might be tempted (or pressured!) to skimp\\non it. Always keep in mind that the data you ingest and process is the raw\\nmaterial of your programs, and that feeding it garbage will yield, well,\\ngarbage.\\nThe first step of our program is to ingest the data in a structure we can perform work in. PySpark\\nprovides two main structures for performing data manipulation:\\nThe Resilient Distributed Dataset (or RDD)2.3 Reading and ingesting data into a data frame\\n16\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 19}), Document(page_content='2.  The data frame\\nThe RDD can be seen like a distributed collection of objects. I personally visualize this as a bag\\nthat you give orders to. You pass orders to the RDD through regular Python functions over the\\nitems in the bag.\\nThe data frame is a stricter version of the RDD: conceptually, you can think of it like a table,\\nwhere each cell can contain one value. The data frame makes heavy usage of the concept of \\n where you perform operation on columns instead of on records, like in the RDD. columns\\nprovides a visual summary of the two structures.\\nIf you’ve used SQL in the past, you’ll find that the data frame implementation takes a lot of\\ninspiration from SQL. The module name for data organization and manipulation is even named \\n! Furthermore, Chapter 7 will teach you how to mix PySpark and SQL code within pyspark.sql\\nthe same program.\\nFigure 2.2 A RDD vs a data frame. In the RDD, each record is processed independently.\\nWith the data frame, we work with its columns, performing functions on them.\\nSIDEBAR Some language convention\\nSince this book will talk about data frames more than anything else, I prefer\\nusing the non-capitalized nomenclature,  \"data frame\". I find this to be i.e.\\nmore readable than using capital letters or even DataFrame without a\\nspace.\\nWhen referring to the PySpark object directly, I’ll use  but DataFrame\\nwith a fixed-width font. This will help differentiate between data frame the\\nconcept and  the object. DataFrame\\nThis book will focus on the data frame implementation as it is more modern and performs faster\\n17\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 20}), Document(page_content='for all but the most esoteric tasks. Chapter 8 will discuss about trade-offs between the RDD and\\nthe data frame. Don’t worry: once you’re learned the data frame, it’ll be a breeze the learn the\\nRDD.\\nReading data into a data frame is done through the  object, which we can DataFrameReader\\naccess through . The code in displays the object, as well as the methods it exposes. spark.read\\nWe recognize a few file formats:  stands for comma separated values (which we’ll use as csv\\nearly as ),  for JavaScript Object Notation (a popular data exchange format) and Chapter 4 json\\n is plain text.text\\nListing 2.3 The  objectDataFrameReader\\nSIDEBAR PySpark reads your data\\nPySpark provides many readers to accommodate the different ways you\\ncan process data. Under the hood,  will map to spark.read.csv()\\n and you may encounter this form in the spark.read.format(\\'csv\\').load()\\nwild. I usually prefer using the direct  method as it provides a handy csv\\nreminder of the different parameters the reader can take.\\norc and  are also data format especially well suited for big dataparquet\\nprocessing. ORC (which stands for ) and Parquet Optimized Row Columnar\\nare competing data format which serves pretty much the same purpose.\\nBoth are open-sourced and now part of the Apache project, just like Spark.\\nPySpark defaults to using parquet when reading and writing files, and\\nwe’ll use this format to store our results through the book. I’ll provide a\\nlonger discussion about the usage, advantages and trade-offs of using\\nParquet or ORC as a data format in Chapter 6.\\nLet’s read our data file. I am assuming that you launched PySpark at the root of this book’s\\nrepository. Depending on your case, you might need to change the path where the file is located.\\nListing 2.4 \"Reading\" our Jane Austen novel in record time\\nWe get a data frame, as expected! If you input your data frame, conveniently named , into book\\nthe shell, you see that PySpark doesn’t actually output any data to the screen. Instead, it printsIn [3]: spark.read\\nOut[3]: <pyspark.sql.readwriter.DataFrameReader at 0x115be1b00>\\nIn [4]: dir(spark.read)\\nOut[4]: [<some content removed>, _spark\\', \\'csv\\', \\'format\\', \\'jdbc\\', \\'json\\',\\n\\'load\\', \\'option\\', \\'options\\', \\'orc\\', \\'parquet\\', \\'schema\\', \\'table\\', \\'text\\']\\nbook = spark.read.text(\"./data/Ch02/1342-0.txt\")\\nbook\\n# DataFrame[value: string]\\n18\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 21}), Document(page_content='the schema, which is the name of the columns and their type. In PySpark’s world, each column\\nhas a type: it represents how the value is represented by Spark’s engine. By having the type\\nattached to each column, you can know instantly what operations you can do on a the data. With\\nthis information, you won’t inadvertently try to add an integer to a string: PySpark won’t let you\\nadd 1 to \"blue\". Here, we have one column, named , composed of a . A quick value string\\ngraphical representation of our data frame would look like . Besides being a helpful reminder of\\nthe content of the data frame, types are integral to how Spark processes data quickly and\\naccurately. We will explore the subject extensively in Chapter 5.\\nFigure 2.3 A high-level schema of a our book data frame, containing a value string\\ncolumn. We can see the name of the column, its type, and a small snippet of the data.\\nIf you want to see the schema in a more readable way, you can use the handy method \\n, illustrated in . This will print a tree-like version of the data frame’s schema. It printSchema()\\nis probably the method I use the most when developing interactively!\\nListing 2.5 Printing the schema of our data frame\\nSame information, displayed in a friendlier way.book.printSchema()\\n# root\\n#  |-- value: string (nullable = true)\\n19\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 22}), Document(page_content='SIDEBAR Speeding up your learning by using the shell\\nThis doesn’t just apply to PySpark, but using the functionality of the shell\\ncan often save a lot of searching into the documentation. I am a big fan of\\nusing  on an object when I don’t remember the exact method I want dir()\\nto apply, like I did in .\\nPySpark’s source code is very well documented, if you’re unsure about\\nthe proper usage of a function, class or method, you can print the doc\\nattribute or, for those using IPython, use a trailing question mark (or two, if\\nyou want more details).\\nListing 2.6 Using PySpark’s documentation directly in the REPL\\nOne of the key advantages of using the REPL for interactive development is that you can peek at\\nyour work as you’re performing it. Now that our data is loaded into a data frame, we can start\\nlooking at how PySpark structured our text.\\nIn , we saw that the default behaviour of imputing a data frame in the shell is to provide the\\nschema or column information of the object. While very useful, sometimes we want to take a\\npeek of the data.\\nEnter the  method. show()2.4 Exploring data in the  structure DataFrameIn [*]: print(spark.read.__doc__)\\n    Interface used to load a :class:`DataFrame` from external storage systems\\n    (e.g. file systems, key-value stores, etc). Use :func:`spark.read`\\n    to access this.\\n    .. versionadded:: 1.4\\nIn [*]: spark.read?\\nType:        property\\nString form: <property object at 0x1159a0958>\\nDocstring:\\nReturns a :class:`DataFrameReader` that can be used to read data\\nin as a :class:`DataFrame`.\\n:return: :class:`DataFrameReader`\\n.. versionadded:: 2.0\\n20\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 23}), Document(page_content='The most fundamental operation of any data processing library or program is displaying the data\\nit contains. In the case of PySpark, it becomes even more important, since we’ll definitely be\\nworking with data that goes beyond a screenful. Still, sometimes, you just want to see the data,\\nraw, without any complications.\\nThe  method displays a sample of the data back to you. Nothing more, nothing less. With show()\\n, it will become one of your best friend to perform data exploration and printSchema()\\nvalidation. By default, it will show 20 rows and truncate long rows. The code in shows the\\ndefault behaviour of the method, applied to our  data frame. For text data, the length book\\nlimitation is limiting (pun intended). Fortunately,  provides some options to display just show()\\nwhat you need.\\nListing 2.7 Showing a little data using the  method. .show()\\nThe  method takes three optional parameters.show()\\nn can be set to any positive integer, and will display that number of rows.\\ntruncate , if set to true, will truncate the columns to display only 20 characters. Set to \\n to display the whole length, or any positive integer to truncate to a specific False\\nnumber of characters.\\nvertical  takes a Boolean value and, when set to , will display each record as a True\\nsmall table. Try it!\\nThe code in shows a couple options, stating with showing 10 records and truncating then at 502.4.1 Peeking under the hood: the  method show()\\nbook.show()\\n# +--------------------+\\n# |               value|\\n# +--------------------+\\n# |The Project Guten...|\\n# |                    |\\n# |This eBook is for...|\\n# |almost no restric...|\\n# |re-use it under t...|\\n# |with this eBook o...|\\n# |                    |\\n# |                    |\\n# |Title: Pride and ...|\\n# |                    |\\n# | Author: Jane Austen|\\n# |                    |\\n# |Posting Date: Aug...|\\n# |Release Date: Jun...|\\n# |Last Updated: Mar...|\\n# |                    |\\n# |   Language: English|\\n# |                    |\\n# |Character set enc...|\\n# |                    |\\n# +--------------------+\\n# only showing top 20 rows\\n21\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 24}), Document(page_content='characters. We can see more of the text now!\\nListing 2.8 Showing less length, more width with the  method parameters show()\\nWith the  and  methods under your belt, you’re now fully ready to show() printSchema()\\nexperiment with your data.book.show(10, truncate=50)\\n# +--------------------------------------------------+\\n# |                                             value|\\n# +--------------------------------------------------+\\n# |The Project Gutenberg EBook of Pride and Prejud...|\\n# |                                                  |\\n# |This eBook is for the use of anyone anywhere at...|\\n# |almost no restrictions whatsoever.  You may cop...|\\n# |re-use it under the terms of the Project Gutenb...|\\n# |    with this eBook or online at www.gutenberg.org|\\n# |                                                  |\\n# |                                                  |\\n# |                        Title: Pride and Prejudice|\\n# |                                                  |\\n# +--------------------------------------------------+\\n# only showing top 10 rows\\n22\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 25}), Document(page_content='1.  \\n2.  \\n3.  \\n4.  \\n5.  SIDEBAR Non-lazy Spark?\\nIf you are coming from an other data frame implementation, such as\\nPandas or R , you might find it odd to see the structure of the data.frame\\ndata frame instead of a summary of the data when calling the variable. The\\n method might appear as a nuisance to you. show()\\nIf we take a step back and think about PySpark’s use-cases, it makes a\\nlot of sense.  is an action, since it perform the visible work of printing show()\\ndata on the screen. As savvy PySpark programmers, we want to avoid to\\naccidentally trigger the chain of computations, so the Spark developers\\nmade  explicit. When building a complicated chain of show()\\ntransformations, triggering its execution is a lot more annoying and\\ntime-consuming than having to type the  method when you’re ready. show()\\nThat being said, there are some moments, especially when learning,\\nwhen you want your data frames to be evaluated after each transformation\\n(which we call ). Since Spark 2.4.0, you can configure the eager evaluation\\n object to support printing to screen. We will cover how to SparkSession\\ncreate a  object in greater details in , but if you want SparkSession Chapter 3\\nto use eager evaluation in the shell, you can paste the following code in\\nyour shell.\\nAll the examples in the book assume that the data frames are evaluated\\nlazily, but this option can be useful if you’re demonstrating Spark. Use it as\\nyou see fit, but remember that Spark owe a lot of its performance to its lazy\\nevaluation. You’ll be leaving some extra horsepower on the table!\\nOur data is ingested and we’ve been able to see the two important aspects of our data frame:\\nits structure, via the  method; printSchema()\\na subset of the data it contains, via the  method. show()\\nWe can now start the real work: performing transformations on the data frame to accomplish our\\ngoal. Let’s take some time to review our 5 steps we outlined at the beginning of the chapter.\\n[DONE] : Read the input data (we’re assuming a plain text file)Read\\nToken: Tokenize each word\\nClean: Remove any punctuation and/or tokens that aren’t words.\\nCount: Count the frequency of each word present in the text\\nAnswer: Return the top 10 (or 20, 50, 100)from pyspark.sql import SparkSession\\nspark = (SparkSession.builder\\n                     .config(\"spark.sql.repl.eagerEval.enabled\", \"True\")\\n                     .getOrCreate())\\n23\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 26}), Document(page_content='Our next step will be to tokenize or separate each word so we can clean and count them.\\nWhen ingesting our selected text into a data frame, PySpark created one record for each line of\\ntext, and provided a  column of type String. In order to tokenize each word, we need to value\\nsplit each string into a list of distinct words.\\nI’ll start by providing the code in one fell swoop, and then we’ll break down each step one at a\\ntime. You can see it in all its glory in .\\nListing 2.9 Splitting our lines of text into arrays or words\\nIn a single line of code (I don’t count the import or the  which is only being used to show()\\ndisplay the result), we’ve done quite a lot. The remainder of this section will introduce basic\\ncolumn operations and explain how we can build our tokenization step as a one-liner. More\\nspecifically, we’ll learn about\\nThe  method and its canonical usage, which is selecting data.select()\\nThe  method to rename transformed columnsalias()\\nImporting column functions from  and using them. pyspark.sql.functions\\nThis section will introduce the most basic functionality of , which is to select one or select()\\nmore columns from your data frame. It’s a conceptually very simple method, but provides the\\nfoundation for many additional operations on your data.\\nIn PySpark’s world, a data frame is made out of  objects, and you perform Column\\ntransformations on them. The most basic transformation is the identity, where you return exactly\\nwhat was provided to you. If you’ve used SQL in the past, you might think that this sounds like a\\n\"SELECT\" statement, and you’d be right! You also get a free pass: the method name is also\\nconveniently named . select()2.5 Moving from a sentence to a list of words\\nfrom pyspark.sql.functions import split\\nlines = book.select(split(book.value, \" \").alias(\"line\"))\\nlines.show(5)\\n# +--------------------+\\n# |               value|\\n# +--------------------+\\n# |[The, Project, Gu...|\\n# |                  []|\\n# |[This, eBook, is,...|\\n# |[almost, no, rest...|\\n# |[re-use, it, unde...|\\n# +--------------------+\\n# only showing top 5 rows\\n2.5.1 Selecting specific columns using select()\\n24\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 27}), Document(page_content='1.  We’ll go over a quick example: selecting the only columns of our  data frame. Since we book\\nalready know the expected output, we can focus on the gymnastics fo the  method. select()\\nprovides the code performing that very useful task.\\nListing 2.10 The simplest select statement ever, provided by PySpark\\nPySpark provides for each column in its data frame a dot notation that refers to the column. This\\nis definitely the simplest way to select a column, as long as the name doesn’t contain any funny\\ncharacters: PySpark will accept  as a column name, but you won’t be able to use the dot $!@#\\nnotation for this column.\\nPySpark provides more than one way to select columns. I displayed the three most common in .\\nListing 2.11 Selecting the  column from the  data frame, three ways value book\\nThe first one is our old trusty dot notation that we got acquainted with a few paragraphs ago.\\nThe second one uses brackets instead of the dot to name the column. It addresses the $!@#\\nproblem since you pass the name of the column as a string.\\nThe last one uses the  function from the  module. The main col pyspark.sql.functions\\ndifference here is that you don’t specify that the column comes from the  data frame. This book\\nwill become very useful when working with more complex data pipelines in Part 2 of the book.\\nI’ll use the  object as much as I can since I consider its usage to be more idiomatic and it’ll col\\nprepare us for more complex use-cases.\\nWe just saw a very simple way to select a column in PySpark. We will now build on this\\nfoundation by selecting a transformation of a column instead. This provides a powerful and\\nflexible way to express our transformations, and as you’ll see, this pattern will be frequently used\\nwhen manipulating data.\\nPySpark provides a  function in the  module for splitting a split() pyspark.sql.functions\\nlonger string into a list of shorter strings. The most popular use-case for this function is to split a\\nsentence into words. The  function takes two parameters. split()\\nA column object containing stringsbook.select(book.value)\\nfrom pyspark.sql.functions import col\\nbook.select(book.value)\\nbook.select(book[\"value\"])\\nbook.select(col(\"value\"))\\n2.5.2 Transforming columns: splitting a string into a list of words\\n25\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 28}), Document(page_content='2.  A Java regular expression delimiter to split the strings against.\\nSince we want to split words, we won’t over-complicate our regular expression and just use the\\nspace character to split. shows the results of our code.\\nListing 2.12 Splitting our lines of text into lists of words\\nThe  functions transformed our  column into an  column, containing one orsplit string array\\nmore  elements. This is what we were expecting: even before looking at the data, seeingstring\\nthat the structure behaves according to plan is a good way to sanity-check our code.\\nLooking at the 5 rows we’ve printed, we can see that our values are now separated by a comma\\nand wrapped in square brackets, which is how PySpark visually represents an array. The second\\nrecord is empty, so we just see , an empty array. []\\nPySpark’s built-in functions for data manipulations are extremely useful and you should\\ndefinitely spend a little bit of time going over the API documentation to see what’s available\\nthere. If you don’t find exactly what you’re after, Chapter 6 will cover how you can create your\\nown function over  objects. Columnfrom pyspark.sql.functions import col, split\\nlines = book.select(split(col(\"value\"), \" \"))\\nlines\\n# DataFrame[split(value,  ): array<string>]\\nlines.printSchema()\\n# root\\n#  |-- split(value,  ): array (nullable = true)\\n#  |    |-- element: string (containsNull = true)\\nlines.show(5)\\n# +--------------------+\\n# |     split(value,  )|\\n# +--------------------+\\n# |[The, Project, Gu...|\\n# |                  []|\\n# |[This, eBook, is,...|\\n# |[almost, no, rest...|\\n# |[re-use, it, unde...|\\n# +--------------------+\\n# only showing top 5 rows\\n26\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 29}), Document(page_content='SIDEBAR Advanced topic: PySpark’s architecture and the JVM heritage\\nIf you’re like me, you might be interested to see how PySpark builds its\\ncore  functions. If you look at the source code for pyspark.sql.functions\\n, you might be in for a disappointment. split()\\nIt effectively refers to the  function of the  object. split sc_jvm.functions\\nThis has to do with how the data frame was built. PySpark’s uses a\\ntranslation layer to call JVM functions for its core functions. This makes\\nPySpark faster, since you’re not transforming your Python code into JVM\\none all the time: it’s already done for you. It also makes porting PySpark to\\nanother platform a little easier: if you can call the JVM functions directly,\\nyou don’t have to re-implement everything.\\nThis is one of the trade-offs of standing on the shoulders of the Spark\\ngiant. This also explains why PySpark uses JVM-base regular expressions\\ninstead of the Python ones in its built-in functions. Part 3 will expand on this\\nsubject greatly, but in the meantime, don’t be surprised if you explore\\nPySpark’s source code!\\nPySpark renamed our column in a very weird way:  isn’t what I’d consider an split(value, )\\nawesome name for our column. Just like the infomercials say, . there must be a better way!\\nWhen performing transformation on your columns, PySpark will give a default name to the\\nresulting column. In our case, we were blessed by the  name after splitting our split(value, )\\nvalue column using a space as the delimiter. While accurate, it’s definitely not programmer\\nfriendly.\\nThere is an implicit assumption that you’ll want to rename the resulting column yourself, using\\nthe  method. It’s usage isn’t very complicated: when applied to a column, it takes aalias()\\nsingle parameter, and returns the column it was applied to, with the new name. A simple\\ndemonstration is provided in .since(1.5)\\n@ignore_unicode_prefix\\ndef split(str, pattern):\\n    \"\"\"\\n    Splits str around pattern (pattern is a regular expression).\\n    .. note:: pattern is a string represent the regular expression.\\n    >>> df = spark.createDataFrame([(\\'ab12cd\\',)], [\\'s\\',])\\n    >>> df.select(split(df.s, \\'[0-9]+\\').alias(\\'s\\')).collect()\\n    [Row(s=[u\\'ab\\', u\\'cd\\'])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.split(_to_java_column(str), pattern))\\n2.5.3 Renaming columns:  and alias withColumnRenamed\\n27\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 30}), Document(page_content='1.  \\n2.  \\n3.  Listing 2.13 Our data frame before and after the aliasing\\nOur new column is called , which isn’t really pretty split(value, )\\nWe aliased our column to the name . Much better! line\\nalias()  provides a clean and explicit way to name your columns after you’ve performed work\\non it. On the other hand, it’s not the only renaming player in town. Another equally valid way to\\ndo so is by using the  method on the data frame. It takes two .withColumnRenamed()\\nparameters: the current name of the column and the wanted name of the column. Since we’re\\nalready performing work on the column with , chaining  makes a lot more sense split alias\\nthan using another method. shows you the two different approaches.\\nWhen writing your own code, choosing between those two options is pretty easy:\\nwhen you’re using a method where you’re specifying which columns you want to appear\\n(like  in our case here, but the next chapters will have many other examples), use select\\n. alias\\nif you just want to rename a column without changing the rest of the data frame, use \\n. .withColumnRenamed\\nListing 2.14 Renaming a column via  on the column and alias withColumnRenamed\\non the DataFrame\\nThis section introduced a new set of PySpark fundamentals: we learned how to select not only\\nplain columns, but also column transformations. We also learned how to explicitly name the\\nresulting columns, avoiding PySpark’s predictable but jarring naming convention. We can then\\nmove forward with the remainder of the operations. If we look at our 5 steps, we’re halfway\\ndone with step 2: we have a list of words, but we need for each token or word to be its own\\nrecords.\\n[DONE] : Read the input data (we’re assuming a plain text file)Read\\n[IN PROGRESS]  : Tokenize each wordTokenbook.select(split(col(\"value\"), \" \")).printSchema()\\n# root\\n#  |-- split(value,  ): array (nullable = true) \\n#  |    |-- element: string (containsNull = true)\\nbook.select(split(col(\"value\"), \" \").alias(\"line\")).printSchema()\\n# root\\n#  |-- line: array (nullable = true) \\n#  |    |-- element: string (containsNull = true)\\n# This looks a lot cleaner\\nlines = book.select(split(book.value, \" \").alias(\"line\"))\\n# This is messier, and you have to remember the name PySpark assigns automatically\\nlines = book.select(split(book.value, \" \"))\\nlines = lines.withColumnRenamed(\"split(value,  )\", \"line\")\\n28\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 31}), Document(page_content='3.  \\n4.  \\n5.  Clean: Remove any punctuation and/or tokens that aren’t words.\\nCount: Count the frequency of each word present in the text\\nAnswer: Return the top 10 (or 20, 50, 100)\\nWhen working with data, a key element in data preparation is making sure that it \"fits the mold\":\\nthis means making sure that the structure containing the data is logical and appropriate for the\\nwork at hand. At the moment, each record of our data frame contains multiple words into an\\narray of strings. It would be better to have one record for each word.\\nEnter the  function. When applied to a column containing a container-like data structure explode\\n(such as an array), it’ll take each element and give it its own row. This is much more easier\\nexplained visually than using words, so explains the process.\\nFigure 2.4 Exploding a data frame of array[String] into a data frame of String. Each\\nelement of each array becomes its own record.\\nThe code follows the same structure as split, and you can see the results in . We now have a data\\nframe containing at most one word per row. We are almost there!\\nBefore continuing our data processing journey, we can take a step back and look at a sample of\\nthe data. Just by looking at the 15 rows returned, we can see that  has a comma and Prejudice,\\nthat the cell between  and  contains the empty string. That gives us a good blueprint Austen This\\nof the next steps that needs to be performed before we start analyzing word frequency.2.6 Reshaping your data: exploding a list into rows\\n29\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 32}), Document(page_content='1.  \\n2.  \\n3.  \\n4.  \\n5.  Listing 2.15 Exploding a column of arrays into rows of elements\\nLooking back at our 5 steps, we can now conclude step 2, and our words are tokenized. Let’s\\nattack the third one, where we’ll be cleaning our words to simplify the counting.\\n[DONE] : Read the input data (we’re assuming a plain text file)Read\\n[DONE] : Tokenize each wordToken\\nClean: Remove any punctuation and/or tokens that aren’t words.\\nCount: Count the frequency of each word present in the text\\nAnswer: Return the top 10 (or 20, 50, 100)\\nSo far, with  and , our pattern has been the following: find the relevant function in split explode\\n, apply it, profit! This section will use the same winning formula to pyspark.sql.functions\\nnormalize the case of our words and remove punctuation, so we’ll walk a little faster.\\ncontains the source code to lower the case of all the words in the data frame. The code should\\nlook very familiar: we select a column transformed by , a PySpark function lowering the lower\\ncase of the data inside the column passed as a parameter. We then alias the resulting column to \\n to avoid PySpark’s default nomenclature. Illustrated, it could look approximately like .word2.7 Working with words: changing case and removing\\npunctuationfrom pyspark.sql.functions import explode, col\\nwords = lines.select(explode(col(\"line\")).alias(\"word\"))\\nwords.show(15)\\n# +----------+\\n# |      word|\\n# +----------+\\n# |       The|\\n# |   Project|\\n# | Gutenberg|\\n# |     EBook|\\n# |        of|\\n# |     Pride|\\n# |       and|\\n# |Prejudice,|\\n# |        by|\\n# |      Jane|\\n# |    Austen|\\n# |          |\\n# |      This|\\n# |     eBook|\\n# |        is|\\n# +----------+\\n# only showing top 15 rows\\n30\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 33}), Document(page_content='Listing 2.16 Lower the case of the words in the data frame\\nFigure 2.5 Applying lower to a column and aliasing the resulting column: a creative\\nexplanation\\nRemoving punctuation and other non-useful characters can be a little trickier. We won’t\\nimprovise a full NLP (Natural Language Processing, which is an amazingly field of data\\nanalysis/science that focuses on text. We’ll cover it briefly in Chapter 9!) library here, relying on\\nthe functionality PySpark provides in its data manipulation toolbox. In the spirit of keeping this\\nexercise simple, we’ll keep the first contiguous group of letters as the word, including\\napostrophes, and remove the rest. It will effectively remove punctuation, quotation marks and\\nother symbols, at the expense of being less robust with more exotic word construction. shows the\\ncode in all its splendour.from pyspark.sql.functions import lower\\nwords_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\\nwords_lower.show()\\n# +-----------+\\n# | word_lower|\\n# +-----------+\\n# |        the|\\n# |    project|\\n# |  gutenberg|\\n# |      ebook|\\n# |         of|\\n# |      pride|\\n# |        and|\\n# | prejudice,|\\n# |         by|\\n# |       jane|\\n# |     austen|\\n# |           |\\n# |       this|\\n# |      ebook|\\n# |         is|\\n# |        for|\\n# |        the|\\n# |        use|\\n# |         of|\\n# |     anyone|\\n# +-----------+\\n# only showing top 20 rows\\n31\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 34}), Document(page_content='SIDEBAR Regular expressions for the rest of us\\nPySpark uses regular expressions in two functions we used so far: \\n and . You do not have to be a regexp expert to regexp_extract() split()\\nwork with PySpark (I certainly am not). Through the book, each time that I’ll\\nuse a non-trivial regular expression, I’ll provide a plain English definition so\\nyou can follow along.\\nIf you are interested in building your own, the RegExr (\\n) website is really useful, as well as the https://regexr.com/ Regular\\n, by Steven Levithan and Jan Goyvaerts (O’Reilly, Expression Cookbook\\n2012).\\nListing 2.17 Using  to keep what looks like a word regexp_extract\\nWe only match for multiple lower-case characters (between  and ). The star ( ) a z *\\nwill match for 0 or more occurrences.\\nOur data frame of words looks pretty regular by now, with the exception of the empty cell\\nbetween  and . We will solve this with a judicious usage of filtering. austen thisfrom pyspark.sql.functions import regexp_extract\\nwords_clean = words_lower.select(\\n    regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word\")  \\n)\\nwords_clean.show()\\n# +---------+\\n# |     word|\\n# +---------+\\n# |      the|\\n# |  project|\\n# |gutenberg|\\n# |    ebook|\\n# |       of|\\n# |    pride|\\n# |      and|\\n# |prejudice|\\n# |       by|\\n# |     jane|\\n# |   austen|\\n# |         |\\n# |     this|\\n# |    ebook|\\n# |       is|\\n# |      for|\\n# |      the|\\n# |      use|\\n# |       of|\\n# |   anyone|\\n# +---------+\\n# only showing top 20 rows\\n32\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 35}), Document(page_content='An important data manipulation operation is to be able to filter records according to a certain\\npredicate. In our case, blank cells shouldn’t be considered: they’re not words! Conceptually, we\\nshould be able to provide a test to perform on each records: if it returns true, we keep the record.\\nFalse? You’re out!\\nPySpark provides not one, but two identical methods to perform this task. You can either use \\n or its alias . This duplication is to ease the transition for users coming from .filter() .where()\\nother data processing engines or libraries: some use one, some the other. PySpark provides both,\\nso no arguments possible! I personally prefer  because it’s one character less and my w where()\\nkey is less used than f, but you might have other motives. If we look at , we can see that columns\\ncan be compared to values using the usual Python comparison operators. In this case, we’re\\nusing the \"not equal\", or . !=\\nListing 2.18 Filtering rows in your data frame, using  or . where filter\\nWe could have tried to filter earlier in our program. It’s a trade-off to consider: if we filtered too\\nearly, our filtering clause would have been comically complex for no good reason. Since\\nPySpark caches all the transformations until an action is triggered, we can focus on the\\nreadability of our code and let Spark optimize our intent, like we saw in Chapter 1. We’ll see in\\nChapter 3 how you can transform PySpark code so it almost reads like a series of written\\ninstructions and take advantage of the lazy evaluation.2.8 Filtering rows\\nwords_nonull = words_clean.where(col(\"word\") != \"\")\\nwords_nonull.show()\\n# +---------+\\n# |     word|\\n# +---------+\\n# |      the|\\n# |  project|\\n# |gutenberg|\\n# |    ebook|\\n# |       of|\\n# |    pride|\\n# |      and|\\n# |prejudice|\\n# |       by|\\n# |     jane|\\n# |   austen|\\n# |     this| <-- See, the blank cell is gone!\\n# |    ebook|\\n# |       is|\\n# |      for|\\n# |      the|\\n# |      use|\\n# |       of|\\n# |   anyone|\\n# | anywhere|\\n# +---------+\\n# only showing top 20 rows\\n33\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 36}), Document(page_content='1.  \\n2.  \\n3.  \\n4.  \\n5.  This seems like a good time to take a break and reflect on what we accomplished so far. If we\\nlook at our 5 steps, we’re 60% of the way there. Our cleaning step took care of non-letter\\ncharacters and filtered the empty records. We’re ready for counting and displaying the results of\\nour analysis.\\n[DONE] : Read the input data (we’re assuming a plain text file)Read\\n[DONE] : Tokenize each wordToken\\n[DONE] : Remove any punctuation and/or tokens that aren’t words.Clean\\nCount: Count the frequency of each word present in the text\\nAnswer: Return the top 10 (or 20, 50, 100)\\nIn terms of PySpark operations, we covered a huge amount of ground in the data manipulation\\nspace. You can now select not only columns but transformations of columns, renaming them as\\nyou please after the fact. We learned how to break nested structures, such as arrays, into single\\nrecords. We finally learn how to filter records using simple tests.\\nWe can now rest. The next chapter will cover the end of our program. We will also be looking at\\nbringing our code in one single file, moving away from the REPL into batch mode. We’ll\\nexplore options to simplify and increase the readability of our program, and then finish by\\nscaling it to larger corpus of texts.\\nAlmost all PySpark programs will revolve around 3 major steps: reading, transforming\\nand exporting data.\\nPySpark provides a REPL (read, eval, print loop) via the  shell where you can pyspark\\nexperiment interactively with data.\\nA PySpark’s data frame is a collection of columns. You operate on the structure using\\nchained transformations. PySpark will optimize the transformations and perform the\\nwork only when you submit an action, such as . This is one of the pillars of show()\\nPySpark’s performance.\\nPySpark’s repertoire of functions that operate on columns are located in \\n. pyspark.sql.functions\\nYou can select columns or transformed columns via the  statement. select()\\nYou can filter columns using the  or  methods and providing a test that where() filter()\\nwill return  or , only the records returning  will be kept. True False True\\nPySpark can have columns of nested values, like arrays of elements. In order to extract\\nthe elements into distinct records, you need to use the  method. explode()\\nRewrite the following code snippet, removing the  method. Which version withColumnRenamed\\nis clearer and easier to read?2.9 Summary\\n2.10 Exercises\\n2.10.1 Exercise 2.1\\n34\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 37}), Document(page_content='The following code blocks gives an error. What is the problem and how can you solve it?\\nLet’s take our  data frame, available in . You can use the code in the repository ( words_nonull\\n) into your REPL to get the data frame loaded. code/Ch02/end_of_chapter.py\\na) Remove all of the occurrences of the word \"is\"\\nb) (Challenge) Using the  function explained in exercise 2.1, keep only the words with length\\nmore than 3 characters.\\nThe  clause takes a Boolean expression over one or many column to filter the data framewhere\\n(see ). Beyond the usual Boolean operators ( , , , , , ), PySpark provides other functions > < == >= !=\\nreturning Boolean columns in the  module. pyspark.sql.functions\\nA good example is the  function, which takes a list of values as a parameter, and will isin()\\nreturn only the records where the value in the column equals a member of the list.\\nLet’s say you want to  the words , ,  and  from your list of words, using a remove is not the if\\nsingle  method on the  data frame (see exercise 2.3). Write the code to do where() words_nonull\\nso.from pyspark.sql.functions import col, length\\n# The `length` function returns the number of characters in a string column.\\nex21 = (\\n    spark.read.text(\"./data/Ch02/1342-0.txt\")\\n    .select(length(col(\"value\")))\\n    .withColumnRenamed(\"length(value)\", \"number_of_char\")\\n)\\n2.10.2 Exercice 2.2\\nfrom pyspark.sql.functions import col, greatest\\nex22.printSchema()\\n# root\\n#  |-- key: string (containsNull = true)\\n#  |-- value1: long (containsNull = true)\\n#  |-- value2: long (containsNull = true)\\n# `greatest` will return the greatest value of the list of column names,\\n# skipping null value\\n# The following statement will return an error\\nex22.select(\\n    greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\\n).select(\\n    \"key\", \"max_value\"\\n)\\n2.10.3 Exercise 2.3\\n2.10.4 Exercise 2.4\\n35\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 38}), Document(page_content='One of your friends come to you with the following code. They have no idea why it doesn’t\\nwork. Can you diagnose the problem, explain why it is an error and provide a fix?2.10.5 Exercise 2.5\\nfrom pyspark.sql.functions import col, split\\nbook = spark.read.text(\"./data/ch02/1342-0.txt\")\\nbook = book.printSchema()\\nlines = book.select(split(book.value, \" \").alias(\"line\"))\\nwords = lines.select(explode(col(\"line\")).alias(\"word\"))\\n36\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 39}), Document(page_content='3\\n1.  \\n2.  \\n3.  \\n4.  \\n5.  This chapter covers:\\nChapter 2 dealt with all the data preparation work for our word frequency program. We  the read\\ninput data,  each word and  our records to only keep lower-case words. If we tokenized cleaned\\nbring out our outline, we only have Step 4 and 5 to complete.\\n[DONE] : Read the input data (we’re assuming a plain text file)Read\\n[DONE] : Tokenize each wordToken\\n[DONE] : Remove any punctuation and/or tokens that aren’t words.Clean\\nCount: Count the frequency of each word present in the text\\nAnswer: Return the top 10 (or 20, 50, 100)\\nAfter tackling those two last steps, we’ll look at packaging our code in a single file to be able to\\nsubmit it to Spark without having to launch a shell. We’ll take a look at our completed program\\nand look at simplifying our program by removing intermediate variables. We’ll finish with\\nscaling our program to accommodate more data sources.Submitting and scaling your first PySpark\\nprogram\\nSummarizing data using  and a simple aggregate function groupby\\nOrdering results for display\\nWriting data from a data frame\\nUsing  to launch your program in batch mode spark-submit\\nSimplify the writing of your PySpark using method chaining\\nScaling your program almost for free!\\n37\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 40}), Document(page_content='If you take our data frame in the same shape as it was left at the end of Chapter 2 (hint: look at \\n if you want to catch up), there is not much to be done. Having code/Ch02/end_of_chapter.py\\na data frame containing one single word per record, we just have to count the word occurrences\\nand take the top contenders. This section will show how to count records using the GroupedData\\nobject and perform an aggregation function (here counting the items) on each group. A more\\ngeneral blueprint for grouping and aggregating data will be touched upon in Chapter 4, but we’ll\\nsee the basics in this Chapter.\\nThe easiest way to count record occurrence is to use the  method, passing the columns groupby\\nwe wish to group on as a parameter. The code in shows that the returned value is a GroupedData\\nobject, not a . I call this  object a : PySpark grouped DataFrame GroupedData transitional object\\nour data frame on the  column, waiting for instructions on how to summarize the word\\ninformation contained in each group. Once we apply the  method, we get back a data count()\\nframe containing the grouping column , as well as  column containing the number of word count\\noccurrences for each word. A visual interpretation of how a  morphes into a DataFrame\\n object is on display in GroupedData3.1 Grouping records: Counting word frequencies\\n38\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 41}), Document(page_content='Listing 3.1 Counting word frequencies using  and groupby() count()\\nFigure 3.1 A schematic representation of our groups object. Each small box represents a\\nrecord.groups = words_nonull.groupby(col(\"word\"))\\ngroups\\n# <pyspark.sql.group.GroupedData at 0x10ed23da0>\\nresults = words_nonull.groupby(col(\"word\")).count()\\nresults\\n# DataFrame[word: string, count: bigint]\\nresults.show()\\n# +-------------+-----+\\n# |         word|count|\\n# +-------------+-----+\\n# |       online|    4|\\n# |         some|  203|\\n# |        still|   72|\\n# |          few|   72|\\n# |         hope|  122|\\n# |        those|   60|\\n# |     cautious|    4|\\n# |       lady\\'s|    8|\\n# |    imitation|    1|\\n# |          art|    3|\\n# |      solaced|    1|\\n# |       poetry|    2|\\n# |    arguments|    5|\\n# | premeditated|    1|\\n# |      elevate|    1|\\n# |       doubts|    2|\\n# |    destitute|    1|\\n# |    solemnity|    5|\\n# |gratification|    1|\\n# |    connected|   14|\\n# +-------------+-----+\\n# only showing top 20 rows\\n39\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 42}), Document(page_content='1.  \\n2.  Peeking at the  data frame in , we see that the results are in no specific order. As a results\\nmatter of fact, I’d be very surprised if you had the exact same order of words as me! This has to\\ndo with how PySpark manages data: in Chapter 1, we learned that PySpark distributes the data\\nacross multiple nodes. When performing a grouping function, such as , each worker groupby\\nperforms the work on its assigned data.  and  are transformations, so PySpark will groupby count\\nqueue them lazily until we request an action. When we pass the  method to our results data show\\nframe, it triggers the chain of computation that we see in .\\nFigure 3.2 A distributed group by on our words_nonull data frame. The work is performed\\nin a distributed fashion until we need to assemble the results in a cohesive display, via\\nshow().\\nBecause of the distributed and lazy nature of PySpark, it makes sense to not care about the\\nordering of records until explicitly mentioned. Since we wish to see the top words on display,\\nlet’s put a little order in our data frame and, by the same occasion, complete the last step of our\\nprogram.\\nIn , we explained why PySpark doesn’t necessarily maintain order of records when performing\\ntransformations. If we look at our 5 step blueprint, the last step is to return the top N records, for\\ndifferent values of N. We already know how to show a specific number of records, so this\\nsection will focus on ordering the records in a data frame.\\n[DONE] : Read the input data (we’re assuming a plain text file)Read3.2 Ordering the results on the screen using orderBy\\n40\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 43}), Document(page_content='2.  \\n3.  \\n4.  \\n5.  [DONE] : Tokenize each wordToken\\n[DONE] : Remove any punctuation and/or tokens that aren’t words.Clean\\n[DONE] : Count the frequency of each word present in the textCount\\nAnswer: Return the top 10 (or 20, 50, 100)\\nThe method PySpark provides for ordering records in a data frame is . Without further orderBy\\nado, let’s take a look at , which performs and shows the results of this computation.\\nListing 3.2 Displaying the top 10 words in Jane’s Austen Pride and Prejudice\\nThe list is very much unsurprising: even though we can’t argue with Ms. Austen’s vocabulary,\\nshe isn’t immune to the fact that the English language needs pronouns and other very common\\nwords. In natural language processing, those words are called  and would be removed. stop words\\nAs far as we are concerned, we solved our original query and can rest easy. Should you want to\\nget the top 20, top 50, or even top 1,000, it’s easily done by changing the parameter to . show()\\nSIDEBAR PySpark’s method naming convention zoo\\nIf you have a very good sense of details, you might have noticed that we\\nused  (lowercase), but  (lowerCamelCase, where you groupby orderBy\\ncapitalize each word but the first). This seems like an odd design choice.\\ngroupby is in fact an alias for , just like  is an alias of groupBy where\\n. My guess is that the PySpark developers found that a lot of typing filter\\nmistakes were avoided by accepting the two cases.  didn’t have orderBy\\nthat luxury, for a reason that escape my understanding, so we need to be\\nmindful. You can see the output of IPython’s auto-complete for those two\\nmethods in .results.orderBy(\"count\", ascending=False).show(10)\\n# +----+-----+\\n# |word|count|\\n# +----+-----+\\n# | the| 4480|\\n# |  to| 4218|\\n# |  of| 3711|\\n# | and| 3504|\\n# | her| 2199|\\n# |   a| 1982|\\n# |  in| 1909|\\n# | was| 1838|\\n# |   i| 1749|\\n# | she| 1668|\\n# +----+-----+\\n# only showing top 10 rows\\n41\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 44}), Document(page_content='Figure 3.3 PySpark’s camelcase vs camelCase\\nSIDEBAR Part of this incoherence is due to Spark’s heritage. Scala prefers\\ncamelCase for methods. On the other hand, we saw , regexp_extract\\nwhich uses Python’s preferred snake_case (words separated by an\\nunderscore) in . There is no magic secret here: you’ll have to be Chapter 2\\nmindful about the different case conventions at play in PySpark.\\nShowing results on the screen is great for quick assessment, but most of the time, you’ll want\\nthem to have some sort of longevity. It’s much better to save those results into a file, so we’ll be\\nable to re-use those results without having to compute everything each time.\\nHaving the data on the screen is great for interactive development, but you’ll often want to\\nexport your results. PySpark treats writing data a little differently than most data processing\\nlibraries, since it can scale to immense volumes of data. For this, we’ll start by naively write our\\nresults in a CSV file, and see how PySpark performs the job.\\nshows the code and results. A data frame exposes the  method, which we can specialize for write\\nCSV by chaining the  method. This is very consistent with the  method we saw in csv read\\nChapter 2. If we look at the results, we can see that PySpark didn’t create a  file. results.csv\\nInstead, it created a directory of the same name, and put 201 files inside the directory (200 CSVs\\n+ 1  file)._SUCCESS3.3 Writing data from a data frame\\n42\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 45}), Document(page_content=\"Listing 3.3 Writing our results in multiple CSV files, one per partition\\nThe results are written in a directory called results.csv\\nThe  file means the operation was successful_SUCCESS\\nWe have  to , which means our results are split across 200 part-00000 part-00199\\nfiles\\nThere it is, ladies and gentleman! The first moment where we have to care about PySpark’s\\ndistributed nature.\\nJust like PySpark will distribute the transformation work across multiple workers, it’ll do the\\nsame for writing data. While it might look like a nuisance for our simple program, it is\\ntremendously useful when working in distributed environments. When you have a large cluster\\nof nodes, having many smaller files makes it easy to logically distribute reading and writing the\\ndata, making it way faster than having a single massive file.\\nBy default, PySpark will give you 1 file per partition. This means that our program, as run on my\\nmachine, yields 200 partitions at the end. This isn’t the best for portability. In order to reduce the\\nnumber to partitions, we can apply the  method with the desired number of partitions. coalesce\\nshows the difference of using  on our data frame before writing to disk. We still get coalesce(1)\\na directory, but there is a single CSV file inside of it. Mission accomplished!results.write.csv('./results.csv')\\n# The following command is run using the shell.\\n# In IPython, you can use the bang pattern (! ls -l)\\n# to get the same results without leaving the console.\\n#`ls -l` is a Unix command listing the content of a directory.\\n# On windows, you can use `dir` instead\\n$ ls -l\\n# [...]\\n# -rw-r--r--@   1 jonathan_rioux  247087069  724726 Jul 30 17:30 1342-0.txt\\n# drwxr-xr-x  404 jonathan_rioux  247087069   12928 Aug  4 13:31 results.csv \\n$ ls -l results.csv/\\n# [...]\\n# -rw-r--r--  1 jonathan_rioux  247087069    0 Aug \\n  4 13:31 _SUCCESS \\n# -rw-r--r--  1 jonathan_rioux  247087069  468 Aug \\n  4 13:31 part-00000-615b75e4-ebf5-44a0-b337-405fccd11d0c-c000.csv\\n# [...]\\n# -rw-r--r--  1 jonathan_rioux  247087069  353 Aug \\n  4 13:31 part-00199-615b75e4-ebf5-44a0-b337-405fccd11d0c-c000.csv \\n43\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion\", metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 46}), Document(page_content=\"Listing 3.4 Writing our results under a single partition\\nNOTE You might have realized that we’re not ordering the file before writing it.\\nSince our data here is pretty small, we could have written the words by\\ndecreasing order of frequency. If you have a very large data set, this\\noperation will be quite expensive. Furthermore, since reading is a\\npotentially distributed operation, what guarantees that it’ll get read the\\nexact same way? Never assume that your data frame will keep the same\\nordering of records unless you explicitly ask via . orderBy()\\nOur workflow has been pretty interactive so far. We write one or two lines of text before\\nshowing the result to the terminal. As we get more and more confident with operating on the data\\nframe’s structure, those shows will become fewer.\\nNow that we’ve performed all the necessary steps interactively, let’s look in putting our program\\nin a single file and looking at refactoring opportunities.\\nInteractive development is fantastic for rapid iteration of our code. When developing programs,\\nit’s great to experiment and validate our thoughts through rapid code inputs into a shell. When\\nthe experimentation is over, it’s good to bring our program into a cohesive body of code.\\nThe  shell allows to go back in history using the directional arrows of your keyboard,pyspark\\njust like a regular python REPL. To make things a bit easier, I am providing the step by step\\nprogram in . This section is dedicated to streamline and make our code terser and more readable.3.4 Putting it all together: countingresults.coalesce(1).write.csv('./results_single_partition.csv')\\n$ ls -l\\n# [...]\\n# -rw-r--r--@   1 jonathan_rioux  247087069  724726 Jul 30 17:30 1342-0.txt\\n# drwxr-xr-x  404 jonathan_rioux  247087069   12928 Aug  4 13:31 results.csv\\n# drwxr-xr-x    6 jonathan_rioux  247087069     192 Aug  4 13:43 results_single_partition.csv\\n$ ls -l results_single_partition.csv/\\n# [...]\\n# -rw-r--r--  1 jonathan_rioux  247087069      0 Aug \\n  4 13:43 _SUCCESS\\n# -rw-r--r--  1 jonathan_rioux  247087069  70993 Aug \\n  4 13:43 part-00000-f8c4c13e-a4ee-4900-ac76-de3d56e5f091-c000.csv\\n44\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion\", metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 47}), Document(page_content='Listing 3.5 Our first PySpark program, dubbed \"Counting Jane Austen\"\\nThis program runs perfectly id you paste its entirety in the  shell. With everything in the pyspark\\nsame file, we can look at making our code friendlier and easier for future you to come back at it.\\nThis program uses five distinct functions from the  modules. We pyspark.sql.functions\\nshould probably replace this with a qualified import, which is Python’s way to import a module\\nby assigning a keyword to it. While there is no hard rule, the common wisdom is to use  to refer F\\nto PySpark’s functions. shows the before and after.\\nListing 3.6 Simplifying our PySpark functions import\\nSince , , ,  and  are all in the col explode lower regexp_extract split\\n, we can import the whole module. Since the new import statement pyspark.sql.functions\\nimports the entirety of the  module, we assign the keyword (or pyspark.sql.functions\\nkey-letter) . The PySpark community seems to have implicitly settled on using  for F F\\n and I encourage you to do the same. It’ll make your programs pyspark.sql.functions\\nconsistent, and since many functions in the module share their name with Pandas or Python\\nbuilt-in functions, you’ll avoid name clashes.from pyspark.sql.functions import col, explode, lower, regexp_extract, split\\nbook = spark.read.text(\"./data/ch02/1342-0.txt\")\\nlines = book.select(split(book.value, \" \").alias(\"line\"))\\nwords = lines.select(explode(col(\"line\")).alias(\"word\"))\\nwords_lower = words.select(lower(col(\"word\")).alias(\"word\"))\\nwords_clean = words_lower.select(\\n    regexp_extract(col(\"word\"), \"[a-z\\']*\", 0).alias(\"word\")\\n)\\nwords_nonull = words_clean.where(col(\"word\") != \"\")\\nresults = words_nonull.groupby(col(\"word\")).count()\\nresults.orderBy(\"count\", ascending=False).show(10)\\nresults.coalesce(1).write.csv(\"./results_single_partition.csv\")\\n3.4.1 Simplifying your dependencies with PySpark’s import conventions\\n# Before\\nfrom pyspark.sql.functions import col, explode, lower, regexp_extract, split\\n# After\\nimport pyspark.sql.functions as F\\n45\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 48}), Document(page_content='WARNING It can be very tempting to do a start import like from\\n. Do not fall into that trap! It’ll make it pyspark.sql.functions import *\\nhard for your readers which functions comes from PySpark and which\\ncomes from regular Python. In Chapter 8, when we’ll use user defined\\nfunctions (UDF), this separation will become even more important. Good\\ncoding hygiene rules!\\nThat was easy enough. Let’s look at how we can simplify our program flow by using one of my\\nfavourite aspect of PySpark, its  abilities. chaining\\nIf we look at the transformation methods we applied on our data frames ( , , select() where()\\n and ), they all have something in common: they take a structure as a groupBy() count()\\nparameter— the data frame or  in the case of — and return a structure. GroupedData count()\\nThere is no concept of in-place modification in PySpark: all transformations can be seen as a\\npipe that ingests a structure and returns a modified structure. This section will look at what is\\nprobably my favourite aspect of PySpark: method chaining.\\nOur program uses intermediate variables quite a lot: everytime we perform a transformation, we\\nassigned the result to a new variable. This is very useful when using the shell as we keep  of state\\nour transformation and can peek at our work at the end of every step. On the other hand, once\\nour program works, this multiplication of variables is not as useful and can clutter our program\\nvisually.\\nIn PySpark, every transformation returns an object, which is why we need to assign a variable to\\nthe result. This means that PySpark doesn’t perform modifications . For instance, the in place\\nfollowing code block by itself wouldn’t do anything because we don’t assign the result to a\\nvariable or perform an action to display or save our results.\\nWe can avoid intermediate variables by  the results of one method to the next. Since chaining\\neach transformation returns a data frame (or a , when we perform the GroupedData groupby()\\nmethod, we can directly append the next method without assigning the result to a variable. This\\nmeans that we can eschew all but one variable assignment. The code in shows the before and\\nafter. Note that we also added the  prefix to our functions, to respect the import convention we F\\noutlined in .3.4.2 Simplifying our program via method chaining\\nresults.orderBy(\"word\").count()\\n46\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 49}), Document(page_content='Listing 3.7 Removing intermediate variables by chaining transformation methods\\nIt’s like night and day: the \"after\" is much terser and readable, and we’re able to easily follow the\\nlist of steps. Visually, we can also see the difference in .# Before\\nbook = spark.read.text(\"./data/ch02/1342-0.txt\")\\nlines = book.select(split(book.value, \" \").alias(\"line\"))\\nwords = lines.select(explode(col(\"line\")).alias(\"word\"))\\nwords_lower = words.select(lower(col(\"word\")).alias(\"word\"))\\nwords_clean = words_lower.select(\\n    regexp_extract(col(\"word\"), \"[a-z\\']*\", 0).alias(\"word\")\\n)\\nwords_nonull = words_clean.where(col(\"word\") != \"\")\\nresults = words_nonull.groupby(\"word\").count()\\n# After\\nresults = (\\n    spark.read.text(\"./data/ch02/1342-0.txt\")\\n    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\\n    .select(F.explode(F.col(\"line\")).alias(\"word\"))\\n    .select(F.lower(F.col(\"word\")).alias(\"word\"))\\n    .select(F.regexp_extract(F.col(\"word\"), \"[a-z\\']*\", 0).alias(\"word\"))\\n    .where(F.col(\"word\") != \"\")\\n    .groupby(\"word\")\\n    .count()\\n)\\n47\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 50}), Document(page_content='Figure 3.4 Method chaining eliminates the need for intermediate variables\\nI am not saying that intermediate variables are absolutely evil and to be avoided. They can hinder\\nyour code readability, so you have to make sure they serve a purpose. A lot of burgeoning\\nPySpark developers take the habit of always writing on top of the same variable. If you see\\nyourself doing something like , chain your methods like in . You’ll get the same result, and\\nprettier code.\\nListing 3.8 If you write over the same variable over and over again, consider\\nchaining your methods.\\n# Don\\'t do that\\ndf = spark.read.text(\"./data/ch02/1342-0.txt\")\\ndf = df.select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\\ndf = ...\\n48\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 51}), Document(page_content='SIDEBAR Make your life easier by using Python’s parentheses\\nIf you look at the \"after\" code of , you’ll notice that I start my right side of the\\nequal sign with an opening parenthesis ( ). This is a trick I spark = ( […]\\nuse when I need to chain methods in Python. If you don’t wrap your result\\ninto a pair of parentheses, you’ll need to add a  character at the end of \\\\\\neach line, which adds visual noise to your program. PySpark code is\\nespecially prone to line breaks when you use method chaining.\\nAs a lazy alternative, I am a big fan of using Black as a Python code\\nformatting tool ( ). It removes a lot of the https://black.readthedocs.io/\\nguesswork of having your code logically laid-out and consistent. Since we\\nread code more than we write code, readability matters.\\nSince we are performing two actions on  (displaying the top 10 words on the screen and results\\nwriting the data frame to a csv file), we have to use a variable. If you only have 1 action to\\nperform on your data frame, you can channel your inner code golfer  by not using any variable5\\nname. Most of the time, I prefer lumping my transformations together and keep the action\\nvisually separate, like we are doing now.\\nOur program is looking much more polished now. The last step will be to add the PySpark’s\\nplumbing to prepare it for batch mode.\\nWhen we launched the  shell, we saw that the  variable was mapped to our pyspark spark\\n entry point, already configured for interactive work. When using batch submit, SparkSession\\nthis isn’t the case. In this section, I teach how to create your own entry point and submit the code\\nin batch mode. You will then be able to submit this program (and any properly coded PySpark\\nprogram).\\nBefore entering in the , let’s see what happens if we submit our program as is. In , we can how?\\nsee that PySpark replies immediately with a , saying that  isn’t defined. NameError spark\\nListing 3.9 Launching a PySpark program without  defined. spark3.5 Your first non-interactive program: using spark-submitresults = spark\\\\\\n          .read.text(\\'./data/ch02/1342-0.txt\\')\\\\\\n          ...\\n$ spark-submit word_count.py\\n[...]\\nTraceback (most recent call last):\\n  File \"/Users/jonathan_rioux/Dropbox/PySparkBook/code/Ch02/word_count.py\", line 3, in <module>\\n    results = (spark\\nNameError: name \\'spark\\' is not defined\\n[...]\\n49\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 52}), Document(page_content='Unlike the  command, Spark provides a single launcher for its programs in batch mode, pyspark\\ncalled . The simplest way to submit a program is to provide the program name as spark-submit\\nthe first parameter. As our programs grows in complexity, I will teach through Part 2 and 3 how\\nto augment the  with other parameters. spark-submit\\nIn Chapter 1, we learned that our main point of action is through an entry point which in our case\\nis a  object. This section covers how to create a simple bare-bone entry point soSparkSession\\nour program can run smoothly.\\nPySpark provides a builder pattern using the object . For those familiar SparkSession.builder\\nwith object-oriented programming, a builder pattern provides a set of methods to create a highly\\nconfigurable object without having multiple constructors. In this chapter, we will only look at the\\nhappiest case, but the  builder pattern will become increasingly useful in Part 3 as SparkSession\\nwe look into performance tuning and adding dependencies to our jobs.\\nIn , we start the builder pattern, and then then chain a configuration parameter which defined the\\napplication name. This isn’t absolutely necessary, but when monitoring your jobs (see Chapter\\n9), having a unique and well thought-out job name will make it easier to know what’s what. We\\nfinish the builder pattern with the  method to create our SparkSession. .getOrCreate()\\nNOTE You can’t have two  objects in your program working at the SparkSession\\nsame time. This is why the  method is called like this. If you getOrCreate()\\nwere to create a new entry point in the  shell, you’d get all kinds of pyspark\\nfunny errors. By using the  method, your program will work getOrCreate()\\nboth in interactive and batch mode.\\nListing 3.10 Creating our own simple SparkSession\\nWe saw at the beginning of how to submit a program using . Let’s try again, in , spark-submit\\nwith our properly configured entry point. The full code is available on the book’s repository,\\nunder .code/Ch02/word_count_submit.py3.6 Using  to launch your program in batch modespark-submit3.5.1 Creating your own SparkSession\\nfrom pyspark.sql import SparkSession\\nspark = (SparkSession.builder\\n                     .appName(\"Counting word occurences from a book.\")\\n                     .getOrCreate())\\n50\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 53}), Document(page_content='Listing 3.11 Submitting our job for real this time\\nTIP You get a deluge of \"INFO\" messages? Don’t forget that you have control\\nover this: use  right after your spark.sparkContext.setLogLevel(\"WARN\")\\n definition. If your local configuration has  as a default, you’ll still spark INFO\\nget a slew of messages until it catches this line, but it won’t obscure your\\nresults.\\nWith this, we’re done! Our program successfully  the book,  it into a cleaned ingests transforms\\nlist of word frequencies and then  it two ways: as a top-10 list on the screen and as a CSV exports\\nfile.\\nIf we look at our process, we applied one transformation interactively at the time, -ing the show()\\nprocess after each one. This will often be your  when working with a new data modus operandi\\nfile. Once you’re confident about a block of code, you can remove the intermediate variables.\\nPySpark gives you out of the box a productive environment to explore large data sets\\ninteractively and provides an expressive and terse vocabulary to manipulate data. It’s also easy to\\ngo from interactive development to batch deployment: you just have to define your \\n and you’re good to go. SparkSession\\nChapter 2 and 3 were pretty dense with information. We learned how to read text data, process it\\nto answer and question, display the results on the screen and write them to a CSV file. On the\\nother hand, there are many elements we left out on purpose. Let’s have a quick look at what we \\n do in this Chapter. didn’t\\nWith the exception of coalescing the data frame in order to write it into a single file, we didn’t\\ncare much for the distributing of the data. We saw in Chapter 1 that PySpark distributes data3.7 What didn’t happen in this Chapter$ spark-submit ./code/Ch02/word_count_submit.py\\n# [...]\\n# +----+-----+\\n# |word|count|\\n# +----+-----+\\n# | the| 4480|\\n# |  to| 4218|\\n# |  of| 3711|\\n# | and| 3504|\\n# | her| 2199|\\n# |   a| 1982|\\n# |  in| 1909|\\n# | was| 1838|\\n# |   i| 1749|\\n# | she| 1668|\\n# +----+-----+\\n# only showing top 10 rows\\n# [...]\\n51\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 54}), Document(page_content='across multiple workers nodes, but our code didn’t pay much attention to this. Not having to\\nconstantly think about partitions, data locality and fault tolerance made our data discovery\\nprocess much faster.\\nWe didn’t spend much time configuring PySpark. Beside providing a name for our application,\\nno additional configuration was inputted in our . It’s not to say we’ll never touch SparkSession\\nthis, but we can start with a bare-bone configuration and tweak as we go. Chapter 6 will expand\\ninto the subject.\\nFinally, we didn’t care much about the order of operations. We made a point to describe our\\ntransformations as logically as they appear to us, and we’re letting Spark’s optimize this into\\nefficient processing steps. We could potentially re-order some and get the same output, but our\\nprogram reads well, is easy to reason about and works well.\\nThis echoes the statement I made in Chapter 1: PySpark is remarkable by not only what it\\nprovides, but also what it can abstract over. You can write your code as a sequence of\\ntransformations that will get you to your destination most of the time. For those cases where you\\nwant more finely-tuned performance or more control about the physical layout of your data,\\nwe’ll see in Part 3 that PySpark won’t hold you back.\\nThat example wasn’t big data. I’ll be the first to say it.\\nTeaching big data processing has a catch 22. While I really want to show the power of PySpark\\nto work with massive data sets, I don’t want you to purchase a cluster or rack up a massive cloud\\nbill. It’s easier to show the ropes using a smaller set of data, knowing that we can scale using the\\nsame code.\\nLet’s take our word counting example: how can we scale this to a larger corpus of text? Let’s\\ndownload more files from Project Gutenberg and place them in the same directory.\\nWhile this is not enough to claim \"We’re doing Big Data(tm)\", it’ll be enough to explain the\\ngeneral concept. If you really want to scale, you can use Appendix C to provision a powerful\\ncluster on the cloud, download more books or other text files and run the same program for a few\\ndollars.3.8 Scaling up our word frequency program\\n$ ls -l data/Ch02\\n[...]\\n-rw-r--r--@ 1 jonathan_rioux  247087069   173595 Aug  4 15:03 11-0.txt\\n-rw-r--r--@ 1 jonathan_rioux  247087069   724726 Jul 30 17:30 1342-0.txt\\n-rw-r--r--@ 1 jonathan_rioux  247087069   607788 Aug  4 15:03 1661-0.txt\\n-rw-r--r--@ 1 jonathan_rioux  247087069  1276201 Aug  4 15:03 2701-0.txt\\n-rw-r--r--@ 1 jonathan_rioux  247087069  1076254 Aug  4 15:03 30254-0.txt\\n-rw-r--r--@ 1 jonathan_rioux  247087069   450783 Aug  4 15:03 84-0.txt\\n52\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 55}), Document(page_content=\"We will modify our  in a very subtle way. Where we , word_count_submit.py .read.text()\\nwe’ll just change the path to account for all files in the directory. shows the before and after: we\\nare just changing the  to a , which is called a . This will select all 1342-0.txt *.txt glob pattern\\nthe  files in the directory..txt\\nListing 3.12 Scaling our word count program\\nNOTE You can also just pass the name of the directory if you want PySpark to\\ningest all the files within the directory.\\nThe results of running the program over all the files in the directory are available in .\\nListing 3.13 Results of scaling our program to multiple files\\nWith this, you can confidently say that you are able to scale a simple data analysis program,\\nusing PySpark. You can use the general formula we’ve outlined here and modify some of the\\nparameters and methods to fit your use-case. Chapter 3 will dig a little deeper into some\\ninteresting and common data transformations, building on what we’ve learned here.# Before\\nresults = (spark\\n           .read.text('./data/ch02/1342-0.txt')\\n# After\\nresults = (spark\\n           .read.text('./data/ch02/*.txt')\\n$ spark-submit ./code/Ch02/word_count_submit.py\\n+----+-----+\\n|word|count|\\n+----+-----+\\n| the|38895|\\n| and|23919|\\n|  of|21199|\\n|  to|20526|\\n|   a|14464|\\n|   i|13973|\\n|  in|12777|\\n|that| 9623|\\n|  it| 9099|\\n| was| 8920|\\n+----+-----+\\nonly showing top 10 rows\\n53\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion\", metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 56}), Document(page_content='You can group records using the  method, passing the column names you want groupby\\nto group against as a parameter. This returns a  object that waits for an GroupedData\\naggregation method to return the results of a computation over the groups, such as the \\n of records. count()\\nPySpark’s repertoire of functions that operate on columns are located in \\n. The unofficial but well respected convention is to qualify this pyspark.sql.functions\\nimport in your program using the  keyword. F\\nWhen writing a data frame to a file, PySpark will create a directory and put one file per\\npartition. If you want to write a single file, use the  method. coaslesce(1)\\nIn order to prepare your program to work in batch mode via , you need to spark-submit\\ncreate a . PySpark provides a builder pattern in the  module. SparkSession pyspark.sql\\nIf your program needs to scale across multiple files within the same directory, you can\\nuse a glob pattern to select many files at once. PySpark will collect them in a single data\\nframe.\\nFor this exercise, you’ll need the  program we worked on this Chapter. word_count_submit.py\\nYou can pick it from the book’s code repository ( ) Code/Ch03/word_count_submit.py\\na) Modifying the  program, return the number of distinct words in Jane word_count_submit.py\\nAusten’s . (Hint,  contains 1 record for each unique word…\\u200b) Pride and Prejudice results\\nb) (Challenge) Wrap your program in a function that takes a file name as a parameter. It should\\nreturn the number of distinct words.\\nTaking , modify the script to return a sample of 20 words that appear word_count_submit.py\\nonly once in Jane Austen’s . Pride and Prejudice\\na) Using the  function (refer to PySpark’s API or the  shell help if needed), substr pyspark\\nreturn the top 5 most popular first letters (keep only the first letter of each word).\\nb) Compute the number of words starting with a consonant or a vowel. (Hint: the isin()\\nfunction might be useful)\\nLet’s say you want to get both the  and  of a  object. Why doesn’t count() sum() GroupedData\\nthis code work? Map the inputs and outputs of each method.3.9 Summary\\n3.10 Exercises\\n3.10.1 Exercise 3.1\\n3.10.2 Exercise 3.2\\n3.10.3 Exercise 3.3\\n3.10.4 Exercise 3.4\\n54\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 57}), Document(page_content='Multiple aggregate function application will be covered in Chapter 4.my_data_frame.groupby(\"my_column\").count().sum()\\n55\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 58}), Document(page_content='4\\nThis chapter covers\\nSo far, in chapters 2 and 3, we’ve dealt with textual data, which is . Through a chain unstructured\\nof transformations, we extracted some information to get the most common words in the text.\\nThis chapter will go a little deeper into data manipulation using  data, which is data structured\\nthat follow a set format. More specifically, we will work with  data, which follows the tabular\\nclassical rows and columns layout. Just like the two previous chapters, we’ll take a data set and\\nanswer a simple question by exploring and processing the data.\\nWe’ll use some public Canadian television schedule data to identify and measure the proportion\\nof commercials over the total programming. The data used is typical of what you see from\\nmainstream relational databases. This chapter and the next builds heavily on chapter 2 and 3, and\\nadd additional methods and information to use the data frame as a tabular data container. We\\nperform some data exploration, assembly, and cleaning, using the  data pyspark.sql\\nmanipulation module, and we finish by answering some questions hidden in our data set. The\\nexercises will at the end of the chapter give you the opportunity to craft your own data\\nmanipulation code.\\nThe initialization part of our Spark program (relevant imports and  creation) is SparkSession\\nprovided in .Analyzing tabular data with pyspark.sql\\nReading delimited data into a PySpark data frame\\nUnderstanding how PySpark represents tabular data in a data frame\\nIngesting and exploring tabular or relational data\\nSelecting, manipulating, renaming and deleting columns in a data frame\\nSummarizing data frames for quick exploration\\n56\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 59}), Document(page_content='Listing 4.1 Relevant imports and scaffolding for this chapter\\nTabular data is data that we can typically represent in a 2-dimensional table. You have rows and\\ncolumns containing a single (or ) value. A good example would be your grocery list: you simple\\nmay have one column for the item you wish to purchase, one for the quantity, and one for the\\nexpected price. provides an example of a small grocery list. We have the three columns\\nmentioned, as well as four rows, each representing an entry in our grocery list.\\nFigure 4.1 My grocery list represented as tabular data. Each row represents an item, and\\neach column represents an attribute.\\nThe easiest analogy we can make for tabular data is the spreadsheet format: the interface\\nprovides you with a large number of rows and columns where you can input and perform\\ncomputation on data. SQL databases, even if they use a different vocabulary, can also be thought\\nof as tables made up of rows and columns. Tabular data is an extremely common data format,\\nand because it’s so popular and easy to reason about, it makes for a perfect first dive into\\nPySpark’s data manipulation API.\\nPySpark’s data frame structure maps very naturally to tabular data. In chapter 2, I explain that\\nPySpark operates either on the whole data frame structure (via methods such as  and select()\\n) of on  objects (for instance when using a function like ). The data groupby() Column split()\\nframe is , so its API focuses on manipulating the columns to transform the data. column-major\\nBecause of this, we can simplify how we reason about data transformations by thinking about\\nwhat operations to do and which columns will be impacted.4.1 What is tabular data?import os\\nimport numpy as np\\nfrom pyspark.sql import SparkSession\\nimport pyspark.sql.functions as F\\nspark = SparkSession.builder.getOrCreate()\\n57\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 60}), Document(page_content='NOTE The resilient distributed dataset, briefly introduced in chapter 1, is a good\\nexample of a structure that is . Instead of thinking about columns, row-major\\nyou are thinking about items with attributes in which you apply functions.\\nIt’s an alternative way of thinking about your data, and chapter 8 contains a\\nlot more information about where it can be useful.\\nIn chapter 2, our data frame always contained a single column, up until the very end where we\\ncounted the occurrence of each word. In other words, we took  data (a body of text), unstructured\\nperformed some transformations, and created a two-column table containing the information we\\nwanted. Tabular data is in a way an extension of this, where we have more than one column to\\nwork with.\\nLet’s take my very healthy grocery list as an example, and load it into PySpark. To make things\\nsimple, we’ll encode our grocery list into a list of lists. PySpark has multiple ways to import\\ntabular data, but the two most popular are the list of lists and the pandas data frame. I cover\\nbriefly how to work with Pandas in chapter 8 and appendix D contains more information about\\nhow to use pandas. Considering the size of our grocery list, it would be a little overkill to import\\na library just for loading 4 records, so I kept it in a list of lists.\\nListing 4.2 Creating a data frame out of our grocery list\\nMy grocery list is encoded in a list of lists.\\nPySpark automatically inferred the type of each field from the information Python\\nhad about each value.\\nWe can easily create a data frame from data in our program with the spark.createDataFrame\\nfunction, as shows. Our first parameter is the data itself. You can either provide a list of items\\n(here a list of lists), a pandas data frame or a Resilient Distributed Dataset, which I cover in\\nchapter 9. The second parameter is the  of the data frame. chapter 6 covers the automatic schema\\nand manual schema definitions in greater depth. In the meantime, passing a list of column names4.1.1 How does PySpark represent tabular data?\\nmy_grocery_list = [\\n    [\"Banana\", 2, 1.74],\\n    [\"Apple\", 4, 2.04],\\n    [\"Carrot\", 1, 1.09],\\n    [\"Cake\", 1, 10.99],\\n]  \\ndf_grocery_list = spark.createDataFrame(my_grocery_list, [\"Item\", \"Quantity\", \"Price\"])\\ndf_grocery_list.printSchema()\\n# root\\n#  |-- Item: string (nullable = true)   \\n#  |-- Quantity: long (nullable = true) \\n#  |-- Price: double (nullable = true)  \\n58\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 61}), Document(page_content='will make PySpark happy, while it infers the types ( , , and , respectively) of string long double\\nour columns. Visually, the data frame will look like , although much more simplified. The master\\nnode knows about the structure of the data frame, but the actual data is represented on the worker\\nnodes. Each column maps to data stored somewhere on our cluster, managed by PySpark. We\\noperate on the abstract structure and let the master delegate the work efficiently.\\nFigure 4.2 Each column of our data frame maps to some place on our worker nodes.\\nPySpark gladly represented our tabular data using our column definitions. This means that all the\\nfunctions we learned so far apply to our tabular data. By having one flexible structure for many\\ndata representations— we’ve done text and tabular so far— PySpark makes it easy to move from\\none domain to another. It removes the need to learn yet another set of functions and a whole new\\nabstraction for our data.\\nMy grocery list was fun, but the potential for analysis work is pretty limited. We’ll get our hands\\non a larger data set, explore it, and ask a few introductory questions that we might find\\ninteresting. This process is called  (or EDA) and is usually the first step exploratory data analysis\\ndata analysts and scientists undertake when placed in front of new data. Our goal is to get\\nfamiliar with the data discovery functions and methods as well as performing some basic data\\nassembly. Being familiar with those steps will remove the awkwardness of working with data\\nyou won’t see transforming before your eyes. Until we can process visually millions of records\\nper second, this chapter will show you a blueprint you can re-use when facing new data frames.4.2 PySpark for analyzing and processing tabular data\\n59\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 62}), Document(page_content='SIDEBAR Graphical exploratory data analysis?\\nA lot of the EDA work you’ll see in the wild incorporates charts and/or\\ntables. Does it mean that PySpark has the option to do the same?\\nWe saw in chapter 2 how to pretty print a data frame so we can view\\nthe content at a glance. This still applies for summarizing information and\\ndisplaying it on the screen. If you want to export the table in an easy to\\nprocess format (to incorporate in a report, for instance), you can use \\n, making sure you coalesce the data frame in a single file. spark.write.csv\\n(See chapter 3 for a refresher on .) By its very nature, table coalesce()\\nsummaries won’t be very huge so you won’t risk running out of memory.\\nPySpark doesn’t provide any charting capabilities and doesn’t play with\\nother charting libraries (like matplotlib, seaborn, altair, or plot.ly). Taking a\\nstep back, it makes a lot of sense: PySpark distributes your data over many\\ncomputers. It doesn’t make much sense to distribute a chart creation. The\\nusual solution will be to transform your data using PySpark, use the \\n method to transform your PySpark data frame into a pandas toPandas()\\ndata frame, and then use your favourite charting library. When using\\ncharts, I provide the code I used to generate them, but instead of\\nexplaining the process each time, I provide explanations— as well as a\\nprimer in using pandas with PySpark— in Appendix D.\\nFor this exercise, we’ll use some open data from the Government of Canada, more specifically\\nthe CRTC (Canadian Radio-television and Telecommunications Commission). Every broadcaster\\nis mandated to provide a complete log of the programs, commercials and all, showcased to the\\nCanadian public. This gives us a lot of potential questions to answer, but we’ll select one specific\\none: what are the channels with the most and least proportion of commercials?\\nYou can download the file on the Canada Open Data portal (\\n), selecting the https://open.canada.ca/data/en/dataset/800106c1-0b08-401e-8be2-ac45d62e662e\\n file. The file is a whopping 994MB to download, which might be BroadcastLogs_2018_Q3_M8\\na little too large for some people. The book’s repository contains a sample of the data under the \\n directory, which you can use in lieu of the original file. You’ll also need to download data/Ch04\\nthe \"Data Dictionary\" in DOC form, as well as the \"Reference Tables\" zip file, unzipping them\\ninto a \"ReferenceTables\" directory in . Once again, the examples are assuming that data/Ch04\\nthe data is downloaded under  and that PySpark is launched from . data/Ch04 src/Ch04/\\nThis section is dedicated to ingesting delimited data in a PySpark data frame, so we can start\\nmanipulating it. I will cover how to use a specialized reader object for delimited data, the most\\ncommon parameters to set, and how to identify frequent patterns when looking at tabular data.4.3 Reading delimited data in PySpark\\n60\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 63}), Document(page_content='1.  \\n2.  Delimited data is a very common, popular, and tricky way of sharing data. In a nutshell, the data\\nis sitting verbatim in a file, separated by two types of delimiters. A visual sample of delimited\\ndata is depicted in .\\nThe first one is a . The row delimiter splits the file in logical records. There row delimiter\\nis one and only one record between delimiters.\\nThe second one is a . Each record is made up of an identical number of field delimiter\\nfields and the field delimiter tells where one field starts and ends.\\nFigure 4.3 A sample of our data, highlighting the field delimiter (|) and row delimiter (\\\\n)\\nThe newline character (  when depicted explicitly) is the de-facto record delimiter. It naturally \\\\n\\nbreaks down the file into visual lines, where one record starts at the beginning of the line and\\nends, well, at the end. The comma character  is the most frequent field delimiter. It’s so ,\\nprevalent that most people call delimited data files \"CSV\", which stands for \"Comma Separated\\nValues\".\\nCSV files are easy to produce and have a loose set of rules to follow to be considered usable.\\nBecause of this, PySpark provides a whopping 25 optional parameters when ingesting a CSV.\\nCompare this to the two for reading text data. In , I use three configuration parameters. This is\\nenough to parse our data into a data frame.\\nListing 4.3 Reading our broadcasting information\\nWe specify the file path where our data resides first\\nOur file uses a vertical bar, so we pass  as a parameter to | sep\\n takes a boolean. When , the first row of your file is parsed as the header true\\ncolumn names.\\n takes a boolean as well. When , it’ll pre-parse the data to infer inferSchema true\\nthe type of the column.DIRECTORY = \"../../data/Ch04\"\\nlogs = spark.read.csv(\\n    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8.CSV\"),  \\n    sep=\"|\",  \\n    header=True,  \\n    inferSchema=True,  \\n)\\n61\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 64}), Document(page_content='The next section expands on the most important parameters when reading CSV data and provides\\nmore detailed explanations behind the code in .\\nThis section focuses on how we can specialize the  object to read delimited data SparkReader\\nand what are the most popular configuration parameters to accommodate the various declinations\\nof CSV.\\nReading delimited data can be a dicey business. Because of how flexible and human editable the\\nformat is, a CSV reader needs to provide many options to cover the many use-cases possible.\\nThere is also a risk that the file is malformed, in which case you will need to treat it as text and\\ngingerly infer the fields manually. I will stay on the happy path and cover the most popular\\nscenario: a single file, properly delimited.\\nJust like when reading text, the only truly mandatory parameter is the , which contains the path\\nfile or files path. As we saw in chapter 2, you can use a glob pattern to read multiple files inside\\na given directory, as long as they have the same structure. You can also explicitly pass a list of\\nfile paths if you want specific files to be read.\\nThe most common variation you’ll encounter when ingesting and producing CSV file is selecting\\nthe right delimiter. The comma is the most popular, but it suffers from being a popular character\\nin text, which means you need a way to differentiate which commas are part of the text and\\nwhich oner are delimiters. Our file use the vertical bar character, an apt choice: it’s easily\\nreachable on the keyboard yet infrequent in text.\\nNOTE In French, we use the comma for separating numbers between their\\nintegral part and their decimal one (e.g. ). This is pretty awful 1.02  1,02\\nwhen in a CSV file, so most French CSV will use the semicolon ( ) as a ;\\nfield delimiter. This is one more example of why you need to be vigilant\\nwhen using CSV.\\nWhen reading CSV data, PySpark will default to using the comma character as a field delimiter.\\nYou can set the optional parameter  (for separator) to the single character you want to use as sep\\na field delimiter.4.3.1 Customizing the  object to read CSV data files SparkReader\\nTHE PATH TO THE FILE YOU WANT TO READ AS THE ONLY MANDATORY\\nPARAMETER\\nPASSING AN EXPLICIT FIELD DELIMITER WITH THE  PARAMETER SEP\\n62\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 65}), Document(page_content='When working with CSV that use the comma as a delimiter, it’s common practice to  the quote\\ntext fields to make sure any comma in the text is not mistaken as a field separator. The CSV\\nreader object provides an optional  parameter that defaults to the double-quote character . quote \"\\nSince I am not passing an explicit value to , we are keeping the default value. This way, quote\\nwe can have a field with the value , whereas we would consider this to be two \"Three | Trois\"\\nfields without the quotation character.\\nIf we don’t want to use any character as a quote, we need to pass explicitly the empty string to \\n.quote\\nThe  optional parameter takes a Boolean flag. If set to true, it’ll use the first row of yourheader\\nfile (or files, if you’re ingesting many) and use it to set your column names.\\nYou also can pass a list of strings as the  optional parameter if you wish to explicitly schema\\nname your columns. If you don’t fill any of those two, your data frame will have  for column _c*\\nnames, where the star is replaced with increasing integers ( , , …\\u200b). _c0 _c1\\nPySpark has a schema discovering capacity. You turn it on by setting  to  (by inferSchema True\\ndefault, this is turned off). This optional parameter forces PySpark to go over the ingested data:\\none time to set the types of each column, one time to ingest the data. This makes the ingestion\\nquite a bit longer but avoids us to write the schema by hand (I go down to this level of detail in\\nchapter 6). Let the machine do the work!\\nWe are lucky enough that the Government of Canada is a good steward of data, and provides us\\nwith clean, properly formatted files. In the wild, malformed CSV files are legion and you will\\nrun into some errors when trying to ingest some of them. Furthermore, if your data is large, you\\noften won’t get the chance to inspect each row one by one to fix mistakes. Chapter 9 covers\\nsome strategies to ease the pain and also shows you some ways to share your data with the\\nschema included.\\nOur data frame schema, displayed on , is coherent with the documentation we’ve downloaded.\\nThe column names are properly displayed and the types make sense. That’s plenty enough to get\\nstarted with some exploration.QUOTING TEXT TO AVOID MISTAKING A CHARACTER FOR A DELIMITER\\nUSING THE FIRST ROW AS THE COLUMN NAMES\\nINFERRING WHAT A COLUMN TYPE WHILE READING THE DATA\\n63\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 66}), Document(page_content='Listing 4.4 The schema of our  data frame logs\\nSIDEBAR Exercise 4.1\\nLets take the following file, called . sample.csv\\nComplete the following code to ingest the file successfully.logs.printSchema()\\n# root\\n#  |-- BroadcastLogID: integer (nullable = true)\\n#  |-- LogServiceID: integer (nullable = true)\\n#  |-- LogDate: timestamp (nullable = true)\\n#  |-- SequenceNO: integer (nullable = true)\\n#  |-- AudienceTargetAgeID: integer (nullable = true)\\n#  |-- AudienceTargetEthnicID: integer (nullable = true)\\n#  |-- CategoryID: integer (nullable = true)\\n#  |-- ClosedCaptionID: integer (nullable = true)\\n#  |-- CountryOfOriginID: integer (nullable = true)\\n#  |-- DubDramaCreditID: integer (nullable = true)\\n#  |-- EthnicProgramID: integer (nullable = true)\\n#  |-- ProductionSourceID: integer (nullable = true)\\n#  |-- ProgramClassID: integer (nullable = true)\\n#  |-- FilmClassificationID: integer (nullable = true)\\n#  |-- ExhibitionID: integer (nullable = true)\\n#  |-- Duration: string (nullable = true)\\n#  |-- EndTime: string (nullable = true)\\n#  |-- LogEntryDate: timestamp (nullable = true)\\n#  |-- ProductionNO: string (nullable = true)\\n#  |-- ProgramTitle: string (nullable = true)\\n#  |-- StartTime: string (nullable = true)\\n#  |-- Subtitle: string (nullable = true)\\n#  |-- NetworkAffiliationID: integer (nullable = true)\\n#  |-- SpecialAttentionID: integer (nullable = true)\\n#  |-- BroadcastOriginPointID: integer (nullable = true)\\n#  |-- CompositionID: integer (nullable = true)\\n#  |-- Producer1: string (nullable = true)\\n#  |-- Producer2: string (nullable = true)\\n#  |-- Language1: integer (nullable = true)\\n#  |-- Language2: integer (nullable = true)\\nItem,Quantity,Price\\n$Banana, organic$,1,0.99\\nPear,7,1.24\\n$Cake, chocolate$,1,14.50\\nsample = spark.read.csv([...],\\n                        sep=[...],\\n                        header=[...],\\n                        quote=[...],\\n                        inferSchema=[...]\\n)\\n64\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 67}), Document(page_content='SIDEBAR Exercise 4.2\\nRe-read the data in a  data frame, taking inspiration from the code logs_raw\\nin , this time without passing any optional parameters. Print the first 5 rows\\nof data, as well as the schema. What are the differences in terms of data\\nand schema between  and ? logs logs_raw\\nWhen working with tabular data, especially if it comes from a SQL data warehouse, you’ll often\\nfind that the data set is split between tables. In our case, our logs table contains a majority of\\nfields suffixed by ; those IDs are listed in other tables and we have to  them to get the ID link\\nlegend of those IDs. This section introduces briefly what a star schema is, why they are so\\nfrequently encountered, and how we can represent them visually to work with them.\\nOur data universe (the set of tables we are working with) follows a very common pattern in\\nrelational databases: a center table containing a bunch of IDs (or ) and some ancillary tables keys\\naround containing a legend between each key and its value. This is called a  since it star schema\\nvisually looks like a star. Star schemas are common in the relational database world because of \\n, a process used to avoid duplicating pieces of data and improve data integrity. normalization\\nData normalization is illustrated in , where our centre table  contain IDs that maps to the logs\\nauxiliary tables around called . In the case of the  link table, it contains link tables CD_Category\\nmany fields (such as  and ) that are made available to Category_CD English_description logs\\nwhen you link the two tables with the  key. Category_ID4.3.2 Exploring the shape of our data universe\\n65\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 68}), Document(page_content='Figure 4.4 The logs table \"ID\" columns map to other tables, like the CD_category table\\nwhich links the Category_ID field.\\nIn Spark’s universe, we often prefer working with a single table instead of linking a multitude of\\ntables to get the data. We call them  tables, or colloquially  tables. We will start denormalized fat\\nby assessing the data available to us directly in the  table before plumping our table, a topic logs\\nI cover in chapter 5. By looking at the  table, its content, and the data documentation, we logs\\nwill avoid linking tables that contain data with no real value for our analysis.\\n66\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 69}), Document(page_content='SIDEBAR The right structure for the right work\\nNormalization, denormalization, what gives? Isn’t this a book about data\\nanalysis?\\nWhile this book isn’t about data architecture, it’s important to\\nunderstand, at least a little bit, how data might be structured so we can\\nwork with it. Normalized data has many advantages when you’re working\\nwith relational information (such as our broadcast tables). Besides being\\neasier to maintain, data normalization reduces the probability of getting\\nanomalies or illogical records in your data.\\nWhen dealing with analytics, a single table containing all the data is\\nbest. However, having to link the data by hand can be tedious, especially\\nwhen working with dozens or even hundreds of link tables. Fortunately,\\ndata warehouses don’t change their structure very often. If you’re faced\\nwith a complex star schema one day, befriend one of the database\\nmanagers. There is a very good chance that they’ll provide you with the\\ninformation to denormalize the tables, most often in SQL, and chapter 7 will\\nshow you how you can adapt the code into PySpark with a minimum of\\nefforts.\\nIt is common practice to explore and summarize the data when you first get acquainted with it.\\nIt’s just like a first date with your data: you want a good overview, not agonize on the details.6\\nThis section shows the most common manipulations done on a data frame in greater detail. I\\nshow how you can select, delete, rename, re-order, and create columns so you can customize\\nhow a data frame is shown. I also cover summarizing a data frame, so you can have a quick\\ndiagnostic overview of the data inside your structure. No flowers required.\\nSo far, we’ve learned that typing our data frame variable into the shell prints the structure of the\\ndata frame, not the data, unless you’re using eagerly evaluated Spark (referenced in chapter 2).\\nWe can also use the  command to display a few records for exploration. I won’t show the show()\\nresults, but if you try it, you’ll see that the table-esque output is garbled, because we are showing\\ntoo many columns at once. Time to  our way to sanity. select()\\nAt its simplest,  can take one or more column objects— or strings representing column select()\\nnames— and will return a data frame containing only the listed columns. This way, we can keep\\nour exploration tidy and check a few columns at the time. An example is displayed in .4.4 The basics of data manipulation: diagnosing our centre table\\n4.4.1 Knowing what we want: selecting columns\\n67\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 70}), Document(page_content='Listing 4.5 Selecting 5 rows of the first 3 columns of our data frame\\nIn chapter 2, you learn that  shows 5 rows without truncating their .show(5, False)\\nrepresentation so we can show the whole content. The  statement is where the magic .select()\\nhappens.\\nEach data frame created keeps a list of column names in its  attribute, and we can slice it columns\\nto pick a subset of columns to see. We’re using the implicit string to column conversion PySpark\\nprovides to avoid some boilerplate.\\nselect() , takes a single parameter, named . This star is used in Python for unpacking *cols\\ncollections, or in our case to illustrate that the function takes a variable number of parameters\\nthat will be collected under the  variable. From a PySpark perspective, the four statements cols\\nin are interpreted the same. Note how prefixing the list with a star removed the container so each\\nelement becomes a parameter of the function. If this looks a little confusing to you, fear not!\\nAppendix D will provide you with a good overview of its usage.\\nListing 4.6 Four ways to select colums in PySpark, all equivalent in term of\\nresults\\nWhen explicitly selecting a few columns, you don’t have to wrap them into a list. If you’re\\nalready working on a list of columns, you can unpack them with a star prefix. This argument\\nunpacking pattern is worth remembering as many other data frame methods taking columns as\\ninput are using the same approach.\\nIn the spirit of being clever (or lazy), let’s expand our selection code to see every column in\\ngroups of three. This will give us a sense of the content. Since  is a Python list, logs.columns\\nwe can use a function on it without any problem. The code in shows one of the ways we can do\\nit.logs.select(*logs.columns[:3]).show(5, False)\\n# +--------------+------------+-------------------+\\n# |BroadcastLogID|LogServiceID|LogDate            |\\n# +--------------+------------+-------------------+\\n# |1196192316    |3157        |2018-08-01 00:00:00|\\n# |1196192317    |3157        |2018-08-01 00:00:00|\\n# |1196192318    |3157        |2018-08-01 00:00:00|\\n# |1196192319    |3157        |2018-08-01 00:00:00|\\n# |1196192320    |3157        |2018-08-01 00:00:00|\\n# +--------------+------------+-------------------+\\n# only showing top 5 rows\\n# Using the string to column conversion\\nlogs.select(\"BroadCastLogID\", \"LogServiceID\", \"LogDate\")\\nlogs.select(*[\"BroadCastLogID\", \"LogServiceID\", \"LogDate\"])\\n# Passing the column object explicitly\\nlogs.select(F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\"))\\nlogs.select(*[F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\")])\\n68\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 71}), Document(page_content=\"Listing 4.7 Peeking at the data frame in chunks of 3 columns\\nThe  function comes from the  package, imported as  atsplit_array() numpy np\\nthe beginning of this chapter.\\nLet’s take each line one at a time.\\nWe start by splitting the  list into approximate groups of 3. To do so, we rely on a logs.columns\\nfunction from the  package called . The function takes an array and a numpy array_split()\\nnumber of desired sub-arrays  and returns a list of  sub-arrays. We wrap our list of columns N N\\n into an array via the  function and pass this as a first parameter. For the logs.columns np.array\\nnumber of sub-arrays, we divide the number of columns by 3, using an integer division . //\\nTIP To be perfectly honest, the call to  can be eschewed since np.array\\n can work on lists. I am still using it since if you are using np.array_split()\\na static type checker, such as mypy, you’ll get a type error. Chapter 8 has a\\nbasic introduction to type checking in Python and Appendix D provides a\\nlittle more guidance on how to type check your code.\\nThe last part of iterates over the list of sub-arrays, using  so select the columns present select()\\ninside each sub-array and  to display them on the screen. show()\\nThis example shows how easy it is to blend Python code with PySpark. On top of providing a\\ntrove of functions, the data frame API also exposes information, such as column names, into\\nconvenient Python structures. I won’t avoid using functionality from libraries when it makes\\nsense, but like in , I’ll do my best to explain what it does and why we’re using it. Chapter 8 goes\\nbeyond on the subject of how you can further combine pure Python code in PySpark.column_split = np.array_split(np.array(logs.columns), len(logs.columns) // 3)  \\nprint(column_split)\\n# [array(['BroadcastLogID', 'LogServiceID', 'LogDate'], dtype='<U22'),\\n#  [...]\\n#  array(['Producer2', 'Language1', 'Language2'], dtype='<U22')]'\\nfor x in column_split:\\n    logs.select(*x).show(5, False)\\n# +--------------+------------+-------------------+\\n# |BroadcastLogID|LogServiceID|LogDate            |\\n# +--------------+------------+-------------------+\\n# |1196192316    |3157        |2018-08-01 00:00:00|\\n# |1196192317    |3157        |2018-08-01 00:00:00|\\n# |1196192318    |3157        |2018-08-01 00:00:00|\\n# |1196192319    |3157        |2018-08-01 00:00:00|\\n# |1196192320    |3157        |2018-08-01 00:00:00|\\n# +--------------+------------+-------------------+\\n# only showing top 5 rows\\n# ... and more tables of 3 columns\\n69\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion\", metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 72}), Document(page_content='In this section, we used the  method to rapidly peek at a sample of our data. Because of select()\\nthe width of our data frame, we split our columns into manageable sets of three to keep the\\noutput tidy on the screen. I use this pattern frequently to have a high-level view of what my data\\nframe contains. Next, we do the opposite: specifying what we do not want to keep.\\nThe other side of selecting columns is choosing what not to select. We could do the full trip with \\n, carefully crafting our list of columns to keep only the one we want. Fortunately, select()\\nPySpark also provides a shorter trip: just drop what you don’t want.\\nIn our current data frame, let’s get rid of two columns in the spirit of tidying up. Hopefully, it\\nwill bring us joy.\\nBroadCastLogID  is the primary key of the table and will serve us no use in answering\\nour questions.\\nSequenceNo  is a sequence number and won’t be useful either.\\nMore will come off later when we start looking at the link tables. The code in does the trick very\\nsimply.\\nListing 4.8 Getting rid of columns using the  method drop()\\nJust like ,  takes a  and returns a data frame, this time excluding the select() drop() *cols\\ncolumns passed as parameters. Just like every other method in PySpark,  returns a new drop()\\ndata frame, so we overwrite our  variable by assigning the result of our code. logs\\nWARNING Unlike , where selecting a column that doesn’t exist will return a select()\\nruntime error, dropping a non-existent column is a no-op. PySpark will just\\nignore the columns it doesn’t find. Careful with the spelling of your column\\nnames!\\nDepending on how many columns you want to preserve, select might be a neater way to keep\\njust what you want. We can see  and  as being two sides of the same coin: one drop() select()\\ndrops what you specify, the other one keeps what you specify. We could reproduce with a \\n method, and does just that. select()4.4.2 Keeping what we need: deleting columns\\nlogs = logs.drop(\"BroadcastLogID\", \"SequenceNO\")\\n# Testing if we effectively got rid of the columns\\nprint(\"BroadcastLogID\" in logs.columns)  # => False\\nprint(\"SequenceNo\" in logs.columns)  # => False\\n70\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 73}), Document(page_content='Listing 4.9 Getting rid of columns, select-style\\nSIDEBAR Advanced topic: An unfortunate inconsistency\\nIn theory, you can also  columns with a list without unpacking it. select()\\nThis code will work as expected.\\nThis is not the case for , where you need to explicitly unpack. drop()\\nI’d rather unpack explicitly and avoid the cognitive load of remembering\\nwhen it’s mandatory and when it’s optional.\\nYou now know the most fundamental operations to perform on a data frame. You can select and\\ndrop columns, and with the flexibility of  presented in chapters 2 and 3, you can apply select()\\nfunctions on existing columns to transform them. The next section will cover how you can create\\nnew columns without having to rely on , simplifying your code, and improving its select()\\nresiliency.\\nSIDEBAR Exercise 4.3\\nCreate a new data frame  that contains only the colunms that logs_clean\\ndo not end with . IDlogs = logs.select(\\n    *[x for x in logs.columns if x not in [\"BroadcastLogID\", \"SequenceNO\"]]\\n)\\nlogs = logs.select(\\n    [x for x in logs.columns if x not in [\"BroadcastLogID\", \"SequenceNO\"]]\\n)\\nlogs.drop(logs.columns[:])\\n# TypeError: col should be a string or a Column\\nlogs.drop(*logs.columns[:])\\n# DataFrame[]\\n71\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 74}), Document(page_content='1.  \\n2.  \\n3.  \\n4.  \\n5.  SIDEBAR Exercise 4.4\\nWhat is the printed result of this code?\\n[\\'item\\' \\'UPC\\']\\n[\\'item\\', \\'upc\\']\\n[\\'price\\', \\'quantity\\']\\n[\\'price\\', \\'quantity\\', \\'UPC\\']\\nRaises an error\\nCreating new columns is such a basic operation that it seems a little far-fetched to rely on \\n. It also puts a lot of pressure on code readability: for instance using  makes it select() drop()\\nobvious we’re removing some columns. It would be nice to have something that signals we’re\\ncreating a new column. PySpark named this function . withColumn()\\nBefore going crazy with column creation, let’s take a simple example, build what we need\\niteratively and then move them to . Let’s take the  column, containing withColumn() Duration\\nthe length of each program shown.\\nPySpark doesn’t have a default type for time without dates or duration, so it kept the column as a\\nstring. We verified the exact type via the  attribute, which returns both the name and type dtypes\\nof a data frame’s columns. A string is a safe and reasonable option, but this isn’t remarkably\\nuseful for our purpose. Thanks to our peeking, we can see that the string is formatted like \\n, where HH:MM:SS.mmmmmmsample_frame.columns\\n# [\\'item\\', \\'price\\', \\'quantity\\', \\'UPC\\']\\nprint(sample_frame.drop(\\'item\\', \\'UPC\\', \\'prices\\').columns)\\n4.4.3 Creating what’s not there: new columns with withColumn()\\nlogs.select(F.col(\"Duration\")).show(5)\\n# +----------------+\\n# |        Duration|\\n# +----------------+\\n# |02:00:00.0000000|\\n# |00:00:30.0000000|\\n# |00:00:15.0000000|\\n# |00:00:15.0000000|\\n# |00:00:15.0000000|\\n# +----------------+\\n# only showing top 5 rows\\nprint(logs.select(F.col(\"Duration\")).dtypes)  \\n# [(\\'Duration\\', \\'string\\')]\\n72\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 75}), Document(page_content='HH is the duration in hours\\nMM is the duration in minutes\\nSS is the duration in seconds\\nmmmmmmm is the duration in milliseconds\\nI ignore the duration in milliseconds since I don’t think it’ll make a huge difference. In , we are\\nextracting the three other sub-fields.\\nListing 4.10 Extracting the hours, minutes and seconds from the  column Duration\\nThe original column, for sanity\\nThe first two characters are the hours\\nThe fourth and fifth characters are the minutes\\nThe seventh and eighth characters are the seconds\\nTo avoid seeing identical rows, I’ve added a  to the results distinct()\\nThe  method takes two parameters. The first gives the position of where the sub-stringsubstr()\\nstarts, the first character being , not  like in Python. The second gives the length of the 1 0\\nsub-string we want to extract in number of characters. Following the application of , substr()\\nwe then cast the result as  (integer) using the  method so we can treat them as integers. int cast\\nCasting is a very common and important operation and is covered in more detail in chapter 6. We\\nfinally provided an alias for each column so we know easily which one is which.\\nI used before  the  method, which de-dupes the data frame. This is explained show() distinct()\\nfurther in chapter 5. I added  to avoid seeing identical occurrences that would distinct()\\nprovide no additional information when displayed.\\nI think that we’re in good shape! Let’s merge all those values into a single field: the duration of\\nthe program in seconds. PySpark can perform arithmetic with column objects using the same\\noperators as Python, so this will be a breeze! The code in takes the code forming the additionallogs.select(\\n    F.col(\"Duration\"),  \\n    F.col(\"Duration\").substr(1, 2).cast(\"int\").alias(\"dur_hours\"),  \\n    F.col(\"Duration\").substr(4, 2).cast(\"int\").alias(\"dur_minutes\"),  \\n    F.col(\"Duration\").substr(7, 2).cast(\"int\").alias(\"dur_seconds\"),  \\n).distinct().show(  \\n    5\\n)\\n# +----------------+---------+-----------+-----------+\\n# |        Duration|dur_hours|dur_minutes|dur_seconds|\\n# +----------------+---------+-----------+-----------+\\n# |00:10:06.0000000|        0|         10|          6|\\n# |00:10:37.0000000|        0|         10|         37|\\n# |00:04:52.0000000|        0|          4|         52|\\n# |00:26:41.0000000|        0|         26|         41|\\n# |00:08:18.0000000|        0|          8|         18|\\n# +----------------+---------+-----------+-----------+\\n# only showing top 5 rows\\n73\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 76}), Document(page_content='columns in and use it in the definition of a single column.\\nListing 4.11 Creating a duration in second field from the  column Duration\\nWe kept the same definitions, removed the alias, and performed arithmetic directly on the\\ncolumns. There are 60 seconds in a minute, and 60 * 60 seconds in an hour. PySpark respects\\noperator precedence, so we don’t have to clobber our equation with parentheses. Overall, our\\ncode is quite easy to follow and we are ready to add our column to our data frame.\\nInstead of using  on all the columns  our new one, let’s use . select() plus withColumn()\\nApplied to a data frame, it’ll return a data frame with the new column appended. takes our field\\nand add it to our  data frame. I also include a sample of the  so you can see logs printSchema()\\nthe column added at the end.\\nListing 4.12 Creating a new column with withColumn()\\nOur  colums has been added at the end of our data frame.Duration_secondslogs.select(\\n    F.col(\"Duration\"),\\n    (\\n        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\\n        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\\n        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\\n    ).alias(\"Duration_seconds\"),\\n).distinct().show(5)\\n# +----------------+----------------+\\n# |        Duration|Duration_seconds|\\n# +----------------+----------------+\\n# |00:10:30.0000000|             630|\\n# |00:25:52.0000000|            1552|\\n# |00:28:08.0000000|            1688|\\n# |06:00:00.0000000|           21600|\\n# |00:32:08.0000000|            1928|\\n# +----------------+----------------+\\n# only showing top 5 rows\\nlogs = logs.withColumn(\\n    \"Duration_seconds\",\\n    (\\n        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\\n        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\\n        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\\n    ),\\n)\\nlogs.printSchema()\\n# root\\n#  |-- LogServiceID: integer (nullable = true)\\n#  |-- LogDate: timestamp (nullable = true)\\n#  |-- AudienceTargetAgeID: integer (nullable = true)\\n#  |-- AudienceTargetEthnicID: integer (nullable = true)\\n#  [... more columns]\\n#  |-- Language2: integer (nullable = true)\\n#  |-- Duration_seconds: integer (nullable = true)  \\n74\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 77}), Document(page_content='WARNING If you’re creating a column  and give it a name that already withColumn()\\nexists in your data frame, PySpark will happily overwrite the column. This is\\noften very useful to keep the number of columns manageable, but make\\nsure you are seeking this effect!\\nWe can create columns using the same expression with  and with . Both select() withColumn()\\napproaches have their use.  will be useful when you’re explicitly working with a few select()\\ncolumns. When you need to create new ones without changing the rest of the data frame, prefer \\n. You’ll quickly get the intuition about which one is easiest to use when faced withColumn()\\nwith the choice.\\nFigure 4.5 select() vs. withColumn(), visually. withColumn() keeps all the pre-existing\\ncolumns without the need the specify them explicitly.\\nThis section covers how to make the order and name of the columns friendlier for you. It might\\nseem a little vapid, but after a few hours of hammering code on a particularly tough piece of\\ndata, you’ll be happy to have this in your toolbox.\\nRenaming columns can be done with  and , of course. We saw briefly in chapter select() alias\\n2 that PySpark provides you an easier way to do so. Enter ! In , I use withColumnRenamed()4.4.4 Tidying our data frame: renaming and re-ordering columns\\n75\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 78}), Document(page_content='to remove the capital letters of my newly created withColumnRenamed() duration_seconds\\ncolumn.\\nListing 4.13 Renaming one column at a type, the  way withColumnRenamed()\\nI’m a huge fan of having column names without capital letters. I’m a lazy typist, and pressing\\nShift all the time really adds up folks! I could potentially use  with a for withColumnRenamed()\\nloop over all the columns to rename all my columns in my data frame. The PySpark developers\\nthought about this and offered a better way to rename all the columns of your data frame in one\\nfell swoop. This relies on a method, , that returns a new data frame with the new toDF()\\ncolumns. Just like ,  takes a  so we’ll need to unpack our column names if drop() toDF() *cols\\nthey’re in a list. The code in shows how you can rename all your columns to lower case in a\\nsingle line using that method.\\nListing 4.14 Batch lower-casing your data frame using the  method toDF()\\nIf you look at the line of code, you can see that I’m not assigning the resulting data frame. I\\nwanted to showcase the functionality, but since we have ancillary tables with column names that\\nmatch, I wanted to avoid the trouble of lower-casing every column in every table.\\nOur final step is  columns. Since re-ordering columns is equivalent to selecting re-ordering\\ncolumns in a different order,  is the perfect method for the job. For instance, if we select()\\nwanted to sort the columns alphabetically, we can use the  function on the list of our data sorted\\nframe columns, just like in .logs = logs.withColumnRenamed(\"Duration_seconds\", \"duration_seconds\")\\nlogs.printSchema()\\n# root\\n#  |-- LogServiceID: integer (nullable = true)\\n#  |-- LogDate: timestamp (nullable = true)\\n#  |-- AudienceTargetAgeID: integer (nullable = true)\\n#  |-- AudienceTargetEthnicID: integer (nullable = true)\\n#  [...]\\n#  |-- Language2: integer (nullable = true)\\n#  |-- Duration_seconds: integer (nullable = true)\\nlogs.toDF(*[x.lower() for x in logs.columns]).printSchema()\\n# root\\n#  |-- logserviceid: integer (nullable = true)\\n#  |-- logdate: timestamp (nullable = true)\\n#  |-- audiencetargetageid: integer (nullable = true)\\n#  |-- audiencetargetethnicid: integer (nullable = true)\\n#  |-- categoryid: integer (nullable = true)\\n#  [...]\\n#  |-- language2: integer (nullable = true)\\n#  |-- duration_seconds: integer (nullable = true)\\n76\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 79}), Document(page_content='Listing 4.15 Selecting our columns in alphabetical order using select()\\nRemember that, in most programming languages, capital letters comes before\\nlower-case ones.\\nWhen working with numerical data, looking at a long column of values isn’t very useful. We’re\\noften more concerned about some key information, which may include count, mean, standard\\ndeviation, minimum, and maximum. The  method does exactly that. By default, it describe()\\nwill show those statistics on all numerical and string columns, which will overflow our screen\\nand be unreadable since we have many columns. I display the columns description one-by-one\\nby iterating over the list of columns and showing the output of  in . Note that describe()\\n will (lazily) compute the data frame but won’t display it, so we have to  the describe() show()\\nresult.\\nListing 4.16 Describing everything in one fell swooplogs.select(sorted(logs.columns)).printSchema()\\n# root\\n#  |-- AudienceTargetAgeID: integer (nullable = true)\\n#  |-- AudienceTargetEthnicID: integer (nullable = true)\\n#  |-- BroadcastOriginPointID: integer (nullable = true)\\n#  |-- CategoryID: integer (nullable = true)\\n#  |-- ClosedCaptionID: integer (nullable = true)\\n#  |-- CompositionID: integer (nullable = true)\\n#  [...]\\n#  |-- Subtitle: string (nullable = true)\\n#  |-- duration_seconds: integer (nullable = true) \\n4.4.5 Summarizing your data frame:  and describe() summary()\\nfor i in logs.columns:\\n    logs.describe(i).show()\\n# +-------+------------------+ \\n# |summary|      LogServiceID|\\n# +-------+------------------+\\n# |  count|           7169318|\\n# |   mean|3453.8804215407936|\\n# | stddev|200.44137201584468|\\n# |    min|              3157|\\n# |    max|              3925|\\n# +-------+------------------+\\n#\\n# [...]\\n#\\n# +-------+ \\n# |summary|\\n# +-------+\\n# |  count|\\n# |   mean|\\n# | stddev|\\n# |    min|\\n# |    max|\\n# +-------+\\n# [... many more little tables]\\n77\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 80}), Document(page_content='Numerical columns will display the information in a description table like so.\\nIf the type of the column isn’t compatible, PySpark displays only the title column.\\nIt will take more time than doing everything in one fell swoop, but the output will be a lot\\nfriendlier. Now, because the mean or standard deviation of a string is not a thing, you’ll see null\\nvalues here. Furthermore, some columns won’t be displayed (you’ll see time tables with only the\\ntitle column), as  will only work for numerical and string columns. For a short line describe()\\nto type, you still get a lot!\\ndescribe()  is a fantastic method, but what if you want more?  at the rescue! summary()\\nWhere  will take  as a parameter (one or more columns, the same way as describe() *cols\\n or ),  will take  as a parameter. This means that you’ll select() drop() summary() *statistics\\nneed to select the columns you want to see before passing the  method. On the other summary()\\nhand, we can customize the statistics we want to see. By default,  shows everything summary()\\n shows, adding the approximate 25%-50% and 75% percentiles. shows how you can describe()\\nreplace  for  and the result of doing so. describe() summary()\\nListing 4.17 Summarizing everything in one fell swoop: default or custom options.\\nBy default, we have , , , , , , ,  as statistics. count mean stddev min 25% 50% 75% max\\nWe can also pass our own following the same nomenclature convention.for i in logs.columns:\\n    logs.select(i).summary().show()  \\n# +-------+------------------+\\n# |summary|      LogServiceID|\\n# +-------+------------------+\\n# |  count|           7169318|\\n# |   mean|3453.8804215407936|\\n# | stddev|200.44137201584468|\\n# |    min|              3157|\\n# |    25%|              3291|\\n# |    50%|              3384|\\n# |    75%|              3628|\\n# |    max|              3925|\\n# +-------+------------------+\\n#\\n# [... many more slightly larger tables]\\nfor i in logs.columns:\\n    logs.select(i).summary(\"min\", \"10%\", \"90%\", \"max\").show()  \\n# +-------+------------+\\n# |summary|LogServiceID|\\n# +-------+------------+\\n# |    min|        3157|\\n# |    10%|        3237|\\n# |    90%|        3710|\\n# |    max|        3925|\\n# +-------+------------+\\n#\\n# [...]\\n78\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 81}), Document(page_content='If you want to limit yourself to a subset of those metrics,  will accept a number of summary()\\nstring parameters representing the statistic. You can input , , ,  or count mean stddev min max\\ndirectly. For approximate percentiles, you need to provide them in  format, such as . XX% 25%\\nBoth methods will work only on non-null values. For the summary statistics, it’s the expected\\nbehaviour, but the \"count\" entry will also count only the non-null values for each column. A\\ngood way to see which columns are mostly empty!\\nWARNING describe()  and  are two very useful methods, but they are notsummary()\\nmeant to be used anywhere else than during development, to quickly peek\\nat data. The PySpark developers don’t guarantee the backward\\ncompatibility of the output, so if you need one of the outputs for your\\nprogram, use the corresponding function in . pyspark.sql.functions\\nThey’re all there.\\nThis chapter covered the ingestion and discovery of a tabular data set, one of the most popular\\ndata representation formats. We built on the basics of PySpark data manipulation, covered in\\nchapters 2 and 3, and added a new layer by working with columns. The next chapter will be the\\ndirect continuation of this one, where we will explore more advanced aspects of the data frame\\nstructure.\\n79\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 82}), Document(page_content='PySpark uses the  object to read any kind of data directly in a data frame. SparkReader\\nThe specialized   is used to ingest comma-separated value (CSV) files. `CSV SparkReader\\nJust like when reading text, the only mandatory parameter is the source location.\\nThe CSV format is very versatile, so PySpark provides many optional parameters to\\naccount for this flexibility. The most important ones are the field delimiter, the record\\ndelimiter, and the quotation character. All of those parameters have sensible defaults.\\nPySpark can infer the Schema of a CSV file by setting the  optional inferSchema\\nparameter to . PySpark accomplishes this by reading the data twice: once for setting True\\nthe appropriate types for each columns, and another time to ingest the data in the inferred\\nformat.\\nTabular data is represented into a data frame in a series of Columns, each having a name\\nand a type. Since the data frame is a column-major data structure, the concept of row is\\nless relevant.\\nYou can use Python code to explore the data efficiently, using the column list as any\\nPython list to expose the elements of the data frame of interest.\\nThe most common operations on a data frame are the selection, deletion, and creation or\\ncolumns. In PySpark, the methods used are ,  and , select() delete() withColumn()\\nrespectively.\\nselect can be used for column re-ordering by passing a re-ordered list of columns.\\nYou can rename columns one by one with the  method, or all at withColumnRenamed()\\nonce by using the  method. toDF()\\nYou can display a summary of the columns with the  or  method. describe() summary()\\n has a fixed set of metrics, while  will take functions as describe() summary()\\nparameters and apply them to all columns.4.5 Summary\\n80\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 83}), Document(page_content='5\\nIn this chapter, you will learn\\nIn chapter 4, we looked at how we can transform a data frame using selection, dropping,\\ncreation, renaming, re-ordering, and summary of columns. Those operations constitute the\\nfoundation working with a data frame in PySpark. In this chapter, I will complete the review of\\nthe most common operations you will perform on a data frame: linking or  data frames joining\\ntogether, as well as grouping data (and performing operations on the  object). We GroupedData\\nconclude this chapter by wrapping our exploratory program into a single script we can submit,\\njust like we performed in chapter 3.\\nWe use the same  table that we left in chapter 4. In practical steps, this chapter’s code logs\\nenriches our table with the relevant information contained in the link tables and then get\\nsummarized into relevant groups, using what can be considered a graduate version of the \\n method I show in chapter 4. If you want to catch up with a minimal amount of fuzz, describe()\\nI provide an  script in the  directory. end_of_chapter.py src/Ch04The data frame through a new lens:\\njoining and grouping\\nJoining two data frames together.\\nHow to select the right type of join for your use-case.\\nGrouping data and understanding the  transitional object. GroupedData\\nBreaking the  with an aggregation method and getting a summarized GroupedData\\ndata frame.\\nFilling null values in your data frame\\n81\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 84}), Document(page_content='When working with data, we’re most often working on one structure at a time. Up until now,\\nwe’ve been exploring the many ways we can slice, dice, and modify a data frame to our wildest\\ndesires. What happens when we need to link two sources together? This section will introduce\\njoins and how we can apply them when using a star schema setup or another set of tables where\\nvalues match exactly. This is the easiest use case: joins can get dicey, and chapter 9 drills deeper\\ninto the subject of efficient joins.\\nJoining data frames is a common operation when working with related tables together. If you’ve\\nused other data processing libraries, you might have seen the same operation being called a \\n or a . Joins are very common operations, but they are very flexible: in the next section, merge link\\nwe set a common vocabulary to avoid confusion and build our understanding on a solid\\nfoundation.\\nAt its most basic, a join operation is a way to take the data from a data frame and add it to\\nanother one according to a set of rules. To introduce the moving parts of a join in a practical\\nfashion, I ingest in listing 5.1 a second table to be joined to our  data frame. I use the same logs\\nparametrization of the  as used for the  table to read our new SparkReader.csv logs\\n table. Once ingested, I filter the data frame so keep only the primary channels, log_identifier\\nas per the data documentation. With this, we should be good to go.5.1 From many to one: joining data\\n5.1.1 What’s what in the world of joins\\n82\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 85}), Document(page_content='1.  \\n2.  \\n3.  Listing 5.1 Exploring our first link table: log_identifier\\nChannel identifier\\nChannel key (maps to our centre table!)\\nBoolean flag: is the channel primary (1) or not (0)? We want only the 1’s\\nWe have two data frames, containing each a set of columns. The join operation has three major\\ningredients:\\ntwo tables, called a  and a  table respectively (more on this in a moment); left right\\none or more  which are the series of conditions that determine if records predicates\\nbetween the two tables are joined;\\nfinally, a  to indicate how we perform the join when the predicate succeeds and method\\nwhen it fails.\\nWith those three ingredients, you can construct a join between two data frames in PySpark by\\nfilling the  with the desired behaviour. In listing 5.2, I show what a barebone join [KEYWORDS]\\nrecipe looks like before we start digging on how to customize those ingredients. The next three\\nsections are dedicated to each one of those parameters.DIRECTORY = \"../../data/Ch04\"\\nlog_identifier = spark.read.csv(\\n    os.path.join(DIRECTORY, \"ReferenceTables/LogIdentifier.csv\"),\\n    sep=\"|\",\\n    header=True,\\n    inferSchema=True,\\n)\\nlog_identifier.printSchema()\\n# root\\n#  |-- LogIdentifierID: string (nullable = true) \\n#  |-- LogServiceID: integer (nullable = true) \\n#  |-- PrimaryFG: integer (nullable = true) \\nlog_identifier.show(5)\\n# +---------------+------------+---------+\\n# |LogIdentifierID|LogServiceID|PrimaryFG|\\n# +---------------+------------+---------+\\n# |           13ST|        3157|        1|\\n# |         2000SM|        3466|        1|\\n# |           70SM|        3883|        1|\\n# |           80SM|        3590|        1|\\n# |           90SM|        3470|        1|\\n# +---------------+------------+---------+\\n# only showing top 5 rows\\nlog_identifier = log_identifier.where(F.col(\"PrimaryFG\") == 1)\\nlog_identifier.count()\\n# 920\\n83\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 86}), Document(page_content='1.  Listing 5.2 A barebone recipe for a join in PySpark: we need to replace the \\n block.[KEYWORDS]\\nA join is performed on two tables at a time. Because of the SQL heritage in the vocabulary of\\ndata manipulation, the two tables are being named  and  tables. Chapter 7 contains a little left right\\nmore explanation about why they are called this way. In PySpark, a neat way to remember which\\none is which is to say that the left table is  of the  method where the right one is to the left join()\\n (inside the parentheses). Knowing which one is which is very useful when choosing to the right\\nthe join method: unsurprisingly, there is a left and right join type…\\u200b\\nOur tables are now identified, so we can update our join blueprint as I do in listing 5.3. We now\\nneed to steer our attention to the next parameter, the predicates.\\nListing 5.3 A barebone recipe for a join in PySpark, with the left and right tables\\nfilled in.\\nThe predicates of a PySpark join are rules between columns of the left and right data frames. A\\njoin is performed record-wise, where each record on the left data frame is compared (via the\\npredicates) to each record on the right data frame. If the predicates return , the join is a True\\nmatch and is a failure if . False\\nThe best way to illustrate a predicate is to create a simple example and explore the results. For\\nour two data frames, we will build the predicate logs[\"LogServiceID\"] ==\\n. In plain English, we can translate this by the following. log_identifier[\"LogServiceID\"]\\nMatch the records from the  data frame to the records from the  data logs log_identifier\\nframe when the value of their  column is equal. LogServiceID\\nI’ve taken a small sample of the data in both data frames and illustrated the result of applying the\\npredicate in figure 5.1. There are two important points to highlight:\\nIf one record in the left table resolves the predicate with more than one record in the right[LEFT].join(\\n    [RIGHT],\\n    on=[PREDICATES]\\n    how=[METHOD]\\n)\\n5.1.2 Knowing our left from our right\\nlogs.join(\\n    log_identifier,\\n    on=[PREDICATES]\\n    how=[METHOD]\\n)\\n5.1.3 The rules to a successful join: the predicates\\n84\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 87}), Document(page_content='1.  \\n2.  \\n1.  \\n2.  table (or vice versa), this record will be duplicated in the joined table.\\nIf one record in the left or in the right table does not resolve the predicate with any record\\nin the other table, it will not be present in the resulting table, unless the join method\\n. specifies a protocol for failed predicates\\nFigure 5.1 Our predicate is applied to a sample of our two tables. 3590 on the left table\\nresolves the predicate twice while 3417 on the left/3883 on the right do not solve it.\\nIn our example, the  record on the left is equal to the two corresponding records on the 3590\\nright, and we see two solved predicates with this number in our result set. On the other hand, the \\n record does not match anything on the right and therefore is not present in the result set.3417\\nThe same thing happens with the  record on the right table. 3883\\nYou are not limited to a single test in your predicate. You can use multiple conditions by\\nseparating them with boolean operators such as  (or) or  (and). You can also use a different test | &\\nthan equality. Here are two examples and their plain English translation.\\n(logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]) &\\n(logs[\"left_col\"] < log_identifier[\"right_col\"])  : Will only match the\\nrecords that have the same  on both side  where the value of the LogServiceID and\\n in the  table is smaller than the value of the  in the left_col logs right_col\\n table. log_identifier\\n(logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]) |\\n(logs[\"left_col\"] > log_identifier[\"right_col\"])  : Will only match the\\n85\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 88}), Document(page_content='2.  \\n1.  \\n2.  records that have the same  on both side  where the value of the LogServiceID or\\n in the  table is greater than the value of the  in the left_col logs right_col\\n table. log_identifier\\nYou can build the operations as complicated as you want. I really recommend wrapping each\\ncondition in parentheses to avoid worrying about the operator precedence and facilitate the\\nreading.\\nBefore adding our predicate to our join in progress, I want to note that PySpark provides a few\\npredicate shortcuts to reduce the complexity of the code.\\nIf  (such as you have multiple  predicates and (left[\"col1\"] == right[\"colA\"]) &\\n), you can put (left[\"col2\"] > right[\"colB\"]) & (left[\"col3\"] != right[\"colC\"])\\nthem into a list such as [left[\"col1\"] == right[\"colA\"], left[\"col2\"] >\\n. This makes you intent more explicit right[\"colB\"], left[\"col3\"] != right[\"colC\"]]\\nand avoids counting parentheses for long chains of conditions.\\nFinally, if you are performing an \"equi-join\", where you are testing for equality between\\nidentically named columns, you can just specify the name of the columns as a string or a list of\\nstrings as a predicate. In our case, it means that our predicate can only be \"LogServiceID\".\\nThis is what I put in listing 5.4.\\nListing 5.4 A barebone recipe for a join in PySpark, with the left and right tables\\nfilled in, as well as the predicate.\\nThe join method influences how you structure predicates, so XREF ch05-join-naming revisits the\\nwhole join operation after we’re done with the ingredient-by-ingredient approach. The last\\nparameter is the , which completes our join operation. how\\nThe last ingredient of a successful join is the  parameter, which will indicate the join method. how\\nMost books explaining joins shows Venn diagrams indicating how each joins colors the different\\nareas, but I find that it is only useful as a reminder, not a teaching tool. I’ll review each type of\\njoin with the same tables we’ve used in figure 5.1, giving the result of the operation.\\nA join method basically boils down to those two questions:\\nWhat happens when the return value of the predicates is ? True\\nWhat happens when the return value of the predicates is ? Falselogs.join(\\n    log_identifier,\\n    on=\"LogServiceID\"\\n    how=[METHOD]\\n)\\n5.1.4 How do you do it: the join method\\n86\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 89}), Document(page_content='Classifying the join methods based on the answer to those two questions is an easy way to\\nremember them.\\nTIP PySpark’s joins are basically the same as SQL joins. If you are already\\ncomfortable with them, feel free to skip this section.\\nA cross join ( ) is the nuclear option. It returns a record for every record pair, how=\"cross\"\\n of the value the predicates return. In our data frame example, our  table contains regardless logs\\n4 records and our  contains 5 records, so the cross join will contain 4 x 5 = 20 logs_identifier\\nrecords. The result is illustrated in figure 5.2.\\nFigure 5.2 A visual example of a cross join. Each record on the left is matched to every\\nrecord on the right.\\nCross joins are seldom the operation that you want, but they are useful when you want a table\\nthat contains every possible combination.\\nTIP PySpark also provides an explicit  method that takes the right crossJoin()\\ndata frame as a parameter.CROSS JOIN: THE NUCLEAR OPTION\\n87\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 90}), Document(page_content='An inner join ( ) is the most common join by a landslide. PySpark will default to an how=\"inner\"\\ninner join if you don’t pass a join method explicitly. It returns a record if the predicate is true and\\ndrops it if false. I consider inner join to be the natural way to think of joins because they are very\\nsimple to reason about.\\nIf we look at our tables, we would have a table very similar to figure 5.1. The record with the \\n on the left will be duplicated because it matches two records on the LogServiceID == 3590\\nright table. The result is illustrated in figure 5.3.\\nFigure 5.3 An inner join. Each successful predicate creates a joined record.\\nLeft (  or ) and right (  or )how=\"left\" how=\"left_outer\" how=\"right\" how=\"right_outer\"\\nare like inner join, in which they generate a record for a successful predicate. The difference is\\nwhat happens when the predicate is false:\\nA  (also called a ) join will add the unmatched records from the  table inleft left outer left\\nthe joined table, filling the columns coming from the  table with right None/null\\nA  (also called a ) join will add the unmatched records from the  inright right outer right\\nthe joined table, filling the columns coming from the  table with left None/null\\nIn practice, it means that your joined table is guaranteed to contain all the records of the table\\nwhich feeds the join (left or right). Visually, figure 5.4 shows that, although  doesn’t satisfy 3417\\nthe predicate, it is still present in the left joined table. The same happens with  and the right 3883\\ntable. Just like an inner join, if the predicate is successful more than once, the record will be\\nduplicated.INNER JOIN\\nLEFT AND RIGHT OUTER JOIN\\n88\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 91}), Document(page_content='Figure 5.4 A left and right joined table. All the records of the direction table are present in\\nthe resulting table.\\nLeft and right joins are very useful when you are not certain if the link table contains every key.\\nYou can then fill the null values— —— see XREF ch05-fillna— —— or process them knowing\\nyou didn’t drop any records.\\nA full outer ( ,  or ) join is simply the fusion of a how=\"outer\" how=\"full\" how=\"full_outer\"\\nleft and right join. It will add the unmatched records from the left and the right table, padding\\nwith . It serves a similar purpose to the left and right join but is not as popular sinceNone/null\\nyou’ll generally have one (and just one) anchor table that you want to preserve all records.FULL OUTER JOIN\\n89\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 92}), Document(page_content='Figure 5.5 A left and right joined table. We can see all the records from both table.\\nThe left semi-join and left anti-join are a little more esoteric, but are really easy to understand.\\nA left semi-join ( ) is the same as an  join, but only keeps the columns in how=\"left_semi\" inner\\nthe left table. It also won’t duplicate the records in the left table if they fulfill the predicate with\\nmore than one record in the right table. Its main purpose is to filter out records from a table based\\non a predicate depending of another table.\\nA left anti-join is the opposite of an  join. It will keep only the records that do not match inner\\nthe predicate with any record in the right table, dropping any successful match.\\nOur blueprint join is now finalized: we are going with an inner join since we want to keep only\\nthe records where the  has additional information in our  table. LogServiceID log_identifier\\nSince our join is complete, I assign the result to a new variable . logs_and_channels\\nListing 5.5 A barebone recipe for a join in PySpark, with the left and right tables\\nfilled in, as well as the predicate and the method.\\nI could have omitted the  parameter outright, since inner join is the default. how\\nWith the first join done, we will link two additional tables to continue our data discovery andLEFT SEMI-JOIN AND LEFT ANTI-JOIN\\nlogs_and_channels = logs.join(\\n    log_identifier,\\n    on=\"LogServiceID\"\\n    how=\"inner\"  \\n)\\n90\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 93}), Document(page_content='processing. The  table contains information about the type of programs and the CategoryID\\n table contains the data that allows us to pinpoint the commercials. ProgramClassID\\nThis time, we are performing  joins since we are not entirely certain about the existence of left\\nthe keys in the link table. In listing 5.6, we follow the same process as we did for the \\n table, in one fell swoop. log_identifier\\nWe read the table using the  and the same configuration as our other SparkReader.csv\\ntables.\\nWe keep the relevant columns.\\nWe join the data to our  table, using PySpark’s method chaining. logs_and_channels\\nListing 5.6 Linking the category and program class tables using a left join\\nWe’re aliasing the  column to remember what it maps to EnglishDescription\\nSame as #1 here, but for the program class.\\nIn listing 5.6, in the select statement, I aliased some columns that were both named \\n. How will PySpark know which one to keep? The next section takes a EnglishDescription\\nbrief detour from our exploration to address one of the most frequent sources of frustration when\\ndealing with columns: staying on top of you column names when joining data frames.DIRECTORY = \"../../data/Ch04\"\\ncd_category = spark.read.csv(\\n    os.path.join(DIRECTORY, \"ReferenceTables/CD_Category.csv\"),\\n    sep=\"|\",\\n    header=True,\\n    inferSchema=True,\\n).select(\\n    \"CategoryID\",\\n    \"CategoryCD\",\\n    F.col(\"EnglishDescription\").alias(\"Category_Description\"),  \\n)\\ncd_program_class = spark.read.csv(\\n    os.path.join(DIRECTORY, \"ReferenceTables/CD_ProgramClass.csv\"),\\n    sep=\"|\",\\n    header=True,\\n    inferSchema=True,\\n).select(\\n    \"ProgramClassID\",\\n    \"ProgramClassCD\",\\n    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),  \\n)\\nfull_log = logs_and_channels.join(cd_category, \"CategoryID\", how=\"left\").join(\\n    cd_program_class, \"ProgramClassID\", how=\"left\"\\n)\\n91\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 94}), Document(page_content='SIDEBAR The science of joining in a distributed environment\\nWhen joining data in a distributed environment, the \"we don’t care about\\nwhere data is\" doesn’t work anymore. To be able to process a comparison\\nbetween records, the data needs to be on the same machine. If not,\\nPySpark will move the data in an operation called a . As you can shuffle\\nimagine, moving large amounts of data over the network is very slow, and\\nwe should aim to avoid this when possible.\\nThis is one of the instances where PySpark’s abstraction model shows\\nsome weakness. Since joins are such an important part of working with\\nmultiple data sources, I’ve decided to introduce the syntax here so we can\\nget things rolling. We revisit performance considerations and join strategies\\nin chapter 9. For the moment, trust the optimizer!\\nBy default, PySpark will not allow two columns to be named the same. If you create a column\\nwith  using an existing column name, PySpark will overwrite (or shadow) thewithColumn()\\ncolumn. When joining data frames, the situation is a little more complicated, as displayed in\\nlisting 5.7\\nListing 5.7 A join that generates two seemingly identically named columns\\nOne  column…LogServiceID\\n… and another!\\nPySpark doesn’t know which column we mean.5.1.5 Naming conventions in the joining world\\nlogs_and_channels_verbose = logs.join(\\n    log_identifier, logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]\\n)\\nlogs_and_channels_verbose.printSchema()\\n# root\\n#  |-- LogServiceID: integer (nullable = true) \\n#  |-- LogDate: timestamp (nullable = true)\\n#  |-- AudienceTargetAgeID: integer (nullable = true)\\n#  |-- AudienceTargetEthnicID: integer (nullable = true)\\n#  [...]\\n#  |-- duration_seconds: integer (nullable = true)\\n#  |-- LogIdentifierID: string (nullable = true)\\n#  |-- LogServiceID: integer (nullable = true) \\n#  |-- PrimaryFG: integer (nullable = true)\\ntry:\\n    logs_and_channels_verbose.select(\"LogServiceID\")\\nexcept AnalysisException as err:\\n    print(err)\\n# \"Reference \\'LogServiceID\\' is ambiguous, could be: LogServiceID, LogServiceID.;\" \\n92\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 95}), Document(page_content='PySpark happily joins the two data frames together but fails when we try to work with the\\nambiguous column. This is a frequent situation when working with data that follows the same\\nconvention for column naming. Fortunately, solving this problem is easy. I show in this section\\nthree methods, from the easiest to the most general.\\nFirst, when performing an equi-join, prefer using the simplified syntax, since it takes care of\\nremoving the second instance of the predicate column. This only works when using equality\\ncomparison, since the data is identical in both columns from the predicate, preventing\\ninformation loss. I show the code and schema of the resulting data frame when using a simplified\\nequi-join in listing 5.8.\\nListing 5.8 Using the simplified syntax for equi-joins results in no duplicate\\ncolumns.\\nThe second approach relies on the fact that PySpark joined data frames remembers the\\nprovenance of the columns. Because of this, we can refer to the  columns using LogServiceID\\nthe same nomenclature as before, e.g. . We can then log_identifier[\"LogServiceID\"]\\nrename this column or delete is, solving our issue. I use this approach in listing 5.9\\nListing 5.9 Using the origin name of the column allows for unambiguous selection\\nof the columns after a join\\nBy dropping one of the two duplicated columns, we can then use the name for the\\nother without any problem.\\nThe last approach is convenient if you use the  object directly. PySpark will not resolve Column\\nthe origin name when you rely on  to work with columns. To solve this the most general F.col()\\nway, we need to  our tables when performing the join, as shown in listing 5.10. alias()logs_and_channels = logs.join(log_identifier, \"LogServiceID\")\\nlogs_and_channels.printSchema()\\n# root\\n#  |-- LogServiceID: integer (nullable = true)\\n#  |-- LogDate: timestamp (nullable = true)\\n#  |-- AudienceTargetAgeID: integer (nullable = true)\\n#  |-- AudienceTargetEthnicID: integer (nullable = true)\\n#  |-- CategoryID: integer (nullable = true)\\n#  [...]\\n#  |-- Language2: integer (nullable = true)\\n#  |-- duration_seconds: integer (nullable = true)\\n#  |-- LogIdentifierID: string (nullable = true)\\n#  |-- PrimaryFG: integer (nullable = true)\\nlogs_and_channels_verbose = logs.join(\\n    log_identifier, logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]\\n)\\nlogs_and_channels.drop(log_identifier[\"LogServiceID\"]).select(\"LogServiceID\")  \\n# DataFrame[LogServiceID: int]\\n93\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 96}), Document(page_content='Listing 5.10 Aliasing our tables makes the origin resolved when using  to F.col()\\nrefer to columns.\\nOur  table gets aliased as .logs left\\nOur  gets aliased as .log_identifier right\\n will resolve  and  as prefix for the column names. F.col() left right\\nAll three approaches are valid. The first one works only in the case of equijoins, but the two\\nothers are mostly interchangeable. PySpark gives you a lot of control over the structure and\\nnaming of your data frame but requires you to be explicit.\\nThis section packed a lot of information about joins, a very important tool when working with\\ninterrelated data frames. Although the possibilities are endless, the syntax is simple and easy to\\nunderstand.\\nleft.join(right , who’s the first parameter.  decides if it’s a match.  indicates how to on how\\noperate on match success and failures.\\nMaybe we could turn this into a rap…\\u200b\\nWith our table nicely augmented, let’s carry on to our last step: summarizing the table using\\ngroupings.\\nSIDEBAR Exercise 5.1\\nWhat is the result of this code?\\none = left.join(right, how=\"left_semi\", on=\"my_column\" two = left.join(right,\\nhow=\"left_anti\", on=\"my_column\") one.union(two)\\nSIDEBAR Exercise 5.2\\nWrite a PySpark code that will return the result of the following code block\\nwithout using a left anti join.\\nleft.join(right, how=\"left_anti\",\\non=\"my_column\").select(\"my_column\").distinct()logs_and_channels_verbose = logs.alias(\"left\").join(  \\n    log_identifier.alias(\"right\"),  \\n    logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"],\\n)\\nlogs_and_channels_verbose.drop(F.col(\"right.LogServiceID\")).select(\\n    \"LogServiceID\"\\n)  \\n# DataFrame[LogServiceID: int]\\n94\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 97}), Document(page_content='1.  \\n2.  SIDEBAR Exercise 5.3 (hard)\\nWrite a PySpark code that will return the result of the following code block\\nwithout using a left semi-join.\\nleft.join(right, how=\"left_semi\",\\non=\"my_column\").select(\"my_column\").distinct()\\nWhen displaying data, especially large amounts of data, you’ll often summarize data using\\nstatistics as a first step. As a matter of fact, chapter 4 showed how you can use  and summary()\\n to compute mean, min, max, etc. over the whole data frame. What if we need to look display()\\nusing a different lens?\\nThis section covers the  method in greater detail than seen in chapter 3. I introduce groupby()\\nhere the  object and its usage. In practical terms, we’ll use  to answer GroupedData groupby()\\nour original question: what are the channels with the most and least proportion of commercials?\\nIn order to answer this, we have to take each channel and sum the  in two duration_seconds\\nways:\\nOne to get the number of seconds when the program is a commercial.\\nOne to get the number of seconds of total programming.\\nOur plan, before we start summing, is to identify what is considered a commercial and what is\\nnot. The documentation doesn’t provide formal guidance on how to do so, so we’ll explore the\\ndata and draw our conclusion. Let’s group!\\nIn chapter 3, we performed a very simple  to count the occurrences of each word. It groupby()\\nwas a very simple example of grouping and counting records based on the words inside the\\n(only) column. In this section, we expand on that simple example by grouping over many\\ncolumns. I also introduce a more general notation than the  we’ve used previously, so count()\\nwe compute more that one summary function.\\nSince you are already acquainted with the basic syntax of , this section starts by groupby()\\npresenting a full code block that computes the total duration (in seconds) of the program class. In\\nlisting 5.11 we perform the grouping, compute the aggregate function, and present the results in\\ndecreasing order.5.2 Summarizing the data via: groupby and GroupedData\\n5.2.1 A simple groupby blueprint\\n95\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 98}), Document(page_content='Listing 5.11 Displaying the most popular types of programs\\nThis small program has a few new parts, so let’s review them one by one.\\nOur grouping routing starts with the  method. A \"grouped by\" data frame is not a data groupby()\\nframe anymore, instead, it becomes a  object, displayed in all its glory in listing GroupedData\\n5.13. This object a transitional object: you can’t really inspect it (there is no  method) .show()\\nand it’s waiting for further instructions to become show-able again. Illustrated, it would look like\\nthe right-hand side of figure 5.7. You have the key (or keys, if you  multiple groupby()\\ncolumns), and the rest of the columns are grouped inside some \"cell\", awaiting a summary\\nfunction so they can be promoted to a bona fide column again.\\nSIDEBARagg() for the lazy\\nagg() also accepts a dictionary, in the form {column_name:\\n where both are string. Because of this, we can aggregation_function}\\nrewrite listing 5.11 like so.\\nListing 5.12 Displaying the most popular types of programs, using a\\ndictionary expression inside agg()\\nIt makes rapid prototyping very easy (you can, just like with column objects,\\nuse the  to refer to all columns). I personally don’t like this approach for \"*\"\\nmost cases since you don’t get to alias your columns when creating them. I\\nam including it since you will see it when reading other people’s code.full_log.groupby(\"ProgramClassCD\", \"ProgramClass_Description\").agg(\\n    F.sum(\"duration_seconds\").alias(\"duration_total\")\\n).orderBy(\"duration_total\", ascending=False).show(100, False)\\n# +--------------+--------------------------------------+--------------+\\n# |ProgramClassCD|ProgramClass_Description              |duration_total|\\n# +--------------+--------------------------------------+--------------+\\n# |PGR           |PROGRAM                               |652802250     |\\n# |COM           |COMMERCIAL MESSAGE                    |106810189     |\\n# |PFS           |PROGRAM FIRST SEGMENT                 |38817891      |\\n# |SEG           |SEGMENT OF A PROGRAM                  |34891264      |\\n# |PRC           |PROMOTION OF UPCOMING CANADIAN PROGRAM|27017583      |\\n# |PGI           |PROGRAM INFOMERCIAL                   |23196392      |\\n# |PRO           |PROMOTION OF NON-CANADIAN PROGRAM     |10213461      |\\n# |OFF           |SCHEDULED OFF AIR TIME PERIOD         |4537071       |\\n# [... more rows]\\n# |COR           |CORNERSTONE                           |null          |\\n# +--------------+--------------------------------------+--------------+\\nfull_log.groupby(\"ProgramClassCD\", \"ProgramClass_Description\").agg(\\n    {\"duration_seconds\": \"sum\"}\\n).withColumnRenamed(\"sum(duration_seconds)\", \"duration_total\").orderBy(\\n    \"duration_total\", ascending=False\\n).show(\\n    100, False\\n)\\n96\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 99}), Document(page_content='Listing 5.13 A  object representation. Unlike the data frame, we haveGroupedData\\nno information about the columns or the structure.\\nFigure 5.6 The original data frame, with the focus on the columns we are grouping by.\\nFigure 5.7 The GroupedData object resulting from grouping by (ProgramClassID,\\nProgramClass_Description). The non-key columns are all in stand-by in the group cell.full_log.groupby()\\n# <pyspark.sql.group.GroupedData at 0x119baa4e0>\\n97\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 100}), Document(page_content='1.  \\n2.  In chapter 3, we brought back the  into a data frame by using the  method, GroupedData count()\\nwhich returns the count of each group. There are a few other, such as , ,  or min() max() mean()\\n. We could have used the  method directly, but we wouldn’t have had the option of sum() sum()\\naliasing the resulting column, getting stuck with  for a name. Instead, sum(duration_seconds)\\nwe use the oddly named . agg()\\nThe  method, for aggregate (or aggregation?), will take one or more agg() aggregate functions\\nfrom the  module we all know and love and apply them on each group pyspark.sql.functions\\nof the  object. In figure 5.8, I start on the left with our  oblect. GroupedData GroupedData\\nCalling  with an appropriate aggregate function pulls the column from the group cell, agg()\\nextracts the values, and performs the function, yielding the answer. Compared to using the sum()\\nfunction on the group by object,  trades a few keystrokes for 2 main advantages: agg()\\nagg() takes an arbitrary number of aggregate functions, unlike using a summary method\\ndirectly. You can’t chain multiple functions on  objects: the first one will GroupedData\\ntransform it into a data frame, and the second one will fail.\\nYou can alias resulting columns, so you control their name and improve the robustness of\\nyour code.\\nFigure 5.8 A data frame arising from the application of the agg() method (aggregate\\nfunction: F.sum() on Duration_seconds)\\nAfter the application of the aggregate function on our  object, we’re back with a GroupedData\\ndata frame. We can then use the  method to order the data by decreasing order of orderBy\\n, our newly created column. We finish by showing 100 rows, which is more duration_total\\nthan what the data frame contains, so it shows everything.\\nLet’s select our commercials. 5.1 shows my picks.\\n98\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 101}), Document(page_content='Now that we’ve done the hard job of identifying our commercial codes, we can start counting!\\nSIDEBARagg() is not the only player in town\\nSince PySpark 2.3, you can also use  with the  method, groupby() apply()\\nin the creatively named \"split-apply-combine\" pattern. I cover this pattern in\\nchapter 8.\\nWhen grouping and aggregating columns in PySpark, we have the whole power of the Column\\nobject at our fingertips. This means that we can group by and aggregate on custom columns! For\\nthis section, we will start by building a definition of , which takes the duration_commercial\\nduration of a program only if it is a commercial, and use this in our  statement to agg()\\nseamlessly compute both the total duration and the commercial duration.\\nIf we encode the content of 5.1 into a PySpark definition, this gives us listing 5.14\\nListing 5.14 Computing only the commercial time for each program in our table\\nI think that the best way to describe the code this time is to literally translate it into plain English.\\nWhen  the field of the (umn) , (med) of spaces at the beginning and end col \"ProgramClass\" trim\\nof the field  our list of commercial codes, then take the value of the field in the column is in\\n. , use 0 as a value. \"duration_seconds\" Otherwise\\nThe blueprint of the  function is as follows. It is possible to chain multiple  if F.when() when()Table 5.1 The types of programs we’ll be considering as commercials m\\nProgramClassCD ProgramClass_Description duration_total\\nCOM COMMERCIAL MESSAGE 106810189\\nPRC PROMOTION OF UPCOMING\\nCANADIAN PROGRAM27017583\\nPGI PROGRAM INFOMERCIAL 23196392\\nPRO PROMOTION OF NON-CANADIAN\\nPROGRAM10213461\\nLOC LOCAL ADVERTISING 483042\\nSPO SPONSORSHIP MESSAGE 45257\\nMER MERCHANDISING 40695\\nSOL SOLICITATION MESSAGE 7808\\n5.2.2 A column is a column: using agg with custom column definitions\\nF.when(\\n    F.trim(F.col(\"ProgramClassCD\")).isin(\\n        [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"PSA\", \"MAG\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\\n    ),\\n    F.col(\"duration_seconds\"),\\n).otherwise(0)\\n99\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 102}), Document(page_content='we have more than one condition, and to omit the  if we’re okay with having otherwise() null\\nvalues when none of the tests are positive.\\n( F.when([BOOLEAN TEST], [RESULT IF TRUE]) .when([ANOTHER BOOLEAN TEST], [RESULT IF TRUE])\\n.otherwise([DEFAULT RESULT, WILL DEFAULT TO null IF OMITTED]) )\\nWe now have a column ready to use. While, we could create the column before grouping by,\\nusing , let’s take it up a notch and use our definition directly in the  clause.withColumn() agg()\\nListing 5.15 does just that, and at the same time, gives us our answer!\\nListing 5.15 Using our new column into  to compute our final answer! agg()\\nWait a moment? Are some channels ? That can’t be it. If we look at the total only commercials\\nduration, we can see that some channels don’t broadcast a lot (1 day = 86,400 seconds). Still, we\\naccomplished our goal: we identified the channels with the most commercials. We finish this\\nchapter with one last task: processing those  values. nullanswer = (\\n    full_log.groupby(\"LogIdentifierID\")\\n    .agg(\\n        F.sum(\\n            F.when(\\n                F.trim(F.col(\"ProgramClassCD\")).isin(\\n                    [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\\n                ),\\n                F.col(\"duration_seconds\"),\\n            ).otherwise(0)\\n        ).alias(\"duration_commercial\"),\\n        F.sum(\"duration_seconds\").alias(\"duration_total\"),\\n    )\\n    .withColumn(\\n        \"commercial_ratio\", F.col(\"duration_commercial\") / F.col(\"duration_total\")\\n    )\\n)\\nanswer.orderBy(\"commercial_ratio\", ascending=False).show(1000, False)\\n# +---------------+-------------------+--------------+---------------------+\\n# |LogIdentifierID|duration_commercial|duration_total|commercial_ratio     |\\n# +---------------+-------------------+--------------+---------------------+\\n# |HPITV          |403                |403           |1.0                  |\\n# |TLNSP          |234455             |234455        |1.0                  |\\n# |MSET           |101670             |101670        |1.0                  |\\n# |TELENO         |545255             |545255        |1.0                  |\\n# |CIMT           |19935              |19935         |1.0                  |\\n# |TANG           |271468             |271468        |1.0                  |\\n# |INVST          |623057             |633659        |0.9832686034602207   |\\n# [...]\\n# |OTN3           |0                  |2678400       |0.0                  |\\n# |PENT           |0                  |2678400       |0.0                  |\\n# |ATN14          |0                  |2678400       |0.0                  |\\n# |ATN11          |0                  |2678400       |0.0                  |\\n# |ZOOM           |0                  |2678400       |0.0                  |\\n# |EURO           |0                  |null          |null                 |\\n# |NINOS          |0                  |null          |null                 |\\n# +---------------+-------------------+--------------+---------------------+\\n100\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 103}), Document(page_content='1.  \\n2.  \\n3.  Null values represent the absence of value. I find this to be a great oxymoron: a value for no\\nvalue? Philosophy aside, we have some nulls in our result set and I would like them gone.\\nPySpark provides two main functionalities to deal with null values: you can either  the dropna()\\nrecord containing them of  the null with a value. We explore in this section both fillna()\\noptions to see which one is best for our analysis.\\nOur first option would be to plainly ignore the records that have null values. For this, we use the\\ndata frame  method. It takes three parameters: dropna()\\nhow, which can take the value  or . If  is selected, PySpark will drop records any all any\\nwhere  of the fields are null. In the case of , only the records where allat least one all\\nfields are null will be removed. By default, PySpark will take the  mode. any\\nthresh takes an integer value. If set (its default is None), PySpark will ignore the how\\nparameter and only drop the records with less than thresh non-null values.\\nFinally,  will take an optional list of columns that drop will use to make its subset\\ndecision.\\nIn our case, we want to keep only the records that have a  that is non-null. commercial_ratio\\nWe just have to pass our column to the  parameter, like in listing 5.16. subset\\nListing 5.16 Dropping only the records that have a  value of . commercial_ratio null\\nThis option is completely legitimate, but it removes some records from our data frame. What if\\nwe want to keep everything?5.3 Taking care of null values: drop and fill\\n5.3.1 Dropping it like it’s hot\\nanswer_no_null = answer.dropna(subset=[\"commercial_ratio\"])\\nanswer_no_null.orderBy(\"commercial_ratio\", ascending=False).show(1000, False)\\n# +---------------+-------------------+--------------+---------------------+\\n# |LogIdentifierID|duration_commercial|duration_total|commercial_ratio     |\\n# +---------------+-------------------+--------------+---------------------+\\n# |HPITV          |403                |403           |1.0                  |\\n# |TLNSP          |234455             |234455        |1.0                  |\\n# |MSET           |101670             |101670        |1.0                  |\\n# |TELENO         |545255             |545255        |1.0                  |\\n# |CIMT           |19935              |19935         |1.0                  |\\n# |TANG           |271468             |271468        |1.0                  |\\n# |INVST          |623057             |633659        |0.9832686034602207   |\\n# [...]\\n# |OTN3           |0                  |2678400       |0.0                  |\\n# |PENT           |0                  |2678400       |0.0                  |\\n# |ATN14          |0                  |2678400       |0.0                  |\\n# |ATN11          |0                  |2678400       |0.0                  |\\n# |ZOOM           |0                  |2678400       |0.0                  |\\n# +---------------+-------------------+--------------+---------------------+\\nprint(answer_no_null.count())  # 322\\n101\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 104}), Document(page_content='1.  \\n2.  The yin to  yang is to provide a default value to the null values. For this, PySpark dropna()\\nprovided the  method. This method takes two parameters. fillna()\\nThe , which is either a Python int, float, string or bool. PySpark will only fill thevalue\\ncompatible columns: for instance, if we were to , our fillna(\"zero\")\\n, being a double, would not be filled. commercial_ratio\\nThe same  parameter we encountered in . We can limit the scope of our subset dropna()\\nfilling to only the columns we want.\\nConcretely, a  value in any of our numerical columns means that the value should be zero, null\\nso listing 5.17 fills the null values with 0.\\nListing 5.17 Filling our numerical records with 0 using the  method. fillna()\\nWe have the two additional records that listing 5.16 dropped.5.3.2 Filling values to our heart’s content\\nanswer_no_null = answer.fillna(0)\\nanswer_no_null.orderBy(\"commercial_ratio\", ascending=False).show(1000, False)\\n# +---------------+-------------------+--------------+---------------------+\\n# |LogIdentifierID|duration_commercial|duration_total|commercial_ratio     |\\n# +---------------+-------------------+--------------+---------------------+\\n# |HPITV          |403                |403           |1.0                  |\\n# |TLNSP          |234455             |234455        |1.0                  |\\n# |MSET           |101670             |101670        |1.0                  |\\n# |TELENO         |545255             |545255        |1.0                  |\\n# |CIMT           |19935              |19935         |1.0                  |\\n# |TANG           |271468             |271468        |1.0                  |\\n# |INVST          |623057             |633659        |0.9832686034602207   |\\n# [...]\\n# |OTN3           |0                  |2678400       |0.0                  |\\n# |PENT           |0                  |2678400       |0.0                  |\\n# |ATN14          |0                  |2678400       |0.0                  |\\n# |ATN11          |0                  |2678400       |0.0                  |\\n# |ZOOM           |0                  |2678400       |0.0                  |\\n# +---------------+-------------------+--------------+---------------------+\\nprint(answer_no_null.count())  # 324 \\n102\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 105}), Document(page_content='SIDEBAR The return of the dict\\nYou can also pass a dict to the fillna method, with the column names as\\nkey and the values as dict values. If we were to use this method for our\\nfilling, the code would be like listing 5.18.\\nListing 5.18 Filling our numerical records with 0 using the fillna()\\nmethod and a dict.\\nJust like with , I prefer avoiding the dict approach because I find it agg()\\nless readable. In this case, you can chain multiple  to achieve the fillna()\\nsame result, with better readability.\\nOur program now is devoid of null values, and we have a full list of channels and their\\nassociated ratio of commercial programming.\\nAt the beginning of the chapter, we gave ourselves an anchor question to start exploring the data\\nand uncover some insights. Through the chapter, we’ve assembled a cohesive dataset containing\\nthe relevant information to identify commercial programs and have ranked the channels based on\\nhow much of their programming is commercial. In Listing 5.19, I’ve assembled all the relevant\\ncode blocks introduced in the chapter into a single program you can . The code is spark-submit\\nalso available in the book’s repository, under . The end-of-chapter src/Ch05/commercials.py\\nexercises also use this code.\\nNot counting data ingestion, comments or docstring, our code is a rather small 35 lines of code.\\nWe could play code golf (trying to shrink the number of characters as much as we can), but I\\nthink we’ve struck a good balance between terseness and ease of reading. Once again, we\\nhaven’t paid much attention to the distributed nature of PySpark. Once again, we took a very\\ndescriptive view of our problem and translated it into code via PySpark’s powerful data frame\\nabstraction and rich function ecosystems.\\nThis chapter is the last chapter of the first part of the book. You are now familiar with the\\nPySpark ecosystem and how you can use its main data structure, the data frame, to ingest and\\nmanipulate two very common sources of data, textual and tabular. You know a variety and\\nmethod and functions that can be applied to data frames and columns and can apply those to your\\nown data problem. You can also leverage the documentation provided through the PySpark\\ndocstrings, straight from the PySpark shell.5.4 What was our question again: our end-to-end programanswer_no_null = answer.fillna(\\n    {\"duration_commercial\": 0, \"duration_total\": 0, \"commercial_ratio\": 0}\\n)\\n103\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 106}), Document(page_content='There is a lot more you can get from the plain data manipulation portion of the book. Because of\\nthis, I created appendix F— —— how to use the online PySpark API documentation— —— as a\\nguide to becoming self-reliant using the API documentation. Now that you have a solid\\nunderstanding of the data model and how to structure simple data manipulation programs, adding\\nnew functions to your PySpark quiver will be easy.\\nThe second part of the book builds heavily on what you’ve learned so far.\\nWe dig deeper into PySpark’s data model and find opportunities to refine our code. We\\nwill also look at PySpark’s column types, how they bridge to Python’s types, and how to\\nuse them to improve the reliability of our code.\\nWe also look at how PySpark modernizes SQL, an influential language for tabular data\\nmanipulation, and how you can blend SQL and Python in a single program.\\nWe look at promoting pure Python code to run in the Spark distributed environment. We\\nformally introduce a lower-level structure, the Resilient Distributed Dataset (RDD) and\\nits row-major model. We also look at User Defined Functions (UDF) as a way to\\naugment the functionality of the data frame.\\nFinally, we go beyond rows and columns by using the document-data capacities of\\nPySpark’s data frame.\\n104\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 107}), Document(page_content='Listing 5.19 Our full program, ordering channels by decreasing proportion of\\ncommercials in their airings.\\n\"\"\"commercials.py\\nThis program computes the commercial ratio for each channel present in the\\ndataset.\\n\"\"\"\\nimport os\\nimport pyspark.sql.functions as F\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.appName(\\n    \"Getting the Canadian TV channels with the highest/lowest proportion of commercials.\"\\n).getOrCreate()\\nspark.sparkContext.setLogLevel(\"WARN\")\\n###############################################################################\\n# Reading all the relevant data sources\\n###############################################################################\\nDIRECTORY = \"./data/Ch04\"\\nlogs = spark.read.csv(\\n    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8.CSV\"),\\n    sep=\"|\",\\n    header=True,\\n    inferSchema=True,\\n)\\nlog_identifier = spark.read.csv(\\n    os.path.join(DIRECTORY, \"ReferenceTables/LogIdentifier.csv\"),\\n    sep=\"|\",\\n    header=True,\\n    inferSchema=True,\\n)\\ncd_category = spark.read.csv(\\n    os.path.join(DIRECTORY, \"ReferenceTables/CD_Category.csv\"),\\n    sep=\"|\",\\n    header=True,\\n    inferSchema=True,\\n).select(\\n    \"CategoryID\",\\n    \"CategoryCD\",\\n    F.col(\"EnglishDescription\").alias(\"Category_Description\"),\\n)\\ncd_program_class = spark.read.csv(\\n    \"./data/Ch03/ReferenceTables/CD_ProgramClass.csv\",\\n    sep=\"|\",\\n    header=True,\\n    inferSchema=True,\\n).select(\\n    \"ProgramClassID\",\\n    \"ProgramClassCD\",\\n    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),\\n)\\n###############################################################################\\n# Data processing\\n###############################################################################\\nlogs = logs.drop(\"BroadcastLogID\", \"SequenceNO\")\\n105\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 108}), Document(page_content='PySpark implements seven join functionalities, using the common \"what, on what, how?\"\\nquestions: cross, inner, left, right, full, left semi and left anti. Choosing the right join\\nmethod depends on how to process the records that resolve the predicates and the ones\\nthat do not.\\nPySpark keeps lineage information when joining data frames. Using this information, we\\ncan avoid column naming clashes.\\nYou can group similar values together using the  method on a data frame. The groupby()\\nmethod takes a number of column objects or strings representing columns and returns a \\n object. GroupedData\\nGroupedData  objects are transitional structures. They contain two types of columns: the \\n columns, which are the one you grouped by with, and the , which is akey group cell\\ncontainer for all the other columns. In order to return to a data frame, the most common\\nway is to summarize the values in the column via the  function, or via one of the agg()\\ndirect aggregation methods, such as  or . count() min()5.5 Summarylogs = logs.withColumn(\\n    \"duration_seconds\",\\n    (\\n        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\\n        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\\n        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\\n    ),\\n)\\nlog_identifier = log_identifier.where(F.col(\"PrimaryFG\") == 1)\\nlogs_and_channels = logs.join(log_identifier, \"LogServiceID\")\\nfull_log = logs_and_channels.join(cd_category, \"CategoryID\", how=\"left\").join(\\n    cd_program_class, \"ProgramClassID\", how=\"left\"\\n)\\nanswer = (\\n    full_log.groupby(\"LogIdentifierID\")\\n    .agg(\\n        F.sum(\\n            F.when(\\n                F.trim(F.col(\"ProgramClassCD\")).isin(\\n                    [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\\n                ),\\n                F.col(\"duration_seconds\"),\\n            ).otherwise(0)\\n        ).alias(\"duration_commercial\"),\\n        F.sum(\"duration_seconds\").alias(\"duration_total\"),\\n    )\\n    .withColumn(\\n        \"commercial_ratio\", F.col(\"duration_commercial\") / F.col(\"duration_total\")\\n    )\\n    .fillna(0)\\n)\\nanswer.orderBy(\"commercial_ratio\", ascending=False).show(1000, False)\\n106\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 109}), Document(page_content='Using the data from the  (not a typo)— —— careful: the delimiter data/Ch04/Call_Signs.csv\\nhere is the comma, not the pipe!— —— add the  to our final table to display Undertaking_Name\\na human-readable description of the channel.\\nThe government of Canada is asking for your analysis, but they’d like the  to be weighted PRC\\ndifferently. They’d like each  second to be considered 0.75 commercial second. Modify the PRC\\nprogram to account for this change.\\nOn the data frame returned from , return the percentage of channels in each commercials.py\\nbucket based on their . (Hint: look at the documentation for  in how to commercial_ratio round\\ntruncate a value.)5.6 Exercises\\n5.6.1 Exercise 5.4\\n5.6.2 Exercise 5.5\\n5.6.3 Exercise 5.6\\ncommercial_ratio proportion_of_channels\\n1.0\\n0.9\\n0.8\\n…\\n0.1\\n0.0\\n107\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 110}), Document(page_content='6\\nThis chapter covers:\\nData is beautiful.\\nWe give data physical qualities like \"beautiful\", \"tidy\" or \"ugly\", but it doesn’t have the same\\ndefinition there as it would have for a physical object. The same aspect applies to the concept of\\n\"data quality\": what makes a high-quality data set?\\nThis Chapter will focus on bringing meaning to the data you ingest and process. While we can’t\\nalways explain everything in data just by looking at it, just peeking at how some data is\\nrepresented can lay the foundation of a successful data product. We will look at how PySpark\\norganizes data within a data frame to accommodate a wide variety of use-cases. We’ll talk about\\ndata representation though types, how they can guide our operations and how to avoid common\\nmistakes when working with then.\\nThis Chapter will give you a solid foundation to ingest various data sources and have a head-start\\nat cleaning and giving context to your data. Data cleaning is one of the dark arts of data science\\nand nothing quite replaces experience; knowing how your toolset can support your exploration\\nand will make it easier to go from a messy data set to a useful one. In terms of actual stepsMaking sense of your data: types, structure\\nand semantic\\nHow PySpark encodes pieces of data inside columns, and how their type conveys\\nmeaning about what operations you can perform on a given column.\\nWhat kind of types PySpark provides, and how they relate to Python’s type\\ndefinition.\\nHow PySpark can represent multi-dimensional data using compound types.\\nHow PySpark structures columns inside a data frame, and how you can provide a\\nschema to manage said structure.\\nHow to transform the type of a column and what are the implications of doing so.\\nHow PySpark treats null values and how you can work with them.\\n108\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 111}), Document(page_content='performed, data cleaning is most-similar to data manipulation: everything you’ve learned in Part\\n1 will be put to good use, and we’ll continue the discovery in the next chapters.\\nI will approach this Chapter a little differently than the ones in Part 1, as there will not be a main\\ndata set and problem statement that will follow us along. Since we want to see the results of our\\nactions, working with small, simple data frames will make it easier to understand the behaviour\\nof PySpark.\\nFor this Chapter and as a convention for the rest of the book, we will be using the following\\nqualified imports. We saw  in Part 1, and we will be adding pyspark.sql.functions\\n to the party. It is common PySpark practice to alias them to  and pyspark.sql.types F T\\nrespectively. I like to have them capitalized, so it’s obvious what comes from PySpark and what\\ncomes from a regular Python module import (which I keep lower-case).\\nData in itself is not remarkably interesting. It’s what we’re able to do with it that makes the eyes\\nof data-driven people sparkle. Whether you’re developing a high-performing ETL (extract,\\ntransform and load) pipeline for your employer or spending a late night fitting a machine\\nlearning model, the first step is always to understand the data at hand. Anybody who worked in\\ndata can attest: this is easier said than done. Because it’s really easy to access data, people and\\norganizations tend to focus most of their efforts collecting and accumulating data, and relatively\\nless time organizing and documenting it.\\nWhen working with data, and PySpark is no exception, I consider that there are three layers of\\nunderstanding data efficiently. Here they are, in order of increasing complexity:\\nAt the lower level, there are , which gives us guidance on what individual pieces of types\\ndata stored mean in terms of value and the operations we can and cannot perform.\\nPySpark use types on each column;\\nThen there is , which is how we organize columns within a data frame for both structure\\neasy and efficient processing;\\nFinally, at the top level, we find , which uses and goes beyond types and semantic\\nstructure. It answers the question about  the data is about and  we can use it, what how\\nbased on the content. This is usually answered by data documentation and governance\\nprinciples.\\nThis chapter will focus on the first two aspects, which are types and structure, with a little bit of\\nexploration about the semantic layer and how PySpark treats null values. Deriving full semantic\\ninformation from your data in context-dependent, and we will cover it through each use-case in\\nthe book. If we look at the use cases we’ve encountered so far, we’ve approached the semantic\\nlayer in two different ways. In practice, and this chapter is no exception, those three concepts are6.1 Open sesame: what does your data tell you?import pyspark.sql.functions as F\\nimport pyspark.sql.types as T\\n109\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 112}), Document(page_content='intertwined. At the risk of selling a punch, demonstrate that the data frame’s structural backbone\\nis also a type! Nonetheless, thinking about type, structure and semantic separately is a useful\\nthought exercise to understand what our data is, how it is organized and what it means.\\nChapters 2 and 3— counting word occurrences from a body of text— was a pretty simple\\nexample from a data understanding point of view. Each line of text became a string of characters\\n(type) in a PySpark data frame (structure), which we tokenized as words (semantic). Those\\nwords were cleaned using heuristics based on a rudimentary understanding of the English\\nlanguage before being lower-cased and grouped together. We didn’t need much more in terms of\\ndocumentation or support material as the data organization and meaning were self-explanatory.\\nChapters 4 and 5— exploring the proportion of commercial time versus total air time\\n(semantic)— was inputted in tabular form (structure) with mostly numerical and text formats\\n(types). To interpret accurately each code in the table, and how they relate to one another, it\\nrequired access to a data dictionary (semantic). The government of Canada thankfully provided\\ngood enough documentation for us to understand what each code meant and to be able to filter\\nout useless records.\\nThe next chapters will have different data sources, each with their way of forming a semantic\\nlayer. As we saw during Part 1, this is \"part of the job\" and we often don’t think about it. It’s just\\nwhen we don’t understand what we have in front of you that documentation feels missed.\\nPySpark won’t make your incomprehensible data crystal clear by itself, but having a solid\\nfoundation with the right type and structure will avoid some headache.\\nSIDEBAR The structure of unstructured data\\nChapter 2 tackled the ingestion and processing of what is called \\n data. Isn’t it an oxymoron to talk about structure in this case? unstructured\\nIf we take a step back and recall the problem at hand, we wanted to\\ncount work occurrences from one (or many) text files. The data started\\nunstructured while it was in a bunch of text files, but we gave it structure (a\\ndata frame with 1 column, containing one line of text per cell) before\\nstarting the transformation.\\nBecause of the data frame abstraction, we implicitly imposed structure\\nto our text collection to be able to work with it. A PySpark data frame can\\nbe very flexible into what the cells can contain but wraps any kind of data it\\ningests into the  and  abstraction, which we cover in . We’ll see in Column Row\\nChapter 9 how the RDD approaches structure differently, where it shines,\\nand how we can move from one to the other easily. (Spoiler alert: the data\\nframe is most often the easiest way to go)\\n110\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 113}), Document(page_content='When manipulating large data sets, we lose the privilege of examining each record one by one to\\nderive the meaning of our data set. In Part 1, you already learned a few handy methods and how\\nto work with them to display either a sample of the data or a summary of it.\\n.show(n)  will display  records in easy-to-read format. Passing the method without a parameter n\\nwill default to 20 records.\\ndescribe(*cols)  will perform and display basic statistics (non-null count, mean, standard\\ndeviation, min, and max) for numeric and string columns.\\nsummary(*statistics)  will perform the statistics passed as a parameter (which you can find in\\nthe  module) for all numeric and string columns. If you don’t passpyspark.sql.functions\\nparameters to the method, it will compute the same statistics as , with the addition of describe\\nthe approximate quartiles (25%, 50% and 75%).\\nOne aspect we didn’t pay much attention is when introducing those functions is the fact that they\\nonly operate on numerical and string data. When you take a step back and reflect on this, it\\nmakes sense: to compute statistics, you need to be able to apply them to your values. One way\\nthat PySpark maintains order in the realm of possible and impossible operations is via the usage\\nof types.\\nA  is, simply put, information that allows the computer to encode and decode data fromtype\\nhuman representation to computer notation. At the core, computers only understand bits (which\\nare represented in human notation by zeroes and ones). In itself,  doesn’t mean 01100101\\nanything. If you give additional context— let’s say we’re dealing with an integer here— the\\ncomputer can translate this binary representation to a human-understandable value, here 101. If\\nwe were to give this binary representation a string type, this would lead to the letter  in ASCII e\\nand UTF-8. Knowing the binary representation of data types is not necessary for using PySpark’s\\ntypes efficiently. The most important thing to remember is that it’s necessary to understand what\\nkind of data your data frame contains in order to perform the right transformations. Each column\\nhas one and only one type associated to it.\\nThe type information is available when you print the name of the data frame by itself. It can also\\nbe displayed in a much friendlier form though the  method. We’ve used this printSchema()\\nmethod to reflect on the composition of our data frames all through Part 1, and this will keep on\\ngoing. This will give you each column name, its type, and if it accepts null values or not (which\\nwe’ll cover in ). Let’s take a very simple data frame to demonstrate four different categories of\\ntypes. The ingestion and schema display happens in . We are again relying on PySpark’s schema\\ninference capabilities, using . This parameter, when set to , tells inferSchema=True True6.2 The first step in understanding our data: PySpark’s scalar\\ntypes\\n111\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 114}), Document(page_content='PySpark to go over the data twice: the first time to infer the types of each column, and the second\\ntime to ingest the data according to the inferred type. This effectively make sure you won’t get\\nany type mismatch— which can happen when you only sample a subset of records to determine\\nthe type— at the expense of a slower ingestion.\\nListing 6.1 Ingesting a simple data frame with 4 columns of different type, string,\\ninteger, double and date/timestamp\\n was inferred as being a  type ( , ) string_column string StringType()\\n was inferred as being an  type` ( , ) integer_column integer IntegerType()\\n was inferred as being a  type ( , ) float_column double DoubleType()\\n was inferred as being a  type (  ) date_column timestamp TimestampType()\\nPySpark provides a type taxonomy that shares a lot of similarity to Python, with some more\\ngranular types for performance and compatibility with other languages Spark speaks. Both and \\nprovide a summary of PySpark’s types and how they relate to Python’s types. Both table and\\nimage are also available in the book’s GitHub repository, should you want to keep a copy handy.\\nPySpark’s types constructors, which you can recognize easily by their form ( , ElementType()\\nwhere  will be the type used) live under the  module. In , I provide Element pyspark.sql.types\\nboth the type constructor, as well as the string short-hand which will be especially useful in\\nChapter 7, but can also be used in place of the type constructor. Both will be explained and used\\nfrom to .$ cat ./data/Ch05/sample_frame.csv\\n# string_column,integer_column,float_column,date_column\\n# blue,3,2.99,2019-04-01\\n# green,7,6.54,2019-07-18\\n# yellow,10,9.78,1984-07-14\\n# red,5,5.17,2020-01-01\\nsample_frame = spark.read.csv(\\n    \"../../data/Ch06/sample_frame.csv\", inferSchema=True, header=True\\n)\\nsample_frame.show()\\n# +-------------+--------------+------------+-------------------+\\n# |string_column|integer_column|float_column|        date_column|\\n# +-------------+--------------+------------+-------------------+\\n# |         blue|             3|        2.99|2019-04-01 00:00:00|\\n# |        green|             7|        6.54|2019-07-18 00:00:00|\\n# |       yellow|            10|        9.78|1984-07-14 00:00:00|\\n# |          red|             5|        5.17|2020-01-01 00:00:00|\\n# +-------------+--------------+------------+-------------------+\\nsample_frame.printSchema()\\n# root\\n#  |-- string_column: string (nullable = true) \\n#  |-- integer_column: integer (nullable = true) \\n#  |-- float_column: double (nullable = true) \\n#  |-- date_column: timestamp (nullable = true) \\n112\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 115}), Document(page_content='Figure 6.1 An illustrated summary of PySpark’s types taxonomy, with Python equivalentsTable 6.1 A summary of the types in PySpark. A star means there are loss of m\\nprecision.\\nType Constructor String representation Python equivalent\\nNullType() null None\\nStringType() string Python’s regular strings\\nBinaryType() N/A bytearray\\nBooleanType() boolean bool\\nDateType() date datetime.date  (from the datetime\\nlibrary)\\nTimestampType() timestamp datetime.datetime  (from the \\n library) datetime\\nDecimalType(p,s) decimal decimal.Decimal  (from the decimal\\nlibrary)*\\nDoubleType() double float\\nFloatType() float float*\\nByteType() byte  or tinyint int*\\nIntegerType() int int*\\nLongType() long  or bigint int*\\nShortType() short  or smallint int*\\nArrayType(T) N/A list, tuple or Numpy array (from the \\n library)numpy\\nMapType(K, V) N/A dict\\nStructType([…]) N/A list or tuple\\n113\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 116}), Document(page_content='Because some types are very similar, I grouped them into logical entities, based on their\\nbehavior. We’ll start with string and bytes, before going through numerical values. Date and time\\nstructures will follow, before null and boolean. We’ll finish by introducing  structures: complex\\nthe array and the map.\\nSIDEBAR The eternal fight between Python and the JVM\\nSince Spark is a Scala framework first and foremost, a lot of the type\\nconventions comes from how Scala represents types. This has one main\\nadvantage: you can move from Spark using another language to PySpark\\nwithout having to worry about the introduction of type-related bugs. When\\nworking on a platform that spans multiple languages, this consistency is\\nreally important.\\nIn Chapter 8, when we discuss about creating user-defined functions\\n(UDF), the situation will get a little more complicated, as UDF use\\nPython-defined types. We will at this point discuss strategies to avoid\\nrunning into the trap of type-mismatch.\\nStrings and bytes are probably the easiest types to reason about, as they map one-to-one to\\nPython types. A PySpark string can be reasoned just like a Python string. For information that is\\nbetter represented in a binary format, such as images, videos, and sounds, the  is BinaryType()\\nvery well suited and is analogous to the  in Python, which serves the same purpose. bytearray\\nWe already used  and will continue using it through this book. Binary columns StringType()\\nrepresentation is a little less ubiquitous, but we see it in action in Chapter 11 when I introduce\\ndeep learning on Spark.\\nThere are many things you can do with  columns in PySpark. So much, in fact, that by String\\ndefault, PySpark provides only a handful of functions on them. This is where UDF (covered in\\nChapter 8) will become incredibly useful. A couple interesting functions are provided in \\n to encode and decode string information, such as string-represented pyspark.sql.functions\\nJSON (Chapter 10), date or datetime values (Section ), or numerical values (). You also get a\\nfunction to concatenate multiple string or binary columns together ( , or concat() concat_ws()\\nif you want a delimiter between string columns only), compute their length ( ). length()\\nTIP By default, Spark stores strings using the UTF-8 encoding. This is a sane\\ndefault for many operations, and is consistent with Python’s internal\\nrepresentation too.6.2.1 String and bytes\\n114\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 117}), Document(page_content='Listing 6.2 A simplistic example of string manipulation\\nimport pyspark.sql.functions as F\\nfruits = [\\n    [\"Apple\", \"Pomme\", \"\"],\\n    [\"Banana\", \"Banane\", \"\"],\\n    [\"Grape\", \"Raisin\", \"\"],\\n]\\ndf = spark.createDataFrame(fruits, [\"English\", \"French\", \"Russian\"])\\ndf.printSchema()\\n# root\\n# |-- English: string (nullable = true)\\n# |-- French: string (nullable = true)\\n# |-- Russian: string (nullable = true)\\ndf.show()\\n# +-------+------+--------+\\n# |English|French| Russian|\\n# +-------+------+--------+\\n# |  Apple| Pomme|  |\\n# | Banana|Banane|   |\\n# |  Grape|Raisin||\\n# +-------+------+--------+\\ndf = df.withColumn(\\n    \"altogether\", F.concat(F.col(\"English\"), F.col(\"French\"), F.col(\"Russian\"))\\n)\\ndf = df.withColumn(\\n    \"binary_encoded\", F.encode(F.col(\"altogether\"), \"UTF-8\")  \\n)\\ndf.select(\"altogether\", \"binary_encoded\").show(10, False)\\n# +-------------------+----------------------------------------------------------------------------------+\\n# |altogether         |binary_encoded                                                                    |\\n# +-------------------+----------------------------------------------------------------------------------+\\n# |ApplePomme   |[41 70 70 6C 65 50 6F 6D 6D 65 D1 8F D0 B1 D0 BB D0 BE D0 BA D0 BE]               |\\n# |BananaBanane  |[42 61 6E 61 6E 61 42 61 6E 61 6E 65 D0 91 D0 B0 D0 BD D0 B0 D0 BD]               |\\n# |GrapeRaisin|[47 72 61 70 65 52 61 69 73 69 6E D0 B2 D0 B8 D0 BD D0 BE D0 B3 D1 80 D0 B0 D0 B4]|\\n# +-------------------+----------------------------------------------------------------------------------+\\ndf.withColumn(\\n    \"binary_encoded\", F.encode(F.col(\"altogether\"), \"UTF-8\")\\n).withColumn(\"length_string\", F.length(F.col(\"altogether\"))).withColumn(\\n    \"length_binary\", F.length(F.col(\"binary_encoded\"))\\n).select(\\n    \"altogether\", \"binary_encoded\", \"length_string\", \"length_binary\"\\n).show(\\n    10\\n)\\n# +-------------------+--------------------+-------------+-------------+\\n# |         altogether|      binary_encoded|length_string|length_binary|\\n# +-------------------+--------------------+-------------+-------------+\\n# |   ApplePomme|[41 70 70 6C 65 5...|           16|           22| \\n# |  BananaBanane|[42 61 6E 61 6E 6...|           17|           22|\\n# |GrapeRaisin|[47 72 61 70 65 5...|           19|           27|\\n# +-------------------+--------------------+-------------+-------------+\\n115\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 118}), Document(page_content='1.  \\n2.  \\n3.  We get the raw bytes from the UTF-8 encoded strings in column together\\n and  return different values since a character length_string length_binary\\n(here, Cyrillic characters) can be encoded in more than 1 byte in UTF-8.\\nComputer representation of numbers is a fascinating and complicated subject. Besides the fact\\nthat we count in base 10, while a computer is fundamentally a base 2 system, many trade-offs are\\nhappening between speed, memory usage, precision, and scale. Unsurprisingly, PySpark gives a\\nlot of flexibility in how you want to store your numerical values.\\nPython, in version 3, streamlined its numeric types and made it much more user-friendly.\\nDecimal values (such as 14.8) are now called  and are stored as double-precision float\\nfloating-point numbers. Integer values (with no decimal values, such as 3 or -349,257,234) are\\nstored as  values. Numerical values can take as much memory as necessary in Python: the int\\nruntime of the language will allocate enough memory to store your value until you run out of\\nRAM. For more specialized applications, Python also provides rationals (or fractions) and the \\n type, for when you can’t afford the loss of precision happening with floating-point Decimal\\nnumbers. If you are interested in this, Appendix D has a portion about the differences between \\n and .float Decimal\\nPySpark follows Spark’s convention and provides a much more granular numerical tower. You\\nhave more control over memory consumption of your data frame, at the expense of keeping track\\nof the limitations of your chosen type.\\nWe have three main representations of numerical values in PySpark.\\nInteger values, which encompass , ,  and Byte Short Integer Long\\nFloating-point values, which encompass  and Float Double\\nDecimal values, which contains only Decimal\\nRationals aren’t available as a type in PySpark.\\nThe definition of integer, floating-point, and decimal values are akin to what we expect from\\nPython. PySpark provides multiple types for integer and floating-point: the difference between\\nall those types is how many bytes are being used to represent the data in memory. More bytes\\nmeans a larger span of numbers representable (in the case of integer) or more precision possible\\n(for floating-point numbers). Fewer bytes means a smaller memory footprint which, when you’re\\ndealing with a huge number of records, can make a difference in memory usage. With those\\ndifferent types, you have more control regarding how much memory will each value occupy. \\nsummarizes the boundaries for integer.6.2.2 The numerical tower(s): integer values\\n116\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 119}), Document(page_content='1.  \\n2.  \\n3.  \\n4.  \\n5.  Since Python’s integers are limited only by the memory of your system, there is always a\\npossible loss of information available when reading very large (both positive or negative) integer\\nvalues. By default, PySpark will default to a  to this problem as much as possible. LongType()\\nCompared to Python, working with a more granular integer tower can cause some headaches at\\ntimes. PySpark helps a little in that department by allowing numerical types to be mixed and\\nmatched when performing arithmetic. The resulting type will be determined by the input types:\\nIf both types are identical, the return type will be the input types (  +  = float float\\n) float\\nIf both types are integral, PySpark will return the largest of the two (  +  = ) int long long\\nDecimal + Integral Type = Decimal\\nDouble + Any type = Double\\nFloat + Any type (besides ) = Double Float\\nRemembering all this from the start isn’t practical nor convenient. It’s best to remember what the\\ntypes mean and know that PySpark will accommodate the largest memory footprint, without\\nmaking assumptions on the data. Since floats and doubles are less precise than integers and\\ndecimals, PySpark will use those types the moment it encounters them.\\nWhen working with bounded numerical types (which means types that can only represent a\\nbounded interval of values), an important aspect to remember is  and . overflow underflow\\nOverflow is when you’re storing a value too big for the box you want to put it into. Underflow is\\nthe opposite, where you’re storing a value too small (too large negatively) for the box it’s meant\\nto go into. There are two distinct cases where overflow can happen and both exhibit different\\nbehaviour.\\nThe first one is when you’re performing an arithmetic operation on integral values that results\\nthem overflowing the box they’re into. displays what happens when you multiply two shorts\\n(bounded from -32,768 to 32,767) together.Table 6.2 Integer representation in PySpark: memory footprint, maximum and m\\nminimum values\\nType Constructor Number of bits used Min value (inclusive) Max value (inclusive)\\nByteType() 8 -127 128\\nShortType() 16 -32768 32767\\nIntegerType() 32 2,147,483,648 2,147,483,647\\nLongType() 64 -9,223,372,036,854,775,808 9,223,372,036,854,775,807\\n117\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 120}), Document(page_content='Listing 6.3 Overflowing  values in a data frame leads to a promotion to the Short\\nappropriate integer type.\\nSince PySpark defaults integer casting to , we’re re-casting every column of Long\\nour data frame into  using  and a list comprehension over the Short select\\ncolumns.\\nOur values are comfortably within the range of shorts.\\nMultiplying 404 and 1,926 leads to -8,328, and multiplying 530 and 2,047 leads to\\n-29,202 when using short values.\\nPySpark’s default behaviour when dealing with overflow or underflow is to  the values. roll over\\nKnowing how rolling-over works isn’t necessary to productively use PySpark unless you’re\\ncounting on that behaviour for your application.short_values = [[404, 1926], [530, 2047]]\\ncolumns = [\"columnA\", \"columnB\"]\\nshort_df = spark.createDataFrame(short_values, columns)\\nshort_df = short_df.select(\\n    *[F.col(column).cast(T.ShortType()) for column in columns]\\n)  \\nshort_df.printSchema()\\n# root\\n# |-- columnA: short (nullable = true)\\n# |-- columnB: short (nullable = true)\\nshort_df.show()\\n# +-------+-------+\\n# |columnA|columnB|\\n# +-------+-------+\\n# |    404|   1926|\\n# |    530|   2047| \\n# +-------+-------+\\nshort_df = short_df.withColumn(\"overflow\", F.col(\"columnA\") * F.col(\"columnB\"))\\nshort_df\\n# DataFrame[columnA: smallint, columnB: smallint, overflow: smallint]\\nshort_df.show()\\n# +-------+-------+--------+\\n# |columnA|columnB|overflow|\\n# +-------+-------+--------+\\n# |    404|   1926|   -8328|\\n# |    530|   2047|  -29202| \\n# +-------+-------+--------+\\n118\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 121}), Document(page_content='SIDEBAR Rolling over: how integer values are represented in memory\\nIf you’ve never had the opportunity to explore how a computer represents\\nintegers in memory, you might find the overflow behaviour a little hard to\\nunderstand. Here is a quick explanation of how integers are stored and\\nwhat happen when you cast a column into a type too small to contain a\\ngiven value.\\nSpark represents integer values using  integers. The first bit is signed\\nthe sign bit and has special behaviour. When set to 1, it will  two substract\\nto the power of the number of remaining bits (7 in the case of a , 15 in Byte\\nthe case of a , and so on). This is how you can get negative values. Short\\nThe rest of the bits are, when set to 1, adding two to the power of their\\nrank, which starts at n-2 (where n is the number of bits) to 0. (An easier\\nway to do it is to start in reverse and start from 0 and increase the\\nexponent). shows how the value 101 is represented in an 8-bit integer\\nvalue.\\nFigure 6.2 A representation of a 8-bit integer storing the value 101 (ByteType)\\nTaking the first record in , 404 is  when represented 00000001 10010100\\nwith 16 bits and 1,926 is . Multiplying those two values 00000111 10000110\\nyields 778,104, which takes more than 16 bits to represent ( 00000000\\n when using 32 bits). PySpark will keep only 00001011 11011111 01111000\\nthe last 16 bits, ignoring the first ones. Our result will then be 11011111\\n, or -8,328. 01111000\\nInteger overflow and underflow happens in many programming\\nlanguages, such as Scala, Java and C and can be the source of\\nhard-to-find bugs. Integer overflow checking adds some overhead that isn’t\\nalways necessary or wanted, so PySpark leaves this in the control of the\\ndeveloper.\\nThe second time where you can be bitten by integer overflow/underflow is when you’re\\ningesting extremely large values. PySpark, when inferring the schema of your data, will default\\n119\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 122}), Document(page_content='integer values to . What happens when you’re trying to read something beyond the bounds Long\\nof a ? The code in shows what happens in practice when reading large integers.Long\\nListing 6.4 Ingesting numerical values in PySpark\\nPySpark refuses to read the value, and silently defaults to a  value. If you want to deal with null\\nlarger numbers than what  provides,  or  () will allow for larger numbers. Long Double Decimal\\nWe will also see in how you can take control over the schema of your data frame and set your\\nschema at ingestion time.\\nFloats have a similar story, with the added complexity of managing scale (what is the range of\\nnumbers you can represent) and precision (how many digits after the decimal point can you\\nrepresent).\\nA  precision number will encode both integer and decimal part in a 32-bits value.single\\nA  precision number will encode both integer and decimal part in a 64-bits valuedouble\\n(double the bits of a single).\\nPySpark will play nice here and default to  when reading decimal values, which DoubleType()\\nprovides a behavior identical to Python. Some specific applications, such as specialized machine\\nlearning libraries will work faster with single-precision numbers. Encoding arbitrary-precision\\nnumbers in a binary format is a fascinating subject. Because Python defaults to doubles when\\nworking with floating-point values, do the same unless you’re short on memory or want to\\nsqueeze the most performance out of your Spark cluster.\\nFloats  and  are amazing on their own. A  can represent a value up to 1.8 x 10 ,Double Double\\nwhich is much more than a . One caveat of this number format is . Because they Long precision\\nuse base 2 (or binary) representation, some numbers can’t be perfectly represented. shows a308df_numerical = spark.createDataFrame(\\n    [[2 ** 16, 2 ** 33, 2 ** 65]],\\n    schema=[\"sixteen\", \"thirty-three\", \"sixty-five\"],\\n)\\ndf_numerical.printSchema()\\n# root\\n# |-- sixteen: long (nullable = true)\\n# |-- thirty-three: long (nullable = true)\\n# |-- sixty-five: long (nullable = true)\\ndf_numerical.show()\\n# +-------+------------+----------+\\n# |sixteen|thirty-three|sixty-five|\\n# +-------+------------+----------+\\n# |  65536|  8589934592|      null|\\n# +-------+------------+----------+\\n6.2.3 The numerical tower(s): double, floats and decimals\\n120\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 123}), Document(page_content='simple example using floats and double. In both cases, adding and subtracting the same value\\nleads to a slight imprecision. The  result is much more precise as we can see, and since it Double\\nfollows Python’s conventions, I usually prefer it.\\nListing 6.5 Demonstrating the precision of double and floats\\nFinally, there is a wild card type: the . Where integers and floating-point values DecimalType()\\nare defined by the number of bits (base 2) they use for encoding values, the decimal format uses\\nthe same base 10 we use currently. It trades computation performance for a representation we’re\\nmore familiar with. This is often unnecessary for most applications, but if you’re dealing with\\nmoney or values where you can’t afford to be even a little bit off (such as money calculations),\\nthen  will be worth it.Decimal\\nDecimal  values in PySpark take two parameters: a  and a . Precision will be the precision scale\\nnumber of digits total (both integer and decimal part) we want to represent, where scale will be\\nthe number of digits in the decimal part. For instance, a  will allow eight digits Decimal(10, 2)\\nbefore the decimal point and two after. Taking the same example as we did with floats and\\ndouble, but applying it to decimals, lead to . I’ve selected a precision of 10 and a scale of 8,\\nwhich means 2 digits before the decimal point and 8 after, for a total of 10.doubles = spark.createDataFrame([[0.1, 0.2]], [\"zero_one\", \"zero_two\"])\\ndoubles.withColumn(\\n    \"zero_three\", F.col(\"zero_one\") + F.col(\"zero_two\") - F.col(\"zero_one\")\\n).show()\\n# +--------+--------+-------------------+\\n# |zero_one|zero_two|         zero_three|\\n# +--------+--------+-------------------+\\n# |     0.1|     0.2|0.20000000000000004|\\n# +--------+--------+-------------------+\\nfloats = doubles.select([F.col(i).cast(T.FloatType()) for i in doubles.columns])\\nfloats.withColumn(\\n    \"zero_three\", F.col(\"zero_one\") + F.col(\"zero_two\") - F.col(\"zero_one\")\\n).show()\\n# +--------+--------+----------+\\n# |zero_one|zero_two|zero_three|\\n# +--------+--------+----------+\\n# |     0.1|     0.2|0.20000002|\\n# +--------+--------+----------+\\n121\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 124}), Document(page_content='Listing 6.6 Demonstrating the precision of decimal. We don’t have loss of\\nprecision after performing arithmetic.\\nRepresenting date and time in a format both humans and computers can understand is one of\\nthose things that look simple, but really aren’t. When you start talking about leap years, leap\\nseconds (yes, they are a thing), timezones and week numbers, it’s easy to start making mistakes.\\nOn top of this, not everyone in the world agrees about how you should  represent a date, textually\\nso parsing them can become quite the problem (is 7/11 July 11th or November 7th?).\\nPySpark provides a type to represent a date without time— — and one with the time DateType()\\nincluded— . The timestamp one is much more useful since it is a super-set of TimestampType()\\nthe date. (You can think of a date as a timestamp with the time internally set to midnight.) You\\ncan naturally parse  and  Python objects into PySpark datetime.date datetime.datetime Date\\nand  naturally, just like we did for numerical values.Timestamp\\nInternally, PySpark will store a timestamp as the number of seconds since the Unix epoch, which\\nis 1970-01-01 00:00:00 UTC (Coordinated Universal Time). The code in demonstrates how an\\ninteger is converted to a timezone-aware timestamp object. PySpark will read the integer value,\\ninterpret it as the number of seconds since the Unix epoch, and then convert the result into the\\nappropriate timestamp for the timezone your cluster is in. We force a timezone via \\n, which allow the override of the configuration of certain settings of your Spark spark.conf.set\\ncluster for the duration of your session.  contains the timezone spark.sql.session.timeZone\\ninformation, which is used for displaying timestamp values in an easy to digest format.doubles = spark.createDataFrame([[0.1, 0.2]], [\"zero_one\", \"zero_two\"])\\ndecimals = doubles.select(\\n    [F.col(i).cast(T.DecimalType(10, 8)) for i in doubles.columns]\\n)\\ndecimals.withColumn(\\n    \"zero_three\", F.col(\"zero_one\") + F.col(\"zero_two\") - F.col(\"zero_one\")\\n).show()\\n# +----------+----------+----------+\\n# |  zero_one|  zero_two|zero_three|\\n# +----------+----------+----------+\\n# |0.10000000|0.20000000|0.20000000|\\n# +----------+----------+----------+\\n6.2.4 Date and timestamp\\n122\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 125}), Document(page_content='Listing 6.7 Converting an integer into a timestamp object according to three\\ntimezones\\nSpark will infer your timezone based on the timezone settings of your cluster.\\nHere, we force the timezone to be set to a specific one, to demonstrate how the\\nsame integer value will yield different timestamp results.\\nConfirming our timezone via spark.conf.get\\n0 seconds since  UTC, converted to UTC, is 1970-01-01 00:00:00 1970-01-01\\n. 00:00:00\\nSince Toronto is 5 hours behind UTC when daylight saving time isn’t in force, we\\nhave  or 5 hours before the Unix epoch.1969-12-31 19:00:00\\nWarsaw is 1 hour ahead of UTC when daylight saving time isn’t in force so we\\nhave 1970-01-01 01:00:00\\nWARNING Although Pyspark’s  objects are similarly constructed, you can’t cast Date\\nan integer into a  object. Doing so will lead to a runtime error. Dateinteger_values = spark.createDataFrame(\\n    [[0], [1024], [2 ** 17 + 14]], [\"timestamp_as_long\"]\\n)\\nfor ts in [\"UTC\", \"America/Toronto\", \"Europe/Warsaw\"]:\\n    spark.conf.set(\"spark.sql.session.timeZone\", ts)  \\n    ts_values = integer_values.withColumn(\\n        \"ts_{}\".format(ts), F.col(\"timestamp_as_long\").cast(T.TimestampType())\\n    )\\n    print(\\n        \"==={}===\".format(spark.conf.get(\"spark.sql.session.timeZone\"))\\n    )  \\n    ts_values.show()\\n# ===UTC===\\n# +-----------------+-------------------+\\n# |timestamp_as_long|            ts_UTC |\\n# +-----------------+-------------------+\\n# |                0|1970-01-01 00:00:00| \\n# |             1024|1970-01-01 00:17:04|\\n# |           131086|1970-01-02 12:24:46|\\n# +-----------------+-------------------+\\n#\\n# ===America/Toronto===\\n# +-----------------+-------------------+\\n# |timestamp_as_long| ts_America/Toronto|\\n# +-----------------+-------------------+\\n# |                0|1969-12-31 19:00:00| \\n# |             1024|1969-12-31 19:17:04|\\n# |           131086|1970-01-02 07:24:46|\\n# +-----------------+-------------------+\\n#\\n# ===Europe/Warsaw===\\n# +-----------------+-------------------+\\n# |timestamp_as_long|   ts_Europe/Warsaw|\\n# +-----------------+-------------------+\\n# |                0|1970-01-01 01:00:00| \\n# |             1024|1970-01-01 01:17:04|\\n# |           131086|1970-01-02 13:24:46|\\n# +-----------------+-------------------+\\n123\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 126}), Document(page_content='1.  \\n2.  Most of the time, we won’t provide date and timestamp values as an integer: they’ll be displayed\\ntextually, and PySpark will need to parse them into something useful. Converting date and time\\nfrom their string representation to a representation you can perform arithmetic and apply\\nfunctions on is tremendously useful.\\nSIDEBAR ISO8601 your way to sanity\\nBy default, PySpark will display the date and timestamp using something\\nakin to the ISO-8601 format. This means . If you yyyy-MM-dd HH:mm:ss\\nhave any control over how your data is stored and used, I cannot\\nrecommend following the standard enough. It will put to rest the \\n (American format) vs.  (elsewhere in the world) MM/dd/yyyy dd/MM/yyyy\\nambiguity that drove me nuts when I was growing up. It also solves the\\nAM/PM debate: use the 24-hour system!\\nIf you are curious about the different ways ISO-8601 allows dates to be\\nprinted, the Wikipedia page ( ) is a https://en.wikipedia.org/wiki/ISO_8601\\npretty good reference. The ISO standard goes more in details, but you’ll\\nneed to pay for it ( ). https://www.iso.org/iso-8601-date-and-time-format.html\\nTwo methods are available for reading string values into a date ( ) and a timestamp ( to_date()\\n). They both work the same ways. to_timestamp()\\nThe first, mandatory, parameter is the column you with to convert.\\nThe second, optional, parameter is the semantic format of your string column.\\nPySpark uses Java’s  grammar to parse date and timestamps string values, SimpleDateFormat\\nand also to echo them into string. The full documentation is available on Java’s website (\\n) and I recommend https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html\\nyou have a read if you plan on parsing dates and timestamps. I use a subset of the functionality\\navailable to parse \"american-style\" date and time.\\nIn , we see that PySpark will naturally read ISO-8601-like string columns. When working with\\nanother format, the second parameter (called ) provides the simple grammar for reading. format\\nshows the format string that will parse . 04/01/2019 5:39PM\\n124\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 127}), Document(page_content='Listing 6.8 Reading string column\\nI am forcing the timezone to UTC so your code will be the same wherever you are\\non the planet.\\nThe  will try to infer the semantic format of the string column. Itto_timestamp()\\nworks well with  formatted datetime. yyyy-MM-dd hh:mm\\nYou can also pass date formatted differently, as long as you provide a format\\nparameter to show PySpark how your values are meant to be read.spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")  \\nsome_timestamps = spark.createDataFrame(\\n    [[\"2019-04-01 17:39\"], [\"2020-07-17 12:45\"], [\"1994-12-03 00:45\"]],\\n    [\"as_string\"],\\n)\\nsome_timestamps = some_timestamps.withColumn(\\n    \"as_timestamp\", F.to_timestamp(F.col(\"as_string\"))  \\n)\\nsome_timestamps.show()\\n# +----------------+-------------------+\\n# |       as_string|       as_timestamp|\\n# +----------------+-------------------+\\n# |2019-04-01 17:39|2019-04-01 17:39:00|\\n# |2020-07-17 12:45|2020-07-17 12:45:00|\\n# |1994-12-03 00:45|1994-12-03 00:45:00|\\n# +----------------+-------------------+\\nmore_timestamps = spark.createDataFrame(\\n    [[\"04/01/2019 5:39PM\"], [\"07/17/2020 12:45PM\"], [\"12/03/1994 12:45AM\"]],\\n    [\"as_string\"],\\n)\\nmore_timestamps = more_timestamps.withColumn(\\n    \"as_timestamp\", F.to_timestamp(F.col(\"as_string\"), \"M/d/y h:mma\")  \\n)\\nmore_timestamps.show()\\n# +------------------+-------------------+\\n# |         as_string|       as_timestamp|\\n# +------------------+-------------------+\\n# | 04/01/2019 5:39PM|2019-04-01 17:39:00|\\n# |07/17/2020 12:45PM|2020-07-17 12:45:00|\\n# |12/03/1994 12:45AM|1994-12-03 00:45:00|\\n# +------------------+-------------------+\\nthis_will_fail_to_parse = more_timestamps.withColumn(\\n    \"as_timestamp\", F.to_timestamp(F.col(\"as_string\"))  \\n)\\nthis_will_fail_to_parse.show()\\n# +------------------+------------+\\n# |         as_string|as_timestamp|\\n# +------------------+------------+\\n# | 04/01/2019 5:39PM|        null|\\n# |07/17/2020 12:45PM|        null|\\n# |12/03/1994 12:45AM|        null|\\n# +------------------+------------+\\n125\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 128}), Document(page_content='If you don’t, PySpark will silently default them to  values. null\\nFigure 6.3 Parsing 04/01/2019 5:39PM as a timestamp object using PySpark/Java’s\\n`SimpleDateFormat’s grammar\\nOnce you have your values properly parsed, PySpark provide many functions on date and\\ntimestamp columns. , , , , ,  will return the year() month() day() hour() minute() second()\\nrelevant portion of your  or  object, when appropriate. You can add and subtract Date Timestamp\\ndays using  and  and even computer the number of days between two date_add() date_diff()\\ndates using  (careful about not adding an underscore!). datediff()\\n126\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 129}), Document(page_content='Listing 6.9 Manipulating time-stamps with PySpark functions\\nFinally, date and timestamp columns are incredibly useful when doing time-series analysis and\\nwhen using windowing. We will cover this in Chapter 9.\\nThose types are not related, but they are so simple in their construction that they don’t warrant a\\nsection to themselves.\\nA boolean in PySpark is simply a bit (meaning a  or  value), where 0 means false and 1 means 0 1\\ntrue. It is by definition the smallest bit of information a piece of data can carry (1 bit can encode\\n2 values, here ). It is just like booleans in most other programming languages, True/False\\nincluding Python, with one key exception. In Python, every type evaluate implicitly to a\\nBoolean, which allows code like such.import datetime as d\\nsome_timestamps = (\\n    spark.createDataFrame(\\n        [[\"2019-04-01 17:39\"], [\"2020-07-17 12:45\"], [\"1994-12-03 00:45\"]],\\n        [\"as_string\"],\\n    )\\n    .withColumn(\"as_timestamp\", F.to_timestamp(F.col(\"as_string\")))\\n    .drop(\"as_string\")\\n)\\nsome_timestamps = (\\n    some_timestamps.withColumn(\\n        \"90_days_turnover\", F.date_add(F.col(\"as_timestamp\"), 90)\\n    )\\n    .withColumn(\"90_days_turnunder\", F.date_sub(F.col(\"as_timestamp\"), 90))\\n    .withColumn(\\n        \"how_far_ahead\",\\n        F.datediff(F.lit(d.datetime(2020, 1, 1)), F.col(\"as_timestamp\")),\\n    )\\n    .withColumn(\\n        \"in_the_future\",\\n        F.when(F.col(\"how_far_ahead\") < 0, True).otherwise(False),\\n    )\\n)\\nsome_timestamps.show()\\n# +-------------------+----------------+-----------------+-------------+-------------+\\n# |       as_timestamp|90_days_turnover|90_days_turnunder|how_far_ahead|in_the_future|\\n# +-------------------+----------------+-----------------+-------------+-------------+\\n# |2019-04-01 17:39:00|      2019-06-30|       2019-01-01|          275|        false|\\n# |2020-07-17 12:45:00|      2020-10-15|       2020-04-18|         -198|         true|\\n# |1994-12-03 00:45:00|      1995-03-03|       1994-09-04|         9160|        false|\\n# +-------------------+----------------+-----------------+-------------+-------------+\\n6.2.5 Null and boolean\\ntrois = 3\\nif trois:\\n   print(\"Trois!)\\n# Trois!\\n127\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 130}), Document(page_content='PySpark’s conditionals, available through the  function (Chapter 2), will require a when()\\nBoolean test. You can’t just use  as this F.when(F.col(\"my_column\"), \"value_if_true\")\\nwill result in a runtime error. One advantage of requiring explicit booleans for truth testing is that\\nyou don’t have to remember what equates to  and what equates to . True False\\nNull values in PySpark represent the absence of value. It is most similar to  in Python. Just None\\nlike in SQL (which we’ll cover briefly in Chapter 7), null values are absorbing in PySpark,\\nmeaning that passing null as an input to a Spark function will return a null value. This is standard\\nbehavior in most data processing libraries. When creating our custom functions, called UDF or \\n, gracefully dealing with null values will be important to keep that User Defined Functions\\nconsistency.\\nnull  in PySpark is consistent with how SQL treats them, which means it’s a little different than\\nother data manipulations libraries, such as Pandas. In PySpark and Spark,  is a value just null\\nlike any other one, and will be kept when applying functions. The best canonical example is\\nwhen you  a data frame containing  values in the key. Where would delete those groupBy null\\nnull keys by default, PySpark will keep them. In general, I prefer this consistent approach of\\nconsidering all  to be a distinct value, as it avoids dropping columns. It makes exploratory null\\ndata analysis a whole lot easier since null values are displayed when summarizing data. On the\\nother hand, it means that filtering for nulls— if you want to get rid of them— is something you\\nneed to remember explicitly.\\nIn PySpark, testing for null values within a column is done through the  and isNull()\\n methods (careful about the case!). isNotNull()\\n128\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 131}), Document(page_content='Listing 6.10 Grouping a data frame containing null values\\nOne of our values is null (converted from Python’s ) None\\nGrouping our column creates a grouping for the  values null\\nRemoving the  values with a  method. null where\\nNull values can create problems when creating a data frame. PySpark will not be able to infer the\\ntype of a column if it only contains only null values. The simplest example is provided in , but\\nthis can also happen when reading CSV files where there aren’t any values in one of the\\ncolumns. When confronted with this problem, you can provide your own schema information.\\nThis is covered in .\\nListing 6.11 Reading a null column leads to a TypeErrorsome_nulls = spark.createDataFrame(\\n    [[1], [2], [3], [4], [None], [6]], [\"values\"]  \\n)\\nsome_nulls.groupBy(\"values\").count().show()\\n# +------+-----+\\n# |values|count|\\n# +------+-----+\\n# |  null|    1| \\n# |     6|    1|\\n# |     1|    1|\\n# |     3|    1|\\n# |     2|    1|\\n# |     4|    1|\\n# +------+-----+\\nsome_nulls.where(F.col(\"values\").isNotNull()).groupBy(\\n    \"values\"\\n).count().show()  \\n# +------+-----+\\n# |values|count|\\n# +------+-----+\\n# |     6|    1|\\n# |     1|    1|\\n# |     3|    1|\\n# |     2|    1|\\n# |     4|    1|\\n# +------+-----+\\nthis_wont_work = spark.createDataFrame([None], \"null_column\")\\n# A heap of stack traces.\\n#\\n# [...]\\n#\\n# TypeError: Can not infer schema for type: <class \\'NoneType\\'>\\n129\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 132}), Document(page_content='PySpark’s ability to use complex types inside the data frame is what allows its remarkable\\nflexibility. While you still have the tabular abstraction to work with, your cells are supercharged\\nsince they can contain more than just a value. It’s just like going from 2D to 3D, and even\\nbeyond!\\nA  type isn’t really complex: I often use the term  or  type and willcomplex container compound\\ninterchangeably during the book. In a nutshell, the difference between them and simple/scalar\\ntypes is their ability to contain more than a single value. In Python, the main complex types are\\nthe list, the tuple, and the dictionary. In PySpark, we have the array, the map, and the struct.\\nWith those 3, you will be able to express an infinite amount of data layout.\\nThe simplest complex type in PySpark is the array. It is not our first encounter: we used it\\nnaturally in Chapter 2 when we transformed lines of text into arrays of words.\\nA PySpark array can be loosely thought as a Python list or a tuple. It is a container for multiple\\nunordered elements . When using the type constructor for an array, you need to of the same type\\nspecify the type of elements it will contain. For instance, an array of longs will be written as a \\n. shows a simple example of an array column inside a data frame. You ArrayType(LongType())\\ncan see that the display syntax is identical to the Python one for lists, using the square bracket\\nand a comma as a delimiter. Because of the default behavior of  in PySpark, it’s always show()\\nbetter to remove the truncation to see a little bit more inside the cell.\\nListing 6.12 Making a PySpark data frame containing an array column, using\\nPython lists\\nThe type of the column we created is array6.3 PySpark’s complex types\\n6.3.1 Complex types: the array\\narray_df = spark.createDataFrame(\\n    [[[1, 2, 3]], [[4, 5, 6]], [[7, 8, 9]], [[10, None, 12]]], [\"array_of_ints\"]\\n)\\narray_df.printSchema()\\n# root\\n# |-- array_of_ints: array (nullable = true) \\n# |    |-- element: long (containsNull = true) \\narray_df.show()\\n# +-------------+\\n# |array_of_ints|\\n# +-------------+\\n# |    [1, 2, 3]|\\n# |    [4, 5, 6]|\\n# |    [7, 8, 9]|\\n# |    [10,, 12]| \\n# +-------------+\\n130\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 133}), Document(page_content='Every  column contains a sub-entry called . Here, our elements are array element\\n, to the resulting type is long Array(Long)\\nAn array can contain null values. Of course, reading an column with only arrays\\nfilled with null values will yield a , just like for columns TypeError\\nIn Chapter 2, we saw a very useful method— — for converting arrays in rows of explode()\\nelements. PySpark provides many other methods to work with arrays. This time, the API is quite\\nfriendly: the methods are prefixed with , when working with a single array column, or array_*\\n when working with two or more. As an example, let’s take an extended version of arrays_*\\nrock-paper-scissors: the Pokemon type chart. Each Pokemon can have one or more types, which\\nmakes perfect sense for an array. The file we’ll be reading our information from comes from\\nBulbapedia , which I’ve cleaned to keep the canonical type for each Pokemon. The file is a DSV7\\nusing the tab character as a delimiter. We’ll use the array type to answer a quick question: which\\nis the most popular  (Pokemons with two types) in the Pokedex? dual-type\\nThe code in exposes one way to answer the question. We read the DSV file, and then take the\\nfirst and second type into an array. Since single-type Pokemons have a second type of , null\\nwhich is cumbersome to explicitly remove, we duplicate their type before building the array (for\\nexample, ). Once the arrays are built, we dedupe the types, keep only [Fire,]  [Fire, Fire]\\nthe dual ones (where the type array contains more than a single deduped value). It’s then just a\\nmatter of group, count, order, and print, which we’ve done plenty.\\n131\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 134}), Document(page_content='Listing 6.13 Identifying the most popular dual-type Pokemons with PySpark’s\\narrays\\npokedex = spark.read.csv(\"../../data/Ch06/pokedex.dsv\", sep=\"\\\\t\").toDF(\\n    \"number\", \"name\", \"name2\", \"type1\", \"type2\"  \\n)\\npokedex.show(5)\\n# +------+----------+----------+-----+------+\\n# |number|      name|     name2|type1| type2|\\n# +------+----------+----------+-----+------+\\n# |  #001| Bulbasaur| Bulbasaur|Grass|Poison|\\n# |  #002|   Ivysaur|   Ivysaur|Grass|Poison|\\n# |  #003|  Venusaur|  Venusaur|Grass|Poison|\\n# |  #004|Charmander|Charmander| Fire|  null|\\n# |  #005|Charmeleon|Charmeleon| Fire|  null|\\n# +------+----------+----------+-----+------+\\n# only showing top 5 rows\\npokedex = (\\n    pokedex.withColumn(\\n        \"type2\",\\n        F.when(F.isnull(F.col(\"type2\")), F.col(\"type1\")).otherwise(  \\n            F.col(\"type2\")\\n        ),\\n    )\\n    .withColumn(\"type\", F.array(F.col(\"type1\"), F.col(\"type2\")))  \\n    .select(\"number\", \"name\", \"type\")\\n)\\npokedex.show(5)\\n# +------+----------+---------------+\\n# |number|      name|           type|\\n# +------+----------+---------------+\\n# |  #001| Bulbasaur|[Grass, Poison]|\\n# |  #002|   Ivysaur|[Grass, Poison]|\\n# |  #003|  Venusaur|[Grass, Poison]|\\n# |  #004|Charmander|   [Fire, Fire]|\\n# |  #005|Charmeleon|   [Fire, Fire]|\\n# +------+----------+---------------+\\n# only showing top 5 rows\\npokedex.withColumn(\"type\", F.array_sort(F.col(\"type\"))).withColumn(\\n    \"type\", F.array_distinct(F.col(\"type\"))  \\n).where(\\n    F.size(F.col(\"type\")) > 1  \\n).groupby(\\n    \"type\"\\n).count().orderBy(\\n    \"count\", ascending=False\\n).show(\\n    10\\n)\\n# +-----------------+-----+\\n# |             type|count|\\n# +-----------------+-----+\\n# | [Flying, Normal]|   27|\\n# |  [Grass, Poison]|   14|\\n# |    [Bug, Flying]|   13|\\n# |    [Bug, Poison]|   12|\\n# |    [Rock, Water]|   11|\\n# |  [Ground, Water]|   10|\\n# |   [Ground, Rock]|    9|\\n# |  [Flying, Water]|    8|\\n132\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 135}), Document(page_content='We use the  method to specify our column names since the file doesn’t toDF()\\nhave a header\\nSince the array can contain null values, we duplicate the first type if the second one\\nis null. Since  doesn’t equal to himself, we avoid the problem of removing null\\nnull values from the array (see question X.X) //TODO!\\nWe build our array using the  function, which will clump columns of the array()\\nsame type in a single array column.\\nWe remove duplicate types (single-type Pokemons) with the array_distinct()\\nfunction.\\nWe keep only dual-type Pokemons using the  function, which will compute size\\nthe length of any compound structure inside a column.\\nFinally, one neat aspect of the array is that you can access elements by position just like a Python\\nlist, using the bracket notation. Should we wanted to keep only the first type for each Pokemon\\nonce they’re clumped in an array, we can simply use  on the column. Should you ask for [index]\\nan index beyond what the array contains, PySpark will return null for the rows.\\nWhere we can approximate an array to a list, a PySpark map is akin to a Python dictionary.\\nUnlike Python dictionaries and just like PySpark arrays, a map needs to have consistent types for\\nkeys and values. A map containing string keys and integer values will have the type \\n, as an example. shows a simple example about a MapType(StringType(), IntegerType())\\nmap column inside a data frame, using this time a simpler rock, paper, scissor example.\\nCreating a map from existing columns is done through the  function. This create_map()\\nfunction takes an even number of arguments and will assign the odd columns passed as\\narguments to the map keys, and the even as map values. Just like an array, the bracket syntax\\nworks, but this time we need to pass the key as the argument. If the key doesn’t exist,  will null\\nbe returned. You can also use the dot notation: in our case, the field after the dot would either be \\n or .key value\\nA few functions for the map are available, and just like for arrays, they are prefixed by . map_*# | [Fairy, Psychic]|    7|\\n# |[Flying, Psychic]|    7|\\n# +-----------------+-----+\\n# only showing top 10 rows\\npokedex.select(F.col(\"type\")[0])\\n6.3.2 Complex types: the map\\n133\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 136}), Document(page_content='Listing 6.14 Using a map to encode the rules of rock, paper, scissor\\nThe  column will map to the literal value \"win\" and the strong_against\\n column will map to \"lose\". weak_against# Choice, strong against, weak against\\nrock_paper_scissor = [\\n    [\"rock\", \"scissor\", \"paper\"],\\n    [\"paper\", \"rock\", \"scissor\"],\\n    [\"scissor\", \"paper\", \"rock\"],\\n]\\nrps_df = spark.createDataFrame(\\n    rock_paper_scissor, [\"choice\", \"strong_against\", \"weak_against\"]\\n)\\nrps_df = rps_df.withColumn(\\n    \"result\",\\n    F.create_map(\\n        F.col(\"strong_against\"),\\n        F.lit(\"win\"),\\n        F.col(\"weak_against\"),\\n        F.lit(\"lose\"),  \\n    ),\\n).select(\"choice\", \"result\")\\nrps_df.printSchema()\\n# root\\n# |-- choice: string (nullable = true)\\n# |-- result: map (nullable = false) \\n# |    |-- key: string\\n# |    |-- value: string (valueContainsNull = false)\\nrps_df.show(3, False)\\n# +-------+-------------------------------+\\n# |choice |result                         |\\n# +-------+-------------------------------+\\n# |rock   |[scissor -> win, paper -> lose]| \\n# |paper  |[rock -> win, scissor -> lose] |\\n# |scissor|[paper -> win, rock -> lose]   |\\n# +-------+-------------------------------+\\nrps_df.select(\\n    F.col(\"choice\"), F.col(\"result\")[\"rock\"], F.col(\"result.rock\")  \\n).show()\\n# +-------+------------+----+\\n# | choice|result[rock]|rock|\\n# +-------+------------+----+\\n# |   rock|        null|null|\\n# |  paper|         win| win|\\n# |scissor|        lose|lose|\\n# +-------+------------+----+\\nrps_df.select(F.map_values(F.col(\"result\"))).show()  \\n# +------------------+\\n# |map_values(result)|\\n# +------------------+\\n# |       [win, lose]|\\n# |       [win, lose]|\\n# |       [win, lose]|\\n# +------------------+\\n134\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 137}), Document(page_content='1.  \\n2.  \\n3.  \\n4.  The  column here is a  containing two elements, a  and result map key value\\nPySpark displays maps like an array, but where each key maps to a value with an\\narrow .\\nThe bracket syntax, passing the key as a parameter, works as expected, as is the dot\\nnotation.\\nIn order to extract only the values as an array, the function  is used. map_values()\\nOne of the key weaknesses of the map or the array is that you can’t represent heterogeneous\\ncollections. All of your elements, key or values need to be of the same type. The struct is\\ndifferent: it will allow for elements to be of a different type.\\nFundamentally, a struct is like a map/dictionary, but with string keys and typed value. Each\\nblock of the struct is defined as a . Should we want to represent a struct containing StructField\\na string, a long and a timestamp value, our struct would be defined as such.\\nIf you squint a little, you might think that this looks like a data frame schema, and you’d be right.\\nAs a matter of fact, , or if you prefer, PySpark encodes rows as structs the struct is the\\n. Since you can have a struct as a column, you can nest building block of a PySpark data frame\\ndata frames into one another and represent hierarchical or complex data relationships.\\nI will take back our Pokedex dataset from to illustrate the dual nature of the struct. For the first\\ntime, I will explicitly define the schema and pass it to the data frame ingestion.\\nThe  takes a list of . Those  take two mandatoryStructType StructField StructField\\nparameters and two optional.\\nThe first one is the name of the field, as a string\\nThe second one is the type of the field as a PySpark type\\nThe third one is a boolean flag that lets PySpark know if we expect null values to be\\npresent.\\nA dictionary containing string keys and simple values for metadata (mostly used for ML\\npipelines, Chapter 13)\\nOnce your data frame is created, the way to add a column is to use the  column, from struct()\\n. It will create a struct containing the columns you pass as an pyspark.sql.functions6.4 Structure and type: The dual-nature of the struct\\nT.StructType(\\n  [\\n    T.StructField(\"string_value\", T.StringType()),\\n    T.StructField(\"long_value\", T.LongType()),\\n    T.StructField(\"timestamp_value\", T.TimestampType())\\n  ]\\n)\\n135\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 138}), Document(page_content='argument. In , I use the two purposes of the struct. First, I create a schema that matches my data\\nin DSV format, using  and a list of  to define the names of the StructType StructField\\ncolumns and their types. Once the data loaded in a data frame, I create a struct column, using the \\n,  and  field. Displaying the resulting data frame’s schema shows that the structname type1 type2\\nadds a second level of hierarchy to our schema, just like the array and the map. This time, we\\nhave full control over the name, types, and number of fields. When requested to  a show()\\nsample of the data, PySpark will simply wrap the fields of the struct into square brackets and\\nmake it look like an array of values.\\n136\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 139}), Document(page_content='Listing 6.15 Using the two natures of the struct (container and type)\\npokedex_schema = T.StructType(  \\n    [\\n        T.StructField(\"number\", T.StringType(), False, None),\\n        T.StructField(\"name\", T.StringType(), False, None),\\n        T.StructField(\"name2\", T.StringType(), False, None),\\n        T.StructField(\"type1\", T.StringType(), False, None),\\n        T.StructField(\"type2\", T.StringType(), False, None),\\n    ]\\n)\\npokedex = spark.read.csv(\\n    \"../../data/Ch06/pokedex.dsv\", sep=\"\\\\t\", schema=pokedex_schema\\n)\\npokedex.printSchema()\\n# root\\n# |-- number: string (nullable = true)\\n# |-- name: string (nullable = true)\\n# |-- name2: string (nullable = true)\\n# |-- type1: string (nullable = true)\\n# |-- type2: string (nullable = true)\\npokedex_struct = pokedex.withColumn(\\n    \"pokemon\", F.struct(F.col(\"name\"), F.col(\"type1\"), F.col(\"type2\"))  \\n)\\npokedex_struct.printSchema()\\n# root\\n# |-- number: string (nullable = true)\\n# |-- name: string (nullable = true)\\n# |-- name2: string (nullable = true)\\n# |-- type1: string (nullable = true)\\n# |-- type2: string (nullable = true)\\n# |-- pokemon: struct (nullable = false) \\n# |    |-- name: string (nullable = true)\\n# |    |-- type1: string (nullable = true)\\n# |    |-- type2: string (nullable = true)\\npokedex_struct.show(5, False)\\n# +------+----------+----------+-----+------+--------------------------+\\n# |number|name      |name2     |type1|type2 |pokemon                   |\\n# +------+----------+----------+-----+------+--------------------------+\\n# |#001  |Bulbasaur |Bulbasaur |Grass|Poison|[Bulbasaur, Grass, Poison]| \\n# |#002  |Ivysaur   |Ivysaur   |Grass|Poison|[Ivysaur, Grass, Poison]  |\\n# |#003  |Venusaur  |Venusaur  |Grass|Poison|[Venusaur, Grass, Poison] |\\n# |#004  |Charmander|Charmander|Fire |null  |[Charmander, Fire,]       |\\n# |#005  |Charmeleon|Charmeleon|Fire |null  |[Charmeleon, Fire,]       |\\n# +------+----------+----------+-----+------+--------------------------+\\npokedex_struct.select(\"name\", \"pokemon.name\").show(5)  \\n# +----------+----------+\\n# |      name|      name|\\n# +----------+----------+\\n# | Bulbasaur| Bulbasaur|\\n# |   Ivysaur|   Ivysaur|\\n# |  Venusaur|  Venusaur|\\n# |Charmander|Charmander|\\n# |Charmeleon|Charmeleon|\\n# +----------+----------+\\npokedex_struct.select(\"pokemon.*\").printSchema()  \\n137\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 140}), Document(page_content='1.  \\n2.  I build a manual data frame schema through , which takes a list of StructType()\\n. StructFields()\\nWe can create a struct column with the  function. I am passing 3 columns struct()\\nas parameters, so my struct has those 3 fields.\\nA  adds another dimension into the data frame, containing an arbitrarystruct\\nnumber of fields, each having their types.\\nA struct is displayed just like an array of the value its fields contain\\nI use the dot notation to extract the  field inside the  struct name pokemon\\nTo \"flatten\" the struct, we can use the  notation. The star refers to \"every field in .*\\nthe struct\"\\nTo extract the value from a struct, we can use the dot or bracket notation just like we did on the\\nmap. If you want to select all fields in the struct, PySpark provides a special star field to do so. \\n will give you all the fields of the  struct. pokemon.* pokemon\\nWhen working with a data frame, we are really working with two structures:\\nThe data frame itself, via methods like ,  or  (which, to be select() where() groupBy()\\npedantic, gives you a  object, but we’ll consider them to be analogous) GroupedData\\nThe  object, via functions (from  and UDF (Chapter 8))Column pyspark.sql.functions\\nand methods such as  (see ) and ) cast() alias()\\nThe data frame keeps the order of your data by keeping an ordered collection of columns.\\nColumn name and type are managed at runtime, just like regular Python. It is your responsibility\\nas the developer to make sure you’re applying the right function to the right structure, and that\\nyour types are compatible. Thankfully, PySpark makes it easy and obvious to peek at the data\\nframe’s structure, via  or simply inputting your data frame name on the REPL. printSchema()# root\\n# |-- name: string (nullable = true)\\n# |-- type1: string (nullable = true)\\n# |-- type2: string (nullable = true)\\n6.4.1 A data frame is an ordered collection of columns\\n138\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 141}), Document(page_content='SIDEBAR A statically typed option: the data set\\nPython, Java, and Scala are all strongly typed languages, meaning that\\nperforming an illegal operation will raise an error. Java and Scala are\\nstatically typed on top of that, meaning that the types are known at\\ncompilation time. Type Errors will lead to a compilation error in Java/Scala,\\nwhere Python will raise a run-time error. The same behavior will happen\\nusing the data frame since the types are dynamic.\\nSpark with Scala or Java also provides a statically typed version of the\\ndata frame, called the data set (or , where  are the types inside DataSet[T] T\\nthe data set). When working with a data set in Spark, you know at compile\\ntime the types of your columns. For instance, our data frame in would have\\nbeen a , and doing an DataSet[String, Integer, Double, Timestamp]\\nillegal operation (such as adding 1 to a string) would yield a compile-time\\nerror.\\nThe  structure is not available for PySpark or Spark for R, sinceDataSet\\nthose languages aren’t statically typed. It wouldn’t make much sense to\\nforego the nature of Python, even when using Spark. Just like when coding\\nwith Python, we’ll have to be mindful of our types. When working with\\nintegrating Python functions as User Defined Functions (mostly in Chapter\\n8), we’ll use mypy ( ), an optional static type checker http://mypy-lang.org/\\nfor Python. It will help a little by avoiding easy type errors.\\nPySpark putting so much emphasis on columns is not something new nor revolutionary. SQL\\ndatabases have been doing this for decades now. In fact, Chapter 7 is all about using SQL within\\nPySpark! Unlike SQL, PySpark treats the column like a bona-fide object, which means you can\\nPython your way into where you want to go. When I was myself learning PySpark, this stumped\\nme for quite some time. I believe an example demonstrates it best.\\nSince PySpark exposes  as an object, we can apply functions on it or use its methods. Column\\nThis is what we’ve been doing since Chapter 2. The neat aspect of separating columns from its\\nparent structure (the data frame) is that you can code both separately. PySpark will keep the\\nchain of transformations of the column, independently from the data frame you want to apply it\\n, and wait for a transformation, just like any other data frame transformation. In practice, itto\\nmeans that you can simplify or encapsulate your transformations. A simple example is\\ndemonstrated in . We create a variable  which is a list of two transformations, transformations\\na column rename, and a  transformation. When defining , PySpark when transformations\\nknows nothing about the data frame it’ll be applied two, but happily stores the chain of\\ntransformations, trusting that it’ll be applied to a compatible data frame later.\\n139\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 142}), Document(page_content='Listing 6.16 Simplifying our transformations using Python and the  object Column\\nWe can store our column transformations into a variable (here a Python list called \\n). transformations\\nI apply the chain of transformations to a data frame. It works like if I wrote the list\\ninside the select itself.\\nAlthough a PySpark data frame is all about the columns, there is a Row object we can access. A\\nrow is very similar to a Python dictionary: you’ll have a key-value pair for each column. You can\\nin fact use the  method for marshalling the  into a python dict. asDict() Row\\nThe biggest usage of the  object is when you want to operate the data frame as an RDD. As Row\\nwe will see in Chapter 8, an RDD is closer to row-major as a structure, rather than the\\ncolumn-major nature of the data frame. When converting a data frame into an RDD, PySpark\\nwill create an RDD of . The same applies: if you have an RDD of , it’s very easy to Rows Rows\\nconvert it into a data frame.pokedex = spark.read.csv(\"../../data/Ch06/pokedex.dsv\", sep=\"\\\\t\").toDF(\\n    \"number\", \"name\", \"name2\", \"type1\", \"type2\"  \\n)\\ntransformations = [\\n    F.col(\"name\").alias(\"pokemon_name\"),\\n    F.when(F.col(\"type1\").isin([\"Fire\", \"Water\", \"Grass\"]), True)\\n    .otherwise(False)\\n    .alias(\"starter_type\"),\\n]  \\ntransformations\\n# [Column<b\\'name AS `pokemon_name`\\'>,\\n# Column<b\\'CASE WHEN (type1 IN (Fire, Water, Grass)) THEN true ELSE false END AS `starter_type`\\'>]\\npokedex.select(transformations).printSchema()  \\n# root\\n# |-- pokemon_name: string (nullable = true)\\n# |-- starter_type: boolean (nullable = false)\\npokedex.select(transformations).show(5, False)\\n# +------------+------------+\\n# |pokemon_name|starter_type|\\n# +------------+------------+\\n# |Bulbasaur   |true        |\\n# |Ivysaur     |true        |\\n# |Venusaur    |true        |\\n# |Charmander  |true        |\\n# |Charmeleon  |true        |\\n# +------------+------------+\\n6.4.2 The second dimension: just enough about the row\\n140\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 143}), Document(page_content='Listing 6.17 Listing title\\nThe  object is constructed with an arbitrary number of fields, which do not needRow\\nto be quoted.\\nPySpark will accept a list of  to  directly Row createDataFrame\\nYou can also  a number  of rows into a Python list of  to manipulate them further take n Rows\\nlocally. In practice, you’ll see that the conversion from PySpark data frames to a local pandas\\ndata frame to be much more common. I’ll cover this in greater detail in Chapter 9. You’ll seldom\\nuse the  object directly when working with structured data but it’s nice to know it’s there and Row\\nready for you should you need the added flexibility.\\nWARNING take() always takes a parameter  specifying the number of rows you n\\nwant. Careful not to pass a too large number since it’ll create a local\\nPython list on the master node.\\nNow armed with knowledge about types, we can now look at changing the type of columns in\\nPySpark. This is a common but dangerous operation, as we can either crash our program or\\nsilently default values to null, as we saw in .row_sample = [\\n    T.Row(name=\"Bulbasaur\", number=1, type=[\"Grass\", \"Poison\"]),  \\n    T.Row(name=\"Charmander\", number=4, type=[\"Fire\"]),\\n    T.Row(name=\"Squirtle\", number=7, type=[\"Water\"]),\\n]\\nrow_df = spark.createDataFrame(row_sample)  \\nrow_df.printSchema()\\n# root\\n# |-- name: string (nullable = true)\\n# |-- number: long (nullable = true)\\n# |-- type: array (nullable = true)\\n# |    |-- element: string (containsNull = true)\\nrow_df.show(3, False)\\n# +----------+------+---------------+\\n# |name      |number|type           |\\n# +----------+------+---------------+\\n# |Bulbasaur |1     |[Grass, Poison]|\\n# |Charmander|4     |[Fire]         |\\n# |Squirtle  |7     |[Water]        |\\n# +----------+------+---------------+\\nrow_df.take(3)\\n# [Row(name=\\'Bulbasaur\\', number=1, type=[\\'Grass\\', \\'Poison\\']),\\n#  Row(name=\\'Charmander\\', number=4, type=[\\'Fire\\']),\\n#  Row(name=\\'Squirtle\\', number=7, type=[\\'Water\\'])]\\n6.4.3 Casting your way to sanity\\n141\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 144}), Document(page_content='The casting of a column is via a method on the column itself, called  or  (both cast() asType()\\nare synonyms). It is usually used in conjunction with a method on the data frame, most often \\n or . The code in shows the most common types of casting: from and to select() withColumn()\\na string field.\\nThe  method is very simple. Applied to a  object, it takes a single parameter,cast() Column\\nwhich is the type you want the column to become. If PySpark can’t make the conversion, it’ll\\nnullify the value.\\nListing 6.18 Casting values using the  function cast()\\nCasting our string-encoded decimal numbers into a double\\nCasting our string-encoded dates into a datedata = [\\n    [\"1.0\", \"2020-04-07\", \"3\"],\\n    [\"1042,5\", \"2015-06-19\", \"17,042,174\"],\\n    [\"17.03.04178\", \"2019/12/25\", \"17_092\"],\\n]\\nschema = T.StructType(\\n    [\\n        T.StructField(\"number_with_decimal\", T.StringType()),\\n        T.StructField(\"dates_inconsistently_formatted\", T.StringType()),\\n        T.StructField(\"integer_with_separators\", T.StringType()),\\n    ]\\n)\\ncast_df = spark.createDataFrame(data, schema)\\ncast_df.show(3, False)\\n# +-------------------+------------------------------+-----------------------+\\n# |number_with_decimal|dates_inconsistently_formatted|integer_with_separators|\\n# +-------------------+------------------------------+-----------------------+\\n# |1.0                |2020-04-07                    |3                      |\\n# |1042,5             |2015-06-19                    |17,042,174             |\\n# |17.03.04178        |2019/12/25                    |17_092                 |\\n# +-------------------+------------------------------+-----------------------+\\ncast_df = cast_df.select(\\n    F.col(\"number_with_decimal\")\\n    .cast(T.DoubleType())  \\n    .alias(\"number_with_decimal\"),\\n    F.col(\"dates_inconsistently_formatted\")\\n    .cast(T.DateType())  \\n    .alias(\"dates_inconsistently_formatted\"),\\n    F.col(\"integer_with_separators\")\\n    .cast(T.LongType())  \\n    .alias(\"integer_with_separators\"),\\n)\\ncast_df.show(3, False)\\n# +-------------------+------------------------------+-----------------------+\\n# |number_with_decimal|dates_inconsistently_formatted|integer_with_separators|\\n# +-------------------+------------------------------+-----------------------+\\n# |                1.0|                    2020-04-07|                      3|\\n# |               null|                    2015-06-19|                   null|\\n# |               null|                          null|                   null| \\n# +-------------------+------------------------------+-----------------------+\\n142\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 145}), Document(page_content='Casting our string-encoded integers into a long\\nWhen PySpark can’t convert the value, it silently outputs a . null\\nI consider casting the black-beast of data processing. Conceptually, it shouldn’t be hard: we are\\nsimply re-encoding values in a format more suitable. In practice, there are so many corner cases.\\nTaking back our original  from , PySpark fails to convert five out of nine records we’ve cast_df\\npassed to it, and will silently convert them to null. When working with large data sets, this \"silent\\nbut deadly\" behaviour can mean you’re losing data without realizing it later in your processing.\\nThere is no surefire way to prevent this completely, but keeping a copy of your previous column\\nand making sure you’re not increasing the number of nulls is a good way to make sure you’re not\\nhaving casting problems.  or  is also a good way to check quickly groupBy drop_duplicates()\\nfor potential problems.\\nListing 6.19 Casting carefully, diagnosing the results before signing on them\\nI changed three columns to throw a wrench in our casting\\nUsing  reduces the number of records to analyze. drop_duplicates() groupby()\\ncan perform the same thing.data = [[str(x // 10)] for x in range(1000)]\\ndata[240] = [\"240_0\"]\\ndata[543] = [\"56.24\"]\\ndata[917] = [\"I\\'m an outlier!\"]  \\nclean_me = spark.createDataFrame(\\n    data, T.StructType([T.StructField(\"values\", T.StringType())])\\n)\\nclean_me = clean_me.withColumn(\\n    \"values_cleaned\", F.col(\"values\").cast(T.IntegerType())\\n)\\nclean_me.drop_duplicates().show(10)\\n# +---------------+--------------+\\n# |         values|values_cleaned|\\n# +---------------+--------------+\\n# |             48|            48|\\n# |             81|            81|\\n# |              6|             6|\\n# |             94|            94|\\n# |          240_0|          null| <---+\\n# |I\\'m an outlier!|          null| <---+--\\n# |             41|            41|\\n# |             18|            18|\\n# |             47|            47|\\n# |             89|            89|\\n# +---------------+--------------+\\nclean_me = clean_me.withColumn(\\n    \"values\", F.split(F.col(\"values\"), \"_\")[0]\\n)  \\n143\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 146}), Document(page_content='1.  \\n2.  \\n3.  \\n1.  \\n2.  \\n3.  As a simple example, we split the strings at the underscore character and keep the\\ngroup before. This will take c are of . 240_0\\nKnowing how to tackle corner cases of casting is very dependent on what your data means.\\nThere are no rules on how  should be encoded as an integer. 56.24\\nYou could round up (57)\\nYou could round down or truncate (56)\\nYou could forego the decimal point (5624)\\nThis is a very small foray in the rich and fascinating world of data semantic, or what is the\\n It sits at the crossroad between experience, domain-knowledge and a meaning behind your data?\\nlittle bit of intuition. Remember that data cleaning, which includes casting the right type,\\nTIP For casting dates and timestamps, since their formatting is much more\\nfree-form than numerical values, you can use the  and to_date()\\n functions. If you need a refresher, head up to . to_timestamp()\\nSIDEBAR The perils of CSV\\nCSV is by far the most popular interchange format for tabular data, and\\nthere is no going around it. It’s easy to produce and human-readable. That\\nbeing said, it suffers from three capital flaws for working with PySpark\\nefficiently:\\nit can’t represent nested data (maps, arrays, and nested structs);\\nthe exporting process needs to be careful with field delimiters (to avoid\\nconfusing a comma in a string field versus a comma as a delimiter, for\\ninstance);\\nmore importantly, everything is a string, so you need to cast everything\\nwhen you read it.\\nThose issues are common to other data processing libraries, but since\\nPySpark will usually deal with larger data sets, those flaws are magnified\\nsince you can’t reasonably inspect every record to make sure everything is\\ndone the right way. In Chapter 9, I discuss other file formats that address\\nthose issues, at the expense of not being readable with Excel or a text\\neditor.\\n144\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 147}), Document(page_content='1.  \\n2.  Filling null values in your data frame is the perfect intersection between types and semantic. You\\nneed to provide a value that will suit your use-case but also respect the type of the column. You\\ncould not, for instance, transform your null values into \"NOTHING\" if your column is filled with\\nintegers.\\nThe method for filling null values is called . This is a method of the data frame (like fillna()\\n and ). It takes two parameters, and both are extremely important:where select\\nThe first one is the value you wish to replace the null values with. The type of the value\\nwill determine which columns it can be applied to.\\nThe second is an optional list of columns you want to apply the filling on. If you don’t\\nprovide one, PySpark will apply the fill to all the compatible type columns.\\nThere are two ways to use the method. The first one is to specify a scalar value (like , -1 False\\nor ) and let PySpark do the application automatically. If you need a more fine-tuned\"N/A\"\\napproach, you can pass a dictionary as a first parameter. The keys will map to column names,\\nand the values will be what to replace the null values for that specific column only. In , I\\ndemonstrate both use-cases.\\nfillna()  will only accept scalar values that are float, int, long, string or bool. Passing another\\ntype of value will give a . ValueError6.4.4 Defaulting values with fillna\\n145\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 148}), Document(page_content='Listing 6.20 Filling nulls using the  method, using the scalar and dict fillna()\\napproach.\\nI’m using the  data frame from . cast_df\\nPySpark will use the same type promotion for arithmetic values we’ve seen in . In\\nthis case,  was applied to both integer and floating-point columns. -1\\nYou can chain  repeatedly. They will be applied sequentially. fillna()\\nThe dictionary method can also be used if you need to be precise with how you fill\\ncertain columns.\\nChoosing the right value is dependant on your use-case. For instance, you might want to set all\\nnull integers to -1, before realizing this won’t work if you use the column for machine learning.\\nBecause PySpark treats null values like any other, keeping them around is not harmful as long as\\nyou know how to account for them.\\nThis Chapter might have been more academic than necessary. On the other hand, it contained a\\nlot of essential information about how PySpark \"thinks\" about the values it processes. Knowingcast_df.show(3, False)  \\n# +-------------------+------------------------------+-----------------------+\\n# |number_with_decimal|dates_inconsistently_formatted|integer_with_separators|\\n# +-------------------+------------------------------+-----------------------+\\n# |                1.0|                    2020-04-07|                      3|\\n# |               null|                    2015-06-19|                   null|\\n# |               null|                          null|                   null|\\n# +-------------------+------------------------------+-----------------------+\\ncast_df.fillna(-1).show()  \\n# +-------------------+------------------------------+-----------------------+\\n# |number_with_decimal|dates_inconsistently_formatted|integer_with_separators|\\n# +-------------------+------------------------------+-----------------------+\\n# |                1.0|                    2020-04-07|                      3|\\n# |               -1.0|                    2015-06-19|                     -1|\\n# |               -1.0|                          null|                     -1|\\n# +-------------------+------------------------------+-----------------------+\\ncast_df.fillna(-1, [\"number_with_decimal\"]).fillna(-3).show()  \\n# +-------------------+------------------------------+-----------------------+\\n# |number_with_decimal|dates_inconsistently_formatted|integer_with_separators|\\n# +-------------------+------------------------------+-----------------------+\\n# |                1.0|                    2020-04-07|                      3|\\n# |               -1.0|                    2015-06-19|                     -3|\\n# |               -1.0|                          null|                     -3|\\n# +-------------------+------------------------------+-----------------------+\\ncast_df.fillna({\"number_with_decimal\": -1, \"integer_with_separators\": -3}).show()  \\n# +-------------------+------------------------------+-----------------------+\\n# |number_with_decimal|dates_inconsistently_formatted|integer_with_separators|\\n# +-------------------+------------------------------+-----------------------+\\n# |                1.0|                    2020-04-07|                      3|\\n# |               -1.0|                    2015-06-19|                     -3|\\n# |               -1.0|                          null|                     -3|\\n# +-------------------+------------------------------+-----------------------+\\n146\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 149}), Document(page_content='this will avoid a lot of guessing when you build your own data manipulation programs.\\nRemembering how your data needs the right type, the right structure will give you head-start in\\nfiguring out how to organize your program so it can be maintainable and resilient.\\nDon’t worry if you weren’t able to absorb everything in one read. You can always jump back to\\nthe specific section as you need it. I also recommend keeping exploration notes when you’re\\ndiscovering new data. Patterns will emerge and you’ll develop an intuition, which ultimately will\\nmake you faster. And the faster you’ll be at processing data into what you want, the more time\\nyou can spend building cool products!\\nPySpark has a wide set of types that it uses to convey how data is encoded within a given\\ncolumn. Types determine which functions can be applied to a given column.\\nThere are two main categories of types. The first one is  or  types, which scalar simple\\ninclude string, numbers, both integral and decimal, date and time, boolean, and null\\nvalues. The second one is  or  types, which are akin to container compound complex\\nstructures in other programming languages. PySpark provides the array, the map and the\\nstruct as compound types.\\nThe struct is both a column type and how PySpark constructs a data frame. You can build\\na data frame schema using , where each field will be encoded in StructType\\n. StructFields\\nCasting columns from a type to another is done through the  or cast() astype()\\nmethod. Casting a value into an incompatible type will yield a null value.\\nNull values in PySpark are similar to how they are treated in other SQL databases, where\\nnull is another distinct value. You can identify null values within a column using \\n and replace null values with . isNull()/isNotNull() fillna()\\nHow could you demonstrate that PySpark stores  as  with the time forced to Date Timestamp\\n00:00:00?\\nTaking the  from , how could you fill the null values with the date ? cast_df 1900-01-01\\nThis chapter covers\\nHow PySpark’s own data manipulation module takes inspiration from SQL’s vocabulary\\nand way of doing things.\\nHow to register data frames as temporary views or tables to query them using Spark\\nSQL.\\nHow the catalog stores metadata about registered tables and views, how to list the\\nexisting references and delete them.6.5 Summary\\n6.6 Exercises\\n6.6.1 Exercise 6.1\\n6.6.2 Exercise 6.2\\n147\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 150}), Document(page_content='How common data manipulations are expressed in PySpark and Spark SQL and how you\\ncan move from one to the other.\\nHow to use SQL-style clauses inside certain PySpark methods.\\n148\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 151}), Document(page_content='7\\nMy answer to \"Python versus SQL, which one should I learn?\" is \"yes\".\\nWhen it comes to manipulating tabular data, SQL is the reigning king. For multiple decades now,\\nit has been the workhorse language for relational databases, and even today, learning how to\\ntame it is a worthwhile exercise. Spark acknowledge the power of SQL heads on: you can use a\\nmature SQL API to transform data frames. On top of that, you can also seamlessly blend SQL\\ncode withing your Spark or PySpark program, making it easier than ever to migrate those old\\nSQL ETL jobs without reinventing the wheel.\\nThis chapter is dedicated on SQL interop with PySpark. I will cover how we can move from one\\nlanguage to the other. I will also cover how we can use a SQL-like syntax within data frame\\nmethods to speed up your code and some of trade-offs you can face. Finally, we’ll blend Python\\nand SQL code together to get the best of both worlds.\\nIf you already have notable exposure to SQL, this chapter will be a breeze for you. Feel free to\\nskim over the SQL-specific sections, but don’t skip the sections on Python and SQL interop, as\\nthere is some PySpark idiosyncrasies I cover there. For those brand new to SQL, this will be— I\\nhope— an eye opener moment and you’ll add another tool under your belt. If you feel that you’d\\nlike a deeper dive into SQL,  by Ben Brumm (Manning, 2017) is a good video SQL in motion\\nsource. If you prefer a book, a very exhaustive reference is Joe Celko’s SQL for Smarties\\n(Morgan Kauffman, 2014).\\nIn order to follow the examples in this chapter, here are the imports I am using.Bilingual PySpark: blending Python and\\nSQL code\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.utils import AnalysisException  \\nimport pyspark.sql.functions as F\\nimport pyspark.sql.types as T\\nspark = SparkSession.builder.getOrCreate()\\n149\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 152}), Document(page_content='1.  TIP This chapter will not be a deep dive in SQL rather than a comparison with\\nPySpark and a way to combine both. I am leaving certain concepts out on\\npurpose, but it doesn’t mean you can’t use them if you’ve comfortable with\\nSQL.\\nSIDEBAR ANSI SQL vs. HiveQL\\nSpark supports both ANSI SQL and the vast majority of\\nHiveQLfootnote::[You can see the functionality supported (and unsupported\\non the Spark website: \\n] https://docs.databricks.com/spark/latest/spark-sql/compatibility/hive.html\\nas a SQL dialect. Spark SQL also has some Spark specific functions baked\\nin to ensure a common set of functionality across languages.\\nIn a nutshell, Hive is a SQL-like interface that can be used over a\\nvariety of data storage. It became very popular because it allowed to query\\nfiles in HDFS (Hadoop Distributed File System) like if they were a table.\\nSpark can integrate with Hive when your environment has it installed.\\nSpark SQL also provides additional syntax to work with larger datasets,\\nwhich I will cover as we need it.\\nBecause of the amount of material and its longevity, and also because\\nthey are very similar in syntax for basic and intermediate queries, I usually\\nrecommend learning ANSI SQL at first and then complete with HiveQL as\\nyou go along. This way, your knowledge will transfer to other SQL-based\\nproducts.\\nUnderstanding Hive in depth is something I won’t cover in this book as\\nit is not a component of PySpark. For those who are curious, Practical Hive\\n(Apress, 2016) seems like a good reference.\\nPySpark’s data frame manipulation hints at its SQL heritage: the name of the module— \\n— is a dead giveaway. PySpark developers recognized the heritage of the SQL pyspark.sql\\nprogramming language for data manipulation and used the same vocabulary to name their\\nmethod. Let’s look at a quick example in both SQL and plain PySpark and look at similarities\\nbetween the keywords used. In , I load a CSV containing information about the periodic table of\\nelements and I query the data set to find the number of entries with a  state per period. liquid\\nThe code is presented both in PySpark and SQL form, and without much context, we can see\\nsimilarities between the two.\\nIn , I put the code side by side and illustrate where the similarities start and end. Even though it’s\\na small example, it shows the main differences between PySpark and SQL.7.1 Banking on what we know:  vs plain SQL pyspark.sql\\n150\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 153}), Document(page_content='1.  \\n2.  PySpark will always start with the name of the data frame you are working with. SQL\\nrefers to the table (or ) using a  keyword; target from\\nPySpark chains the transformations and actions as methods on the data frame, where\\nSQL splits them into two groups: the  group and the  group. The first operation condition\\none is before the  clause and operates on columns. The second is after the from from\\nclause and group, filters, and orders the structure of the result table.\\nTIP SQL is not case sensitive, so you can either use lower-case or upper-case.\\nI usually prototype in lower-case then convert in upper-case when I am\\nconfident about my query, to differentiate the two visually.\\nFigure 7.1 The same simple statement, represented in PySpark and SQL\\nBoth would return the same results: one element in period four (Bromine) and one in period six\\n(Mercury).\\nWhether you prefer the order of operations from PySpark and SQL will depend on how you\\nbuild query mentally and how familiar you are with the respective language. Fortunately,\\nPySpark makes it easy to move from on to the other, and even work with both all at once.elements = spark.read.csv(\\n    \"../../data/Ch07/Periodic_Table_Of_Elements.csv\",\\n    header=True,\\n    inferSchema=True,\\n)\\nelements.where(F.col(\"phase\") == \"liq\").groupby(\"period\").count().show()\\n// In SQL: We assume that the data is in a table called `elements`\\nSELECT\\n  period,\\n  count(*)\\nFROM elements\\nWHERE phase = \"liq\"\\nGROUP BY period;\\n151\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 154}), Document(page_content='Since we can think of PySpark data frames like tables on steroids, it’s not farfetched to think\\nabout querying them using a language designed to query tables. Spark provides a full SQL API\\nwhich is documented in the same fashion as the PySpark one (\\n). We also see that the functions defined in https://spark.apache.org/docs/latest/api/sql/index.html\\n are defined. pyspark.sql.functions\\nNOTE Spark’s SQL API only covers the data manipulation subset of Spark. You\\nwon’t be able to do machine learning using SQL, for instance.\\nPySpark maintains boundaries between its own namespacing and Spark SQL’s namespacing.\\nThis means that a data set loaded in PySpark won’t be automatically available to querying using\\nSQL. The code in shows an example of this.\\nListing 7.1 Trying (and failing) at querying a data frame SQL-style\\nHere, PySpark doesn’t make the link between the python variable , which points to the elements\\ndata frame, and a potential table  that can be queried by Spark SQL. In order to allow a elements\\ndata frame to be queried via SQL, we need to  them as tables. I illustrated the process in . register\\nWhen we assign a data frame to a variable, Python points to the data frame. Spark SQL does not\\nhave visibility over the variables Python assigns.\\nWhen you want to create a table to query with Spark SQL, you can use the \\n method. This method takes a single string parameter which is createOrReplaceTempView()\\nthe name of the table you want to use. This transformation will look at the data frame referenced\\nby the Python variable on which the method was applied and will create a Spark SQL reference\\nto the same data frame. We see an example of this in the bottom halr.\\nNOTE Although you can name the table the same name as the variable you are\\nusing, you are not forced to do so. I could have named my table \\n if I wanted to. starneifwpy\\nOnce we have registered our  table that point to the same data frame that our Python elements7.2 Using SQL queries on a data frame\\n7.2.1 Promoting a data frame to a Spark table\\ntry:\\n    spark.sql(\\n        \"select period, count(*) from elements where phase=\\'liq\\' group by period\"\\n    ).show(5)\\nexcept AnalysisException as e:\\n    print(e)\\n# \\'Table or view not found: elements; line 1 pos 29\\'\\n152\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 155}), Document(page_content='variable of the same name, we can query our table without any problem. Let’s re-run the same\\ncode block as and see that it succeds.\\nNOTE In this chapter, I use the term \"table\" and \"view\" pretty loosely. In SQL, they\\nare distinct concepts: the table is materialized in memory and the view is\\ncomputed on the fly. Spark’s temp views are conceptually closer to a view\\nthan a table. Spark SQL also has tables but we will not be using them,\\npreferring reading and materializing our data into a data frame.\\nListing 7.2 Trying (and succeding) at querying a data frame SQL-style\\nWe register our table using the  method on the createOrReplaceTempView()\\n data frame. element\\nThe same query works once Spark is able to dereference the SQL view name.\\nSo now we have a view registered. In the case of a low number of views to manage, it’s pretty\\neasy to keep their name in memory. What about if you have dozens of them or if you need to\\ndelete some? Enter the catalog, Spark’s way of managing its SQL namespace.elements.createOrReplaceTempView(\"elements\")  \\nspark.sql(\\n    \"select period, count(*) from elements where phase=\\'liq\\' group by period\"\\n).show(5)\\n# +------+--------+\\n# |period|count(1)|\\n# +------+--------+\\n# |     6|       1|\\n# |     4|       1|\\n# +------+--------+ \\n153\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 156}), Document(page_content='1.  \\n2.  SIDEBAR Advanced-ish topic: Spark SQL views and persistence\\nTL;DR: Use  as it does what you expect. createOrRepleaceTempView()\\nPySpark has four methods to create temporary views and they look\\nquite similar at first glance.\\ncreateGlobalTempView\\ncreateOrReplaceGlobalTempView\\ncreateOrReplaceTempView\\ncreateTempView\\nWe can see that there is two by two matrix of possibilities:\\nDo I want to replace an existing view ( )? OrReplace\\nDo I want to create a Global view ( )? Global\\nThe first one is relatively easy to answer: if you use  with a createTempView\\nname already being used for another table, the method will fail, where it’ll\\nreplace the reference if you use . In SQL, it is createOrReplaceTempView()\\nequivalent to use  vs. . I personally CREATE VIEW CREATE OR REPLACE VIEW\\nalways use the latter as it mimics Python’s way of doing things: when\\nre-assigning a variable, you comply.\\nWhat about ? The difference between a local view and a global Global\\nview has to do with how long it will last in memory. A local table is tied to\\nyour  where a global table is tied to the Spark application.SparkSession\\nThe differences at this time are not significant at all as we are not using\\nmultiple  that need to share data. Because of this, I usually SparkSession\\ndon’t use the  methods. Global\\nThe Spark catalog is an object that allows working with Spark SQL tables and views. A lot of its\\nmethods has to do with managing the metadata of those tables, such as their name and the level\\nof caching (which I’ll cover in details in Chapter 9). We will look at the most basic set of\\nfunctionality here, leaving the more advanced parts, such as caching and user-defined functions,\\nto their respective chapters.\\nWe can use the catalog to list the tables/views we have registered and drop them if we are done.\\nThe code in provides the simple methods to do those tasks. Since they are mostly mimicing\\nPySpark’s data frame functionality, I think that an example shows it best.7.2.2 Using the Spark catalog\\n154\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 157}), Document(page_content='Listing 7.3 Using the catalog to display our registered view and then drop it\\nThe catalog is reached through the  property of our catalog SparkSession\\nThe  method gives us a list of  objects containing thelistTables Table\\ninformation we want\\nTo delete a view, we use the method , passing the name of the dropTempView()\\nview as a parameter\\nOur catalog now has no table for us to query\\nHow that we understand how we can manage a Spark SQL view within PySpark, we can start\\nlooking at manipulating data using both languages.\\nThe integration between Python (through PySpark) and SQL is very well thought out and can\\nimprove the speed at which we can write code. This section will cover the fundamental case,\\nwhich is using a single language. I will review the most common operations from a pure-SQL\\nand a purist PySpark way, to illustrate how basic manipulations are written.\\nFor the remainder of the chapter, we will be using a public data set provided by BackBlaze\\nwhich provided hard drive data and statistics. Their data is in the gigabytes range, which\\nalthough not \"big\" yet is certainly Spark worthy as it’ll be more than the memory available on\\nyour home computer. For those willing to scale their program beyond a single computer, I\\nrecommend peeking at Appendix B to provision a cloud cluster and use the whole data provided\\non the website. A convenience shell script is also provided for downloading everything in one\\nfell swoop. For those working locally and afraid of blowing your memory, you can use only Q3\\n2019. The syntax will differ marginally between both workflows. A 16GB laptop should be able\\nto use all files.\\nBackblaze provides documentation mostly in the form of SQL statements, which is perfect for\\nwhat we’re learning.7.3 SQL and PySparkspark.catalog  \\n#  <pyspark.sql.catalog.Catalog at 0x117ef0c18>\\nspark.catalog.listTables()  \\n#  [Table(name=\\'elements\\', database=None, description=None,\\n#         tableType=\\'TEMPORARY\\', isTemporary=True)]\\nspark.catalog.dropTempView(\"elements\")  \\nspark.catalog.listTables()  \\n# []\\n155\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 158}), Document(page_content=\"To get the files, you can either download them from the website (\\n) or use the https://www.backblaze.com/b2/hard-drive-test-data.html\\n available in the code repository, which need the  package backblaze_download_data.py wget\\nto be installed. The data needs to be in the  directory. ./data/Ch07\\nListing 7.4 Downloading the data from backblaze\\nWindows users, use dir ..\\\\..\\\\data\\\\Ch07\\nMake sure you unzip the files into the directory before trying to read them. Unlike many other\\ncodecs (Gzip, Bzip2, Snappy and LZO, for instance), PySpark will not decompress zip files\\nautomatically when reading them, so we need to do it ahead of time. The  command can be unzip\\nused if you are using the command line (you might need to install the tool on Linux). On\\nWindows, I usually use the Windows Explorer and unzip by hand.\\nThe code to ingest and prep the data is pretty straightforward. We read each data source\\nseparately, and then we make sure that each data frame has the same columns as its peers. In our\\ncase, the data for the fourth quarter has two more columns than the others, so we add the missing\\ncolumns. When unioning the four data frames together, we use a select method on the data\\nframes so their column order is all the same. We continue by casting all the columns containing a\\nSMART measurement as a Long, since they are documented as integral values. Finally, we\\nregister our data frame as a view so we can use SQL statements on it.$ pip install wget\\n$ python backblaze_download_data.py full\\n# [some data download progress bars]\\n$ ls  ../../data/Ch07   \\n# Periodic_Table_Of_Elements.csv data_Q2_2019.zip               data_Q4_2019.zip\\n# data_Q1_2019.zip               data_Q3_2019.zip\\n$ unzip '../../data/Ch07/*.zip'\\n156\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion\", metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 159}), Document(page_content='Listing 7.5 Reading the backblaze data into a data frame and registering a view\\nOur goal in this section is to perform a quick exploratory data analysis on a subset of the\\ncolumns presented. We will reproduce the failure rates that Backblaze computes themselves and\\nidentify the models with the most and least amount of failures in 2019.\\nThose operations are like the bread and butter of data manipulation, yet we already see a\\ndifference in how SQL orders the operations differently than PySpark. The code in explores a\\nfew models that have failed. This is useful as a quick-and-dirty way to see if the data looks\\nconsistent and see the serial numbers nomenclature.7.4 Using SQL-like syntax within data frame methodsDATA_DIRECTORY = \"../../data/Ch07/\"\\nq1 = spark.read.csv(\\n    DATA_DIRECTORY + \"drive_stats_2019_Q1\", header=True, inferSchema=True\\n)\\nq2 = spark.read.csv(\\n    DATA_DIRECTORY + \"data_Q2_2019\", header=True, inferSchema=True\\n)\\nq3 = spark.read.csv(\\n    DATA_DIRECTORY + \"data_Q3_2019\", header=True, inferSchema=True\\n)\\nq4 = spark.read.csv(\\n    DATA_DIRECTORY + \"data_Q4_2019\", header=True, inferSchema=True\\n)\\n# Q4 has two more fields than the rest\\nq4_fields_extra = set(q4.columns) - set(q1.columns)\\nfor i in q4_fields_extra:\\n    q1 = q1.withColumn(i, F.lit(None).cast(T.StringType()))\\n    q2 = q2.withColumn(i, F.lit(None).cast(T.StringType()))\\n    q3 = q3.withColumn(i, F.lit(None).cast(T.StringType()))\\n# if you are only using the minimal set of data, use this version\\nbackblaze_2019 = q3\\n# if you are using the full set of data, use this version\\nbackblaze_2019 = (\\n    q1.select(q4.columns)\\n    .union(q2.select(q4.columns))\\n    .union(q3.select(q4.columns))\\n    .union(q4)\\n)\\n# Setting the layout for each column according to the schema\\nq = backblaze_2019.select(\\n    [\\n        F.col(x).cast(T.LongType()) if x.startswith(\"smart\") else F.col(x)\\n        for x in backblaze_2019.columns\\n    ]\\n)\\nbackblaze_2019.createOrReplaceTempView(\"backblaze_stats_2019\")\\n7.4.1 Select and where\\n157\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 160}), Document(page_content='1.  \\n2.  \\n3.  Listing 7.6 Comparing  and  in PySpark and SQL select where\\nSince a SQL statement returns a data frame, we still have to  it to see the show()\\nresults.\\nPySpark makes you think about how you want to chain the operations. In our case, we start by\\nfiltering the data frame and then we select the column of interest. SQL presents an alternative\\nconstruction:\\nYou put the columns you want to select at the beginning of your statement. This is called\\nthe SQL operation\\nYou them add one or more tables to query, called the target\\nAfter, you add the , such as filtering. conditions\\nEvery operation we will look in this chapter will be classified as an operation, a target, or a\\ncondition, so you can know where it fits in the statement.\\nTIP If you have a table you want to extract as a data frame, you can just assign\\nthe result of a  statement to variable. SELECT\\nHere, we are looking at the capacity in gigabytes of the hard drives included in the data, by\\nmodel. For this, we use a little bit of arithmetic and the  function, that elevates its first pow()\\nargument to the power of the second. We can see similarities about the SQL and PySpark\\nvocabulary, but once again the order of the transformations is different.spark.sql(\\n    \"select serial_number from backblaze_stats_2019 where failure = 1\"\\n).show(\\n    5\\n)  \\nbackblaze_2019.where(\"failure = 1\").select(F.col(\"serial_number\")).show(5)\\n# +-------------+\\n# |serial_number|\\n# +-------------+\\n# |    57GGPD9NT|\\n# |     ZJV02GJM|\\n# |     ZJV03Y00|\\n# |     ZDEB33GK|\\n# |     Z302T6CW|\\n# +-------------+\\n# only showing top 5 rows\\n7.4.2 Group and order by\\n158\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 161}), Document(page_content='Listing 7.7 Grouping and ordering in PySpark and SQL\\nIn PySpark, once again, we look at the logical order of operations. We  the groupby\\n columns. which is a computed column. Just like in PySpark, arithmetic operations capacity_GB\\ncan be performed using the usual syntax in SQL. Furthermore, the  function— available in pow()\\n— is also implemented in Spark SQL. If you need to see which pyspark.sql.functions\\nfunctions can be used out of the box, the Spark SQL API doc contains the necessary information\\n( ).https://spark.apache.org/docs/latest/api/sql/index.html\\nGrouping and ordering are conditions in SQL, so they are at the end of the statement. One thing\\nworth noting is that we group by  and order by . This is a short-hand way of referring to 1 3 DESC\\nthe columns in the SQL operation by position rather than name. In this case, it saves us from\\nwriting  or group by capacity_bytes / pow(1024, 3) order by max(capacity_bytes /\\n in the conditions block. pow(1024,3)) DESC\\nLooking at the results from our query, there are some drives that report more than one capacity.\\nFurthermore, we have some drives reporting negative capacity, which is really odd. Let’s focus\\non seeing how prevalent this is.\\nLet’s assume that, for each model, the maximum reported capacity is the correct one. Because of\\nhow SQL is evaluated, we can’t refer to an aliased field in our  clause. Because of this, we WHERE\\nhave to rely to another keyword in order to compare our min and max reported capacity. The\\ncode in shows how we can accomplish this in both languages.spark.sql(\\n    \"\"\"SELECT\\n           model,\\n           min(capacity_bytes / pow(1024, 3)) min_GB,\\n           max(capacity_bytes/ pow(1024, 3)) max_GB\\n        FROM backblaze_stats_2019\\n        GROUP BY 1\\n        ORDER BY 3 DESC\"\"\"\\n).show(5)\\nbackblaze_2019.groupby(F.col(\"model\")).agg(\\n    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\\n    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\\n).orderBy(F.col(\"max_GB\"), ascending=False).show(5)\\n# +--------------------+--------------------+-------+\\n# |               model|              min_GB| max_GB|\\n# +--------------------+--------------------+-------+\\n# |       ST16000NM001G|             14902.0|14902.0|\\n# | TOSHIBA MG07ACA14TA|-9.31322574615478...|13039.0|\\n# |HGST HUH721212ALE600|             11176.0|11176.0|\\n# |       ST12000NM0007|-9.31322574615478...|11176.0|\\n# |       ST12000NM0008|             11176.0|11176.0|\\n# +--------------------+--------------------+-------+\\n# only showing top 5 rows\\n7.4.3 Having\\n159\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 162}), Document(page_content='Listing 7.8 Using  in SQL, and relying on  in PySpark having where\\nHaving is a syntax unique to SQL: it can be thought of a  clause that can only be applied to where\\naggregate fields, such as  or . Since it’s equivalent in functionality to count(*) min(date)\\nwhere,  is in the condition block after the  clause. In PySpark, we do not have having group by\\n as a method. Since each method returns a new data frame, we do not have to have a having\\ndifferent keyword, and can just use  with the column we created instead. where\\nWe will ignore (for now) those capacity reporting inconsistencies. They’ll come back as\\nexercises.\\nNow that we have queried the data and are getting the hang of it in SQL, we might want to\\ncheckpoint our work and save some data so we do not have to process everything from scratch\\nthe next time. For this, we can either create a table of a view, which we’ll then be able to query\\ndirectly.\\nCreating a table or a view is very easy in SQL: we just have to prefix our query by CREATE\\n. Here, creating a table or a view will have a different impact. If you have a Hive TABLE/VIEW\\nmetastore connected, creating a table will materialize the data (for more information, seespark.sql(\\n    \"\"\"SELECT\\n           model,\\n           min(capacity_bytes / pow(1024, 3)) min_GB,\\n           max(capacity_bytes/ pow(1024, 3)) max_GB\\n        FROM backblaze_stats_2019\\n        GROUP BY 1\\n        HAVING min_GB != max_GB\\n        ORDER BY 3 DESC\"\"\"\\n).show(5)\\nbackblaze_2019.groupby(F.col(\"model\")).agg(\\n    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\\n    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\\n).where(F.col(\"min_GB\") != F.col(\"max_GB\")).orderBy(\\n    F.col(\"max_GB\"), ascending=False\\n).show(\\n    5\\n)\\n# +--------------------+--------------------+-------+\\n# |               model|              min_GB| max_GB|\\n# +--------------------+--------------------+-------+\\n# | TOSHIBA MG07ACA14TA|-9.31322574615478...|13039.0|\\n# |       ST12000NM0007|-9.31322574615478...|11176.0|\\n# |HGST HUH721212ALN604|-9.31322574615478...|11176.0|\\n# |       ST10000NM0086|-9.31322574615478...| 9314.0|\\n# |HGST HUH721010ALE600|-9.31322574615478...| 9314.0|\\n# +--------------------+--------------------+-------+\\n# only showing top 5 rows\\n7.4.4 Create tables/views\\n160\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 163}), Document(page_content='appendix B) where a view will only keep the query. To take a baking analogy, CREATE TABLE\\nwill store a cake, where  will refer to the ingredients (the original data) and the CREATE VIEW\\nrecipe (the query).\\nTo demonstrate this, I will reproduce the  and  that compute the number of drive_days failures\\ndays of operation a model has and the number of drive failures it has had, respectively. The code\\nin shows how it is done: you prefix your select query with a . CREATE [TABLE/VIEW]\\nIn PySpark, we do not have to rely on extra syntax. A newly created data frame just has to be\\nassigned to a variable and we are good to go.\\nListing 7.9 Creating a view in Spark SQL and in PySpark\\nSIDEBAR Creating tables from data in SQL\\nYou can also create a table from data on a hard drive or HDFS. For this,\\nyou can use a modified SQL query like so. Since we are reading a CSV\\nfile, we prefix our path by . csv.\\nI much prefer relying on PySpark syntax for reading and setting the\\nschema from my data source and then using SQL, but the option is there\\nfor the taking.spark.sql(\\n    \"\"\"\\n    CREATE VIEW drive_days AS\\n        SELECT model, count(*) AS drive_days\\n        FROM drive_stats\\n        GROUP BY model\"\"\"\\n)\\nspark.sql(\\n    \"\"\"CREATE VIEW failures AS\\n           SELECT model, count(*) AS failures\\n           FROM drive_stats\\n           WHERE failure = 1\\n           GROUP BY model\"\"\"\\n)\\ndrive_days = backblaze_2019.groupby(F.col(\"model\")).agg(\\n    F.count(F.col(\"*\")).alias(\"drive_days\")\\n)\\nfailures = (\\n    backblaze_2019.where(F.col(\"failure\") == 1)\\n    .groupby(F.col(\"model\"))\\n    .agg(F.count(F.col(\"*\")).alias(\"failures\"))\\n)\\nspark.sql(\"create table q1 as select *\\n    from csv.`../../Data/Ch07/drive_stats_2019_Q1`\")\\n161\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 164}), Document(page_content='So far, we’ve seen how to query a single table at a time. In practice, you’ll often get multiple\\ntables related to one another. We already witnessed this problem by having one historical table\\nper quarter, which needed to be stacked (or unioned) together, and with our  and drive_days\\n tables, which each paint a single dimension of the story until they are merged (or failures\\njoined) together.\\nJoins and unions are the only clauses we’ll see that are modifying the target piece in our SQL\\nstatement. In SQL, a query is operating on a single target at a time. We already saw at the\\nbeginning of the chapter how to use PySpark to union tables together. In SQL, we follow the\\nsame blueprint: SELECT columns FROM table1 UNION ALL SELECT columns FROM table2\\nWARNING PySpark’s   SQL union() UNION\\nIn SQL,  removes the duplicate records. PySpark’s  doesn’t, UNION union()\\nwhich is why it’s equivalent to a SQL . If you want to drop the UNION ALL\\nduplicates, which is an expensive operation when working in a distributed\\ncontext, use the  function after your . This is one of the distinct() union()\\nrare case where PySpark’s vocabulary doesn’t follow SQL’s, but it’s for a\\ngood reason. Most of the time, you’ll want the  behaviour. UNION ALL\\nIt is always a good idea to make sure that your data frames have the same columns, with the\\nsame types, in the same order, before attempting a union. In the PySpark solution, we used the\\nfact that we can extract the columns in a list to  the data frames in the same fashion. select\\nSpark SQL does not have a simple way to do the same, so one would have to type all the\\ncolumns. It’s alright when you just have a few, but we’re talking hundred here.\\nOne easy way to circumvent this is to use the fact that a Spark SQL statement is a string. We can\\ntake our list of columns, transform it into a SQL-esque string and be done with it. This is exactly\\nwhat I did in . It’s not a pure Spark SQL solution, but it’s much friendlier than making you type\\nall the columns one by one.\\nWARNING Do not allow for plain string insertion if you are processing user input! This\\nis the best way to have a SQL injection, where a user can craft a string that\\nwill wreck havok on your data. For more information about SQL injections\\nand why , have a look at \\n. https://owasp.org/www-community/attacks/SQL_Injection7.4.5 Union and join\\n162\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 165}), Document(page_content='Listing 7.10 Unioning tables together in Spark SQL and in PySpark\\nWe use the  method on a separator string to create a string containing all the join()\\nelements in the list, separated by `, `\\nWe promote our quarterly data frames to Spark SQL views so we can use them in\\nour query\\nThis is taken from \\nJoin are equally as simple in SQL. We add a [DIRECTION] JOIN table [ON] [LEFT COLUMN]\\n in the target portion of our statement. The direction is the same [OP] [RIGHT COLUMN]\\nparameter of our  in PySpark. The  clause is a series of comparison between columns. In how on\\nthe example, we are joining the records where the value in the  column is equal ( ) on both model =\\n and  tables. drive_days failures\\nListing 7.11 Joining the  and  tables together, in Spark SQL drive_days failures\\nand in PySparkcolumns_backblaze = \", \".join(q4.columns)  \\nq1.createOrReplaceTempView(\"Q1\")  \\nq1.createOrReplaceTempView(\"Q2\")\\nq1.createOrReplaceTempView(\"Q3\")\\nq1.createOrReplaceTempView(\"Q4\")\\nspark.sql(\\n    \"\"\"\\n    CREATE VIEW backblaze_2019 AS\\n    SELECT {col} FROM Q1 UNION ALL\\n    SELECT {col} FROM Q2 UNION ALL\\n    SELECT {col} FROM Q3 UNION ALL\\n    SELECT {col} FROM Q4\\n\"\"\".format(\\n        col=columns_backblaze\\n    )\\n)\\nbackblaze_2019 = (  \\n    q1.select(q4.columns)\\n    .union(q2.select(q4.columns))\\n    .union(q3.select(q4.columns))\\n    .union(q4)\\n)\\nspark.sql(\\n    \"\"\"select\\n           drive_days.model,\\n           drive_days,\\n           failures\\n    from drive_days\\n    left join failures\\n    on\\n        drive_days.model = failures.model\"\"\"\\n).show(5)\\ndrive_days.join(failures, on=\"model\", how=\"left\").show(5)\\n163\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 166}), Document(page_content='The last piece of SQL syntax we will look on its own is the subquery and the common table\\nexpression. A lot of SQL references do not talk about them until very late which is a shame\\nbecause they are a) easy to understand and b) are very helpful in keeping your code clean. In a\\nnutshell, they allow to create tables that are local to your query. In Python, this would be similar\\nto using the  statement or using a function block to limit the scope of a query. I will show with\\nthe function approach as it is much more commonfootnote::[The  statement is usually used with\\nwith resources that need to be cleaned up at the end. It doesn’t really apply here, but I felt like\\nthe comparison was worth mentionning.].\\nFor our example, we will take our  and  table definitions and bundle them drive_days failures\\ninto a single query that will measure the models with the highest rate of failure in 2019. The code\\nin shows how we can do this using a subquery. A subquery simply replace a table name with a\\nstand-alone SQL query. In the example, we can see that the name of the table has been replaced\\nby the  query that formed the table. We can alias the \"table\" referred in the subquery by SELECT\\nadding the name at the end of the statement, after the closing parenthesis.\\nListing 7.12 Finding the drive models with the highest rate of failure using\\nsubqueries\\nSubqueries are cool but can be hard to read and debug, since you are adding some complexity\\ninto the main query. This is where common table expressions, or CTE, are especially useful. A\\nCTE is a table definition, just like in the subquery case. The difference here is that you put them\\nat the top of your main statement (before your main ) and prefix with the word . In , SELECT WITH7.4.6 Subqueries and common table expressions\\nspark.sql(\\n    \"\"\"\\n    SELECT\\n        model,\\n        failures / drive_days failure_rate\\n    FROM (\\n        SELECT\\n            model,\\n            count(*) AS drive_days\\n        FROM drive_stats\\n        GROUP BY model) drive_days\\n    INNER JOIN (\\n        SELECT\\n            model,\\n            count(*) AS failures\\n        FROM drive_stats\\n        WHERE failure = 1\\n        GROUP BY model) failures\\n    ON\\n        drive_days.model = failures.model\\n    ORDER BY 2 desc\\n    \"\"\"\\n).show(5)\\n164\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 167}), Document(page_content='I am taking the same statement as with the subquery case but using two CTE instead. They can\\nalso be considered makeshift  statements that get dropped at the end of the query, just CREATE\\nlike the  keyword in Python. with\\nListing 7.13 Finding the drive models with the highest rate of failure using\\ncommon table expressions\\nIn Python, the best comparison I’ve found was to wrap our statements in a function. Any\\nintermediate variable created in the scope of the function would not be kept once the function\\nreturns. My version of the query using PySpark is in .\\nListing 7.14 Finding the drive models with the highest rate of failure using Python\\nscope rules to approximate CTE.spark.sql(\\n    \"\"\"\\n    WITH drive_days as (\\n        SELECT\\n            model,\\n            count(*) AS drive_days\\n        FROM drive_stats\\n        GROUP BY model),\\n    failures as (\\n        SELECT\\n            model,\\n            count(*) AS failures\\n        FROM drive_stats\\n        WHERE failure = 1\\n        GROUP BY model)\\n    SELECT\\n        model,\\n        failures / drive_days failure_rate\\n    FROM drive_days\\n    INNER JOIN failures\\n    ON\\n        drive_days.model = failures.model\\n    ORDER BY 2 desc\\n    \"\"\"\\n).show(5)\\ndef failure_rate(drive_stats):\\n    drive_days = drive_stats.groupby(F.col(\"model\")).agg(  \\n        F.count(F.col(\"*\")).alias(\"drive_days\")\\n    )\\n    failures = (\\n        drive_stats.where(F.col(\"failure\") == 1)\\n        .groupby(F.col(\"model\"))\\n        .agg(F.count(F.col(\"*\")).alias(\"failures\"))\\n    )\\n    answer = (  \\n        drive_days.join(failures, on=\"model\", how=\"inner\")\\n        .withColumn(\"failure_rate\", F.col(\"failures\") / F.col(\"drive_days\"))\\n        .orderBy(F.col(\"failure_rate\").desc())\\n    )\\n    return answer\\nfailure_rate(drive_stats).show(5)\\nprint(\"drive_days\" in dir())  \\n165\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 168}), Document(page_content='We are creating intermediate data frames within the body of the function to avoid\\nhaving a monster query.\\nOur  data frame uses both intermediate data framesanswer\\nTesting if we have a variable  in scope once the function returned drive_days\\nconfirms that our intermediates frames are neatly confined inside the function\\nscope.\\nThis section took a (very) simple and high level EDA and demonstrated how we can do it using\\nPySpark or Spark SQL. PySpark gives the floor to SQL without too much ceremony. This can be\\nvery convenient if you happen to hang out with DBAs and SQL developpers, as you can\\ncollaborate using their preferred language, while knowing that Python is right around the corner.\\nEverybody wins!\\nPySpark borrowed a lot of vocabulary from the SQL world. I think this was a very smart idea:\\nthere are generations of programmers that know SQL, and adopting the same keywords\\nPython-style makes it easy to communicate. Where we see a lot of difference is in the order of\\nthe operations: PySpark will naturally encourage you to think about the order of which the\\noperations should be performed. SQL follows a more rigit framework where you need to\\nremember if your operation belong in the operation, the target or the condition clause.\\nI find PySpark’s way of treating data manipulation more intuitive, but will rely on my years of\\nSQL experience as a data analyst when convenient. When writing SQL, I usually write my query\\nout order, starting with the target and building as I go. Not everything needs to be top to bottom!\\nSo far, I’ve made a lot of effort to keep both languages in a vacuum. We’ll break the barrier now\\nand unleash the power of Python+SQL. This will simplify how we write certain transformations,\\nmake our code easier to write and a less busy.\\nPySpark is rather accommodating when taking method and function parameters: you can pass a\\ncolumn name instead of a column object ( ) when using , for instance. On top F.col() groupby()\\nof this, there are a few methods we can use to cram a little SQL syntax into our PySpark code.\\nYou’ll see that there isn’t many methods where you can use this, but it’s so useful and well done\\nthat you’ll end up using it all the time.\\nThis section will build on the code we’ve written so far. We’re going to write a function that, for\\na given capacity, will return the top 3 most reliable drives according to our failure rate. We’ll\\nleverage the code written so far and simplify it.7.5 Simplifying our code: blending SQL and Python together7.4.7 A quick summary of PySpark vs. SQL syntax\\n166\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 169}), Document(page_content='We start by simplifying the code to read the data. The data ingestion part of the program is\\ndisplayed in . Compared to our original data ingestion, I’ve made a few changes.\\nFirst, I’ve put all the directories in a list so I could read them using a list comprehension. This\\nremoves some repetitive code and will also work easily if I remove or add some files (if you\\nwere only using Q3 2019, you can remove the other entries in the list).\\nSecond, since we do not need the SMART measurements, I’ve taken the intersection of the\\ncolumns instead of trying to fill the missing columns with  values. In order to create a null\\ncommon intersection that would apply to any number of data sources, I’ve used  which reduce\\napplies the anonymous function on all the column sets, resulting in the common set of columns\\nbetween all the data frames. For those unfamiliar with  and other higher-order functions, reduce\\nAppendix D contains a deeper explanation of how they works. I have also added an assertion on\\nthe common set of columns, as I want to make sure it contains the columns I need for the\\nanalysis. Assertions are a good way to short circuit an analysis if certains conditions are not met.\\nIn this case, if I am missing one of the colums, I’d rather have my program fail early with an \\n rather than have a huge stack trace later. Assertions are covered in detail in AssertionError\\nChapter 13.\\nFinally, I have used a second  for unioning all the distinct data frames in a cohesive one. reduce\\nThe same principle is used as when I was creating the common set of variables. This makes the\\ncode a lot cleaner and it will work without any modifications should I want to add more sources\\nor remove some.7.5.1 Reading our data\\n167\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 170}), Document(page_content='1.  \\n2.  \\n3.  \\n4.  Listing 7.15 The data ingestion part of our program\\nNow that our data has been read and is in a steady state, we can process it so we have an easy\\ntime answering our question. I will use SQL-style expressions when appropriate to showcase\\nwhen it makes sense to fuse both languages. At the end of this section, we’ll have code that will:\\nSelect only the useful columns for our query\\nGet our drive capacity in gigabytes\\nCompute the  and  data frames drive_days failures\\nJoin the two data frames into a summarized one and compute the failure rate\\nThe code is available in .from functools import reduce\\nimport pyspark.sql.functions as F\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\nDATA_DIRECTORY = \"../../data/Ch07/\"\\nDATA_FILES = [\\n    \"drive_stats_2019_Q1\",\\n    \"data_Q2_2019\",\\n    \"data_Q3_2019\",\\n    \"data_Q4_2019\",\\n]\\ndata = [\\n    spark.read.csv(DATA_DIRECTORY + file, header=True, inferSchema=True)\\n    for file in DATA_FILES\\n]\\ncommon_columns = list(\\n    reduce(lambda x, y: x.intersection(y), [set(df.columns) for df in data])\\n)\\nassert set([\"model\", \"capacity_bytes\", \"date\", \"failure\"]).issubset(\\n    set(common_columns)\\n)\\nfull_data = reduce(\\n    lambda x, y: x.select(common_columns).union(y.select(common_columns)), data\\n)\\n7.5.2 Using SQL-style expressions in PySpark\\n168\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 171}), Document(page_content='Listing 7.16 Processing our data so it’s ready for the query function\\nSQL-style expression, one that we already have been using. One of the new ones is \\n which is just like the  method with the exception that it will process selectExpr()` select()\\nSQL-style operations. I am quite a fan of this since it removes a bit of syntax when manipulating\\ncolumns with functions and arithmetic. In our case, the PySpark alternative (displayed in ) is a\\nlittle more verbose and cumbersome to write and read, especially since we have to create a literal\\n column to apply the  function.1024 pow()\\nListing 7.17 Replacing  by a regular  in our final program selectExpr() select()\\nThe second one is simply called . It wraps a SQL-style expression into a column. This is expr()\\nkind of a generalized , where you can use it in lieu of of  (or the column selectExpr() F.col()\\nname) when you want to modify a column. If we take our  table from , we can use an failures\\n (or ) as the  argument. This alternative syntax is shown in . I like doing itexpr expression agg()\\nin  parameters, because it saves a lot of .agg() alias()\\nListing 7.18 Using a SQL  data frame code. expr`ession in our `failures\\nThe third one, and my favourite, is the  method. Filtering conditions in SQL where()/filter()full_data = full_data.selectExpr(\\n    \"model\", \"capacity_bytes / pow(1024, 3) capacity_GB\", \"date\", \"failure\"\\n)\\ndrive_days = full_data.groupby(\"model\", \"capacity_GB\").agg(\\n    F.count(\"*\").alias(\"drive_days\")\\n)\\nfailures = (\\n    full_data.where(\"failure = 1\")\\n    .groupby(\"model\", \"capacity_GB\")\\n    .agg(F.count(\"*\").alias(\"failures\"))\\n)\\nsummarized_data = (\\n    drive_days.join(failures, on=[\"model\", \"capacity_GB\"], how=\"left\")\\n    .fillna(0.0, [\"failures\"])\\n    .selectExpr(\"model\", \"capacity_GB\", \"failures / drive_days failure_rate\")\\n    .cache()\\n)\\nfull_data = full_data.selectExpr(\\n    F.col(\"model\"),\\n    (F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"capacity_GB\"),\\n    F.col(\"date\"),\\n    F.col(\"failure\")\\n)\\nfailures = (\\n    full_data.where(\"failure = 1\")\\n    .groupby(\"model\", \"capacity_GB\")\\n    .agg(F.expr(\"count(*) failures\"))\\n)\\n169\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 172}), Document(page_content='is something I am very familiar with and being able to use them directly in PySpark with no\\nceremony is a godsend. In our final program, I was able to use full_data.where(\"failure =\\n instead of having to wrap the column name in  like we’ve been doing since the1\") F.col()\\nbeginning of this book.\\nI re-use this convenience in the query function, which is displayed in . This time, I’ve used string\\ninterpolation in conjunction with between. This doesn’t save many keystokes, but it’s very easy\\nto understand and you don’t get as much line noise as when using the \\n or (if you prefer using the data.capacity_GB.between(capacity_min, capacity_max)\\ncolumn function) . At this F.col(\"capacity_GB\").between(capacity_min, capacity_max)\\npoint, it’s very much a question of personal style and how familiar you are with each approach. I\\nrecommend you to try the SQL one if you don’t have a favourite yet.\\nListing 7.19 The  function, that computes the most_reliable_drive_for_capacity()\\nthe top N drives for a given capacity\\nI used an SQL-style expression in my  method, without having to use any where()\\nother special syntax or method\\nSince we want to return the top N results, not just show them, I use limit()\\ninstead of . show()\\nYou do not need to learn or use SQL to effectively work with PySpark. That being said, since the\\ndata manipulation API shares so much vocabulary and functionality with SQL makes it a plus to\\nat least have a basic understanding of the syntax and query structure.\\nMy family speaks both English and French, and sometimes you don’t always know where one\\nlanguage starts and one ends. I tend to think in both languages, and sometimes blends them in a\\nsingle sentence. Likewise, I find that some problems are easier to solve with Python and some7.6 Conclusiondef most_reliable_drive_for_capacity(data, capacity_GB=2048, precision=0.25, top_n=3):\\n    \"\"\"Returns the top 3 drives for a given approximate capacity.\\n    Given a capacity in GB and a precision as a decimal number, we keep the N\\n    drives where:\\n    - the capacity is between (capacity * 1/(1+precision)), capacity * (1+precision)\\n    - the failure rate is the lowest\\n    \"\"\"\\n    capacity_min = capacity_GB / (1 + precision)\\n    capacity_max = capacity_GB * (1 + precision)\\n    answer = (\\n        data.where(f\"capacity_GB between {capacity_min} and {capacity_max}\")  \\n        .orderBy(\"failure_rate\", \"capacity_GB\", ascending=[True, False])\\n        .limit(top_n)  \\n    )\\n    return answer\\n170\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 173}), Document(page_content='are more in SQL’s territory. You will find your own balance as well, which is why it’s nice to\\nhave the option. Just like spoken languages, the goal is to express your thoughts and intentions as\\nclearly as possible, keeping your audience in mind.\\nSpark provides an SQL API for data manipulation. This API supports ANSI SQL.\\nSpark (and PySpark’s by extension) borrows a lot of vocabulary and expected\\nfunctionality from the way SQL manipulates tables. This is especially evident since the\\ndata manipulation module is called . pyspark.sql\\nPySpark’s data frames need to be registered as views or tables before they can be queried\\nwith Spark SQL. You can give them a different name than the data frame you’re\\nregistering.\\nPySpark’s own data frame manipulation methods and functions borrow SQL\\nfunctionality for the most part. Some exceptions, such as , are present and union()\\ndocumented in the API.\\nSpark SQL queries can be inserted in a PySpark program through the spark.sql\\nfunction, where  is the running . spark SparkSession\\nSpark SQL tables references are kept in a  which contains the metadata for all Catalog\\ntables accessible to Spark SQL.\\nPySpark will accept SQL-style clauses in  ,  and , which where() expr() selectExpr()\\ncan simplify the syntax for complex filtering and selection.\\nWhen using Spark SQL queries with user-provided input, be careful about sanitizing the\\ninputs to avoid potential SQL injection attacks.\\nIf we look at the code in , we can simplify it even further, avoiding the creation of the two tables\\noutright. Can you write a summarized_data without having to use another table than full_data\\nand no join? (Bonus: try using pure PySpark, then pure Spark SQL, then a combo of both.)\\nOur analysis is a flawed in that the age of a drive is not taken into consideration. Instead of\\nordering the drives by failure rate, order them by average age at failure (assume that every drive\\nfails on 2020-01-01 if they are still alive at the end of the year).\\nWhat is the total capacity (in TB) that BackBlaze records at the end of each month?\\nNote: We will revisit this exercise in Chapter 10 when we look at window functions. In the\\nmeantime, this can be solved with a judicious usage of group bys and joins.7.7 Summary\\n7.8 Exercises\\n7.8.1 Exercise 7.1\\n7.8.2 Exercise 7.2\\n7.8.3 Exercise 7.3\\n7.8.4 Exercise 7.4\\n171\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 174}), Document(page_content='If you look at the data, you’ll see that some drive models can report an erroneous capacity. In the\\ndata preparation stage, use the most common capacity for each drive so we do not have more\\nthan one entry in our function.\\n172\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 175}), Document(page_content=\"8\\nThis chapter covers:\\nOur journey with PySpark so far has proven that it is a powerful and versatile data processing\\ntool. So far, we’ve explored many out-of-the-box functions and methods to manipulate data in a\\ndata frame. PySpark’s data frame manipulation functionality takes our Python code and applies\\nan optimized query plan, introduced in Chapter 1. This makes our data jobs efficient, consistent,\\nand predictable, just like coloring within the lines. What if we need to go off-script and\\nmanipulate our data according to our own rules?\\nIn this chapter, I cover how we can build Python functions and scale them in PySpark. I start by\\nintroducing the resilient distributed dataset (or RDD), a more primitive and lower-level structure\\ncompared to the data frame. I explain how you manipulate data in an RDD and how its element\\n(or row) major nature complements the data frame column-major approach.\\nI build on the knowledge of the RDD and explain how we can transfer this to the data frame\\nthrough user-defined functions (or UDF). Following this, I move to pandas UDF, a speedy and\\npowerful way to distribute Python functions on data frames. Finally, I close the loop by\\ndiscussing how to use Scala modules in PySpark, so your programs can build on the shoulders of\\nothers. With those additional tools at your disposal, no data manipulation task will leave you\\nstumped!Extending PySpark with Python: RDD and\\nuser-defined-functions\\nHow to use the RDD as a low level, flexible data container.\\nHow to promote regular Python functions to UDF to run in a distributed fashion.\\nHow to use scalar UDF as an alternative to Python UDF, using pandas' API.\\nHow to use grouped map and grouped aggregate UDF on  object to GroupedData\\nsplit data frame computation on manageable chunks.\\nHow to apply UDF on local data to ease debugging.\\n173\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion\", metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 176}), Document(page_content='This chapter uses pandas from 8.3 onwards, for pandas UDF. Extensive pandas knowledge is a\\nnice-to-have but is in no way expected. This chapter will cover the necessary pandas skills to use\\nin within a basic pandas UDF. If your wish to level up you pandas skills to become a pandas\\nUDF ninja, I warmly recommend the  book, by Boris Parkhaver (Manning, Pandas in Action\\n2021).\\nFor the examples in the chapter, we will need three previously unused libraries: pandas,\\nscikit-learn, and PyArrow. If you have installed Anaconda (see appendix B), you can use conda\\nto install the libraries; otherwise, you can use  . pip7\\nThis section covers the resilient distributed dataset (RDD) and how you use it to manipulate data.\\nMy goal is to provide a good overview of what an RDD is and how you manipulate data using\\nPython functions. Besides being useful in itself, this section’s material is a perfect introduction to\\nuser-defined functions, the data frame’s answer to the RDD operating model. I cover\\nuser-defined functions in the following sections.\\nThe RDD can be thought of as the ultimate flexible data container structure. You can cram pretty\\nmuch anything that can be pickled (python’s way of serializing objects) into it. Where the data\\nframe has good structure documentation though  objects, types, and schemas, the RDD Column\\ndoes not force you into a specific layout. Each element is independent of the other. In listing 8.1,\\nI create a list containing multiple objects of different types, then promote it to an RDD via the \\n method. The resulting RDD is depicted in figure 8.1. parallelize\\nListing 8.1 Promoting a Python list to a resilient distributed dataset\\nMy  is a list of an integer, a string, a float, a tuple, and a dict.collection8.1 PySpark, freestyle: the resilient distributed dataset\\n# Conda installation\\nconda install pandas sklearn pyarrow\\n# Pip installation\\npip install pandas sklearn pyarrow\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\ncollection = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}]  \\nsc = spark.sparkContext  \\ncollection_rdd = sc.parallelize(collection)  \\nprint(collection_rdd)\\n# ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195  \\n174\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 177}), Document(page_content='The RDD functions and methods are under the  object, accessible as SparkContext\\nan attribute of our . I alias it to  for convenience. SparkSession sc\\nThe list gets promoted to an RDD using the  method of the parallelize\\nSparkContext\\nOur  object is effectively an RDD. PySpark returns the type ofcollection_rdd\\nthe collection when we print the object.\\nFigure 8.1 The colletion_rdd RDD. Each object is independent from each other in the\\ncontainer. No column, no structure, no schema.\\nCompared to a data frame, the RDD is much more  (pardon the 90’s reference) in terms free style\\nof what it accepts. Manipulating data follows the same pattern: instead of using transformations\\non column objects, we go straight to the data, element by element. The next section introduces\\nthe RDD data manipulation API.\\nThis section explains the building blocks of data manipulation using an RDD. I discuss the\\nconcept of  and we use them to transform data. I finish with a quick higher-order function\\noverview of MapReduce, a fundamental concept in large-scale data processing, and place in the\\ncontext of Spark and the RDD.\\nManipulating data with an RDD feels to me like giving orders to an army as a general: you have\\nfull obedience from your privates/divisions into the field/RDD, but if you give an incomplete or\\nwrong order, you’ll cause havoc within your troop. Furthermore, each division has its own,\\n8.1.1 Manipulating data the RDD way: map, filter and reduce\\n175\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 178}), Document(page_content='specific type of order they can perform, and you don’t have a reminder of what’s what (unlike\\nwith a data frame schema). Sounds like a fun job…\\u200b\\nAn RDD provides many methods (which you can find in the API documentation for the \\n object), but we put our focus on three specific methods: ,  and pyspark.RDD map() filter()\\n. Together, they capture the ethos of data manipulation with an RDD; knowing how reduce()\\nthose three work will give you the necessary foundation to understand the others.\\nmap() ,  and  all take a function (that we will call ) as their only parameter.filter() reduce() f\\nWe call functions that take other functions as parameters . They can be a higher-order functions\\nlittle difficult to understand if it’s the first time you are encountering them; fear not, after seeing\\nthem in action, you’ll be very comfortable using them in PySpark (and in Python, if you have a\\nlook at appendix D).\\nmap()  is the easiest one to figure out: it applies the function taken as a parameter to every\\nelement of the RDD. I try, and fail, to map a simple function that adds 1 to a value in listing 8.2.\\nWhat’s happening?\\nListing 8.2 Mapping a simple function  to each element to an RDD. add_one()\\nA seeminly inoffensive function;  adds one to the value passed as an add_one()\\nargument.\\nI apply my function to every element in the RDD, via the  method. map()\\n materializes an RDD into a Python list on the master node. collect()\\nTo understand why our code is failing, we’ll break down the mapping process, illustrated in\\nfigure 8.2. I apply the  function to each element in the RDD by passing it as an add_one()\\nargument to the  method.  is a regular Python function, applied to regular map() add_one()\\nPython objects. Since we have incompatible types (taking an example,  is not a legal \"two\" + 1\\noperation in Python), three of our elements are . When I  the RDD to peek TypeError collect()\\nat the values, it explodes into a stack trace right in my REPL.\\nMAP\\ndef add_one(value):\\n    return value + 1  \\ncollection_rdd = collection_rdd.map(add_one)  \\nprint(collection_rdd.collect())  \\n# Stack trace galore! The important bit, you\\'ll get one of the following:\\n# TypeError: can only concatenate str (not \"int\") to str\\n# TypeError: unsupported operand type(s) for +: \\'dict\\' and \\'int\\'\\n# TypeError: can only concatenate tuple (not \"int\") to tuple\\n176\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 179}), Document(page_content=\"NOTE The RDD is a lazy collection. If you have an error in your function\\napplication, it will not be visible until you perform an action (such as \\n), just like with the data frame. collect()\\nFigure 8.2 Applying the add_one() function to each element of the RDD via map. If the\\nfunction cannot be applied, an error will be raised during action time.\\nFortunately, since we are working with Python, we can use a  block to prevent try/except\\nerrors. I provide an improved  function in listing 8.3, which returns the safer_add_one()\\noriginal element if the function runs into a type error.\\nListing 8.3 Mapping the function  to each element to an RDD. safer_add_one()\\nEach element that can’t be incremented will remain the same in the resulting\\nRDD.\\ncollection_rdd = sc.parallelize(collection)  \\ndef safer_add_one(value):\\n    try:\\n        return value + 1\\n    except TypeError:\\n        return value  \\ncollection_rdd = collection_rdd.map(safer_add_one)\\nprint(collection_rdd.collect())\\n# [2, 'two', 4.0, ('four', 4), {'five': 5}] \\n177\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion\", metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 180}), Document(page_content='I recreate my RDD from scratch to remove the erreneous operation in the thunk\\n(see chapter 1 for a description of a computation thunk).\\nOur function returns the original value untouched if it encounters a TypeError\\nThe relevant elements of the RDD have been incremented by one.\\nIn summary, you use  to apply a function to every element of the RDD. Because of the map()\\nflexibility of the RDD, PySpark does not give you any safeguards about the content of the RDD.\\nYou are responsible, as the developer, to make your function robust regardless of the input.\\nfilter()  is used to keep only the element that satisfies a predicate. In the data frame world, we\\nencountered the  methods that do just that. In the RDD world,  is where()/filter() filter()\\nmuch more flexible: it takes a function  and keeps only the elements that return a truthful value. f\\nIn listing 8.4, I filter my RDD to keep only the integer and float elements, using a lambda\\nfunction. The  function returns  if the first argument’s type is present in the isinstance() True\\nsecond argument; in our case, it’ll test if each element is either a  or an . float int\\nListing 8.4 Filtering our RDD with a lambda function. Our resulting RDD only has \\n and  values.int float\\nJust like , the function passed as a parameter to  is applied to every element in map() filter()\\nthe RDD. This time, though, instead of returning the result in a new data frame, we keep the\\noriginal value if the result of the function is truthy. If the result is falsey, we drop the element. I\\nshow the breakdown of the  operation in figure 8.3. filter()\\nSIDEBAR Truthy/Falsey in Python\\nPython has its own rules for boolean testing: because of this, I avoid using\\nabsolute True/False when talking about filtering in Python and PySpark. As\\na rule of thumb, ,  (the number zero in any Python numerical type), False 0\\nand empty sequences and collections (list, tuple, dict, set, range) are\\nfalsey. For more precisions on how Python imputes boolean values on\\nnon-booleans types, refer straight to the Python documentation: \\n. docs.python.org/3/library/stdtypes.html#truth-value-testing\\nFILTER\\ncollection_rdd = collection_rdd.filter(lambda elem: isinstance(elem, (float, int)))\\nprint(collection_rdd.collect())\\n# [2, 4.0]\\n178\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 181}), Document(page_content='Figure 8.3 Filtering our RDD to keep only int and float. Our predicate function is applied\\nelement-wise, and only the values leading to truthy predicates are kept.\\nThe last operation is  and as its name implies, it is being used to reduce elements in an reduce()\\nRDD. By reducing, I mean taking 2 elements and applying a function that will return only one\\nelement. PySpark will apply the function to the first two elements, then apply it again to the\\nresult and the third element, and so on until there are no elements left. I find the concept easier\\nwhen explained visually, so figure 8.4 shows the process of summing the value of the elements\\nin the RDD using reduce. In code form, listing 8.5 presents how to use the  method on reduce()\\na data frame.\\nREDUCE\\n179\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 182}), Document(page_content='Figure 8.4 Reducing our RDD by summing the values of the elements.\\nListing 8.5 Applying the  function via  to get the sum of the values add() reduce()\\nof the elements of our RDD.\\nThe  module contains the function version or common operators such as operator\\n ( ), so we do not have to pass a .+ add() lambda a, b: a + b\\nmap() , , and  appear at a first glance like simple concept: they take afilter() reduce()\\nfunction and apply it to all the elements inside the collection. The result is treated differently\\ndepending on the method chosen, and  requires a function of two arguments returning reduce()\\na single value. Well, in 2004, Google used this humble concept and caused a revolution in the\\nlarge-scale data processing world by publishing its MapReduce framework (\\n). You can’t argue on inspiration: the name is a combination of research.google/pubs/pub62/\\n and ! This framework was a direct inspiration to big data frameworks such as map() reduce()\\nHadoop and Spark. Although modern abstractions such as the data frame aren’t as close to the\\noriginal MapReduce, the ideas remain, and by understanding at a high level the building blocks,\\nit’ll be easier to understand some higher-level design choices.\\nfrom operator import add  \\ncollection_rdd = sc.parallelize([4, 7, 9, 1, 3])\\nprint(collection_rdd.reduce(add))  # 24\\n180\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 183}), Document(page_content='SIDEBARreduce() in a distributed world\\nBecause of PySpark’s distributed nature, the data of an RDD can be\\ndistributed across multiple partitions. The  function will be applied reduce()\\nindependently on each partition, and then each intermediate value will be\\nsent to the master node for the final reduction. Because of this, you need to\\nprovide a  and  function to . commutative associative reduce()\\nA  is a function in which you do not care about thecommutative function\\norder in which the arguments are applied. For example,  is add()\\ncommutative, since . Oh the flip side,  is not: a + b = b + a subtract() a -\\n. b != b - a\\nAn  function is a function in which you do not care aboutassociative\\nhow the values are grouped.  is associative, since add() (a + b) + c = a\\n.  is not: . + (b + c) subtract() (a - b) - c != a - (b = c)\\nadd(), ,  and  are both associative andmultiply() min() max()\\ncommutative.\\nThis concludes our whirlwind tour of the PySpark RDD API. We covered how the RDD applies\\ntransformations to its elements through higher-order functions such as , , and map() filter()\\n. Those higher-order functions apply the functions passed as parameters to each reduce()\\nelement, making the RDD \"element-major\" (or \"row-major\"). If you are curious about the other\\napplications of the RDD, I recommend looking at Appendix F as a companion to the PySpark\\nonline API documentation. Most of the methods on the RDD have a direct equivalent to the data\\nframe or grow directly from the usage of , , and . We revisit the RDD map() filter() reduce()\\nbriefly in chapter 9 when talking about performance. The next sections build on the concept of\\napplying Python functions directly, but this time on a data frame. The fun only begins!\\nEXERCISE 8.1\\nThe PySpark RDD API provides a  method that returns the number count()\\nof elements in the RDD as an integer. Reproduce the behavior of this\\nmethod using ,  and/or . map() filter() reduce()\\n181\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 184}), Document(page_content='1.  \\n2.  \\n3.  \\n4.  \\n5.  EXERCISE 8.2\\nWhat is the return value of the following code block?\\n[1]\\n[0, 1]\\n[0, 1, 0.0]\\n[]\\n[1, []]\\nSIDEBAR Optional topic: going full circle, a data frame is an RDD!\\nTo show the ultimate flexibility of the RDD, have a look at this: you can\\naccess an implicit RDD within a data frame via the  attribute of a data rdd\\nframe.\\nListing 8.6 Uncovering the RDD from within a data frame using the\\n attributerdd\\nFrom a PySpark perspective, a data frame is also an  (from RDD[Row]\\n), where each row can be thought of a dictionary: the key pyspark.sql.Row\\nis the column name and the value is the value contained in the record. To\\ndo the opposite trip, you can pass the RDD to  with spark.createDataFrame\\nan optional schema. Remember that, when moving from a data frame to an\\nRDD, you give up the schema safety of the data frame!\\nIt can be tempting to move back and forth between a data frame and an\\nRDD depending on the operation you wish to perform. Bear in mind that\\nthis will come at a performance cost (which we’ll explore further in chapter\\n9), but also makes your code harder to follow. You will also have to make\\nsure all your  follow the same schema before promoting your RDD into Row\\na data frame. The next sections cover how you can harness most of the\\npower of the RDD without leaving the comfort of the data frame.\\na_rdd = sc.parallelize([0, 1, None, [], 0.0])\\na_rdd.filter(lambda x: x).collect()\\ndf = spark.createDataFrame([[1], [2], [3]], schema=[\"column\"])\\nprint(df.rdd)\\n# MapPartitionsRDD[22] at javaToPython at NativeMethodAccessorImpl.java:0\\nprint(df.rdd.collect())\\n# [Row(column=1), Row(column=2), Row(column=3)]\\n182\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 185}), Document(page_content='In the previous section, we got a taste of flexibility with the RDD approach to data manipulation.\\nThis section takes the same question — how can we run Python code on our data? — and applies\\nit to the data frame. More specifically, we focus on the  transformation: for each record map()\\nthat comes in, one record comes out. Map-type transformations are by far the most frequent and\\nthe easiest to implement.\\nUnlike the RDD, the data frame has structure enforced by columns. To address this constraint,\\nPySpark provides the possibility to create  via the user-defined functions\\n function. What comes in is a regular Python function, and out pyspark.sql.functions.udf()\\nis a function promoted to work on PySpark columns.\\nTo illustrate this, we will mock up a data type not present in PySpark: the . Fractions Fraction\\nare made of a numerator and a denominator. In PySpark, we’ll represent this as an array of two\\nintegers. In listing 8.7, I create a data frame containing two columns, standing for the numerator\\nand the denominator. I fuse the two columns in an array column via the  function. array()\\nListing 8.7 Creating a data frame containing a single array column, where the first\\nelement is the numerator and the second the denominator.\\nI start the range for the denominator at 1, since a fraction with 0 for the\\ndenominator is undefined\\nThe  function takes two or more columns of the same type and creates aarray()\\nsingle column containing an array of the columns passed as parameter.\\nTo support our new makeshift fraction type, we create a few functions that provide basic\\nfunctionality. This is a perfect job for Python UDF, and I take the opportunity to introduce the8.2 Using Python to extend PySpark via user-defined functions\\nimport pyspark.sql.functions as F\\nimport pyspark.sql.types as T\\nfractions = [[x, y] for x in range(100) for y in range(1, 100)]  \\nfrac_df = spark.createDataFrame(fractions, [\"numerator\", \"denominator\"])\\nfrac_df = frac_df.select(\\n    F.array(F.col(\"numerator\"), F.col(\"denominator\")).alias(\"fraction\"),  \\n)\\nfrac_df.show(5, False)\\n# +--------+\\n# |fraction|\\n# +--------+\\n# |[0, 1]  |\\n# |[0, 2]  |\\n# |[0, 3]  |\\n# |[0, 4]  |\\n# |[0, 5]  |\\n# +--------+\\n# only showing top 5 rows\\n183\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 186}), Document(page_content='1.  \\n2.  \\n3.  two ways PySpark enables its creation.\\nThis section covers creating a Python function that will work seamlessly with a PySpark data\\nframe. While Python and Spark are like PB & J, creating and using UDF requires a few\\nprecautions. I introduce how you can use Python type hints to make sure your code will work\\nseamlessly with PySpark types. At the end of this section, we will have a function to reduce a\\nfraction and one to transform a fraction into a floating-point number.\\nMy blueprint when creating a function destined to become a Python UDF is as follow:\\nCreate and document the function\\nMake sure the input and output types are compatible\\nTest the function\\nTesting PySpark code (including UDF) is covered in chapter 14. For this section, I provide a\\ncouple of assertions to make sure the function is behaving like expected.\\nBehind every UDF is a Python function, so our two functions are in listing 8.8. I introduce\\nPython type annotations in this code block: the rest of the section covers how they are used in\\nthis context and why they are a powerful tool when combined with Python UDF.\\nListing 8.8 Creating our three python functions, complete with type annotation\\nand assertions\\n8.2.1 It all starts with plain Python: using typed Python functions\\nfrom fractions import Fraction  \\nfrom typing import Tuple, Optional  \\nFrac = Tuple[int, int]  \\ndef py_reduce_fraction(frac: Frac) -> Optional[Frac]:  \\n    \"\"\"Reduce a fraction represented as a 2-tuple of integers.\"\"\"\\n    num, denom = frac\\n    if denom:\\n        answer = Fraction(num, denom)\\n        return answer.numerator, answer.denominator\\n    return None\\nassert py_reduce_fraction((3, 6)) == (1, 2)  \\nassert py_reduce_fraction((1, 0)) is None\\ndef py_fraction_to_float(frac: Frac) -> Optional[float]:\\n    \"\"\"Transforms a fraction represented as a 2-tuple of integers into a float.\"\"\"\\n    num, denom = frac\\n    if denom:\\n        return num / denom\\n    return None\\nassert py_fraction_to_float((2, 8)) == 0.25\\nassert py_fraction_to_float((10, 0)) is None\\n184\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 187}), Document(page_content='We rely on the  data type from the  module to avoid Fraction fractions\\nreinventing the wheel\\nSome specific types need to be imported to be used: the standard library contains\\nthe types for scalar values, but containers like  and  need to be Option Tuple\\nexplicitly imported.\\nWe create a type synonym . This is equivalent to telling Python/mypy \" Frac When\\n    \" (a tuple containing two integers). you see Frac , assume it’s a Tuple[int, int]\\nThis makes the type annotations easier to read.\\nOur function takes a  as argument and returns a , which Frac Optional[Frac]\\ntranslates to \"     \". either a Frac orNone\\nI create a few assertions to sanity check my code and make sure I get the expected\\nbehavior.\\nBoth functions are very similar, so I’ll take  and go through it line by line. py_reduce_fraction\\nMy function definition has a few new elements. The  parameter has a  and we have a frac : Frac\\n before the colon. Those additions are  and are an amazing  Optional[Frac] type annotations\\ntool in making sure the function accepts and returns what we expect. Python is a dynamic\\nlanguage: this means that the type of an object is known at runtime. When working with\\nPySpark’s data frame, where each column has one and only one type, we need to make sure that\\nour UDF will return consistent types. We can use type hints to ensure this.\\nPython’s type checking is enabled by using a library called mypy. You install it via pip\\n. Once installed, you can run mypy on your file with . install mypy mypy MY_FILE.py\\nAppendix D contains a deeper introduction to the  module and mypy and how it applies typing\\n(and why it should apply) beyond UDF. I’ll add type annotation when relevant, as they can be\\nuseful documentation besides making our code more robust. ( What does my function expect?\\n) What does it return?\\nIn my function definition, I announce that the  function parameter is of type , which is frac Frac\\nequivalent to a , or a 2-element tuple containing two integers. If I get to share Tuple[int, int]\\nmy code with others, this  sends a signal about the input type of my function. type annotation\\nFurthermore, mypy will complain if I try to pass an incompatible argument to my function: if I\\ntry to do , mypy will tell me the following. py_reduce_fraction(\"one half\")\\nI can already see the type errors vanishing…\\u200b\\nThe second type annotation, located after the function arguments and prefixed with an arrow, is\\nthe type annotation for the return type of the function. We recognize the , but this time, I Frac\\nerror: Argument 1 to \"py_reduce_fraction\" has incompatible type \"str\"; expected \"Tuple[int, int]\"\\n185\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 188}), Document(page_content='1.  \\n2.  wrapped it into an  type. Optional\\nIn 8.1, when creating functions to be distributed over the RDD, I needed to make sure that they\\nwould not trigger an error, returning  instead. I apply the same concept here. I test for None denom\\nbeing a truthy value: if it is equal to 0, I return . This is such a frequent use-case that Python None\\nprovides the  type, which means \"either the type between the brackets or  \". Optional[…] None\\nPySpark will accept  values as  (see appendix D for the complete list of Python vs. None null\\nSpark types).\\nSIDEBAR Type annotations: stop cluttering my code!\\nType annotations are incredibly useful out of the box, but they are\\nespecially nifty when used with Python UDF. Since PySpark’s execution\\nmodel is lazy, you’ll often get your error stack trace at action time. UDF\\nstack traces are not any harder to read than any other stack trace in\\nPySpark — which is not saying much — but a vast majority of the bugs are\\nbecause of a bad input or return value. Now, type annotations are not a\\nsilver bullet, but they are a great tool to avoid and diagnose type errors.\\nWith all this said, Python’s type annotations are not available\\neverywhere. When working with pandas, numpy and PySpark’s data\\nstructures, you might encounter \" \". Python’s typing no stub file for [module]\\nstory is still early, and you might run into some rough edges. Fortunately,\\nmypy tries to minimize the annoyance by only checking the annotated\\nfunctions, and you can search for \"data science types\" on PyPI for an\\ninterim solution.\\nThe rest of the function is relatively straightforward: I ingest the numerator and denominator in a\\n object, which reduces the fraction. I then extract the numerator and denominator from Fraction\\nthe  and return them as a tuple of 2 integers, as I promised in my return typeFraction\\nannotation.\\nWe have our two functions, with well-defined input and output types. In the next section, I show\\nhow you promote regular Python functions to UDF and apply them to your data frame.\\nPySpark provides two equivalent ways to promote a function to a UDF. In this section, I explain\\nhow to use both approaches. I also discuss how to provide return Python type hints when\\ncreating a UDF.\\nPySpark provides a  function in the  module to promote Python udf() pyspark.sql.functions\\nfunction to their UDF equivalent. The function takes two parameters.\\nThe function you want to promote.\\n8.2.2 From Python function to UDF: two approaches\\n186\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 189}), Document(page_content='2.  Optionally, the return type of the generated UDF. In table 8.1, I summarize the type\\nequivalences between Python and PySpark. If you provide a return type, it  be must\\ncompatible with the return value of your UDF.\\nIn listing 8.9, I promote the  function to a UDF via the  function. py_reduce_fraction() udf()\\nJust like I did with the Python equivalent, I provide a return type to the UDF (this time, an Array\\nof , since  is the companion type of the tuple and  the one for Python integers).Long Array Long\\nOnce the UDF is created, we can apply it like any other PySpark function on columns. I chose to\\ncreate a new column to showcase the before and after: in the sample shown, the fraction appears\\nproperly reduced.\\nTable 8.1 A summary of the types in PySpark. A star next to the \"Python m\\nequivalent\" column means the Python type is more precise or can contain larger\\nvalues, so you need to be careful with the values you return.\\nType Constructor String representation Python equivalent\\nNullType() null None\\nStringType() string Python’s regular strings\\nBinaryType() N/A bytearray\\nBooleanType() boolean bool\\nDateType() date datetime.date  (from the datetime\\nlibrary)\\nTimestampType() timestamp datetime.datetime  (from the \\n library) datetime\\nDecimalType(p,s) decimal decimal.Decimal  (from the decimal\\nlibrary)*\\nDoubleType() double float\\nFloatType() float float*\\nByteType() byte  or tinyint int*\\nIntegerType() int int*\\nLongType() long  or bigint int*\\nShortType() short  or smallint int*\\nArrayType(T) N/A list, tuple or Numpy array (from the \\n library)numpy\\nMapType(K, V) N/A dict\\nStructType([…]) N/A list or tuple\\n187\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 190}), Document(page_content='Listing 8.9 Creating a UDF explicitly with the  function, and applying it to our udf()\\ndata frame.\\nI alias the \"array of long\" PySpark type to the  variable. SparkFrac\\nI promote my Python function using the  function, passing my udf() SparkFrac\\ntype alias as the return type.\\nA UDF can be used like any other PySpark column function\\nYou also have the option to create your Python function and promote it as a UDF using the udf\\nfunction as a decorator. In listing 8.10, I define by  (now called py_fraction_to_float()\\nsimply ) directly as a UDF by preceding my function definition by fraction_to_float()\\n. In both cases, you can access the underlying function from the UDF @F.udf([return_type])\\nby calling the attribute . frac\\nSparkFrac = T.ArrayType(T.LongType())  \\nreduce_fraction = F.udf(py_reduce_fraction, SparkFrac)  \\nfrac_df = frac_df.withColumn(\\n    \"reduced_fraction\", reduce_fraction(F.col(\"fraction\"))  \\n)\\nfrac_df.show(5, False)\\n# +--------+----------------+\\n# |fraction|reduced_fraction|\\n# +--------+----------------+\\n# |[0, 1]  |[0, 1]          |\\n# |[0, 2]  |[0, 1]          |\\n# |[0, 3]  |[0, 1]          |\\n# |[0, 4]  |[0, 1]          |\\n# |[0, 5]  |[0, 1]          |\\n# +--------+----------------+\\n# only showing top 5 rows\\n188\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 191}), Document(page_content='Listing 8.10 Creating a UDF directly using the  decorator. udf()\\nThe decorator performs the same function as the  function, but return a UDF udf()\\nbearing the name of the function defined under.\\nIn order to perform my assertion, I use the  attribute of the UDF, which func\\nreturns the function ready to be called.\\nEXERCISE 8.3\\nCreate a UDF that adds two fractions together, and test it by adding the \\n to itself in the  data frame. reduced_fraction frac_df\\nEXERCISE 8.4\\nOur  will not work if the numerator or denominatorpy_reduce_fraction\\nexceeds  or is lower than . Modify the pow(2, 63)-1 -pow(2, 63)\\n to return  if this is the case. py_reduce_fraction None\\nBonus: Does this change the type annotation provided? Why?\\n@F.udf(T.DoubleType())  \\ndef fraction_to_float(frac: Frac) -> Optional[float]:\\n    \"\"\"Transforms a fraction represented as a 2-tuple of integers into a float.\"\"\"\\n    num, denom = frac\\n    if denom:\\n        return num / denom\\n    return None\\nfrac_df = frac_df.withColumn(\\n    \"fraction_float\", fraction_to_float(F.col(\"reduced_fraction\"))\\n)\\nfrac_df.select(\"reduced_fraction\", \"fraction_float\").distinct().show(5, False)\\n# +----------------+-------------------+\\n# |reduced_fraction|fraction_float     |\\n# +----------------+-------------------+\\n# |[3, 50]         |0.06               |\\n# |[3, 67]         |0.04477611940298507|\\n# |[7, 76]         |0.09210526315789473|\\n# |[9, 23]         |0.391304347826087  |\\n# |[9, 25]         |0.36               |\\n# +----------------+-------------------+\\n# only showing top 5 rows\\nassert fraction_to_float.func((1, 2)) == 0.5  \\n189\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 192}), Document(page_content='NOTE Spark 3.0 brings a ton of improvements and new functionality to pandas\\nUDF, including scalar iterator, map iterator, and cogrouped map pandas\\nUDF. The chapter is written with Spark 2.4.5, the latest available version, in\\nmind, but I plan on including a new section containing the new Spark 3.0\\nUDF (and refresh the current material if necessary) once I get them under\\nthe microscope.\\nPython UDF, while very flexible, only operates on a single record at a time, just like the map()\\nmethod of the RDD. This section introduces a fresh way to approach UDF: pandas (or vectorized\\n) UDF. Just like their name indicates, they rely on pandas, a very popular data manipulation\\nlibrary in Python.\\nWARNING Vectorized UDF were introduced in Spark 2.3 (scalar, grouped map) and\\nimproved upon in Spark 2.4 (grouped aggregate). I recommend using the\\nmost up-to-date stable version everywhere in the book, but this section\\nrequires it.\\nAt the core, pandas UDF can be seen as nothing more than distributing pandas data manipulation\\ncode within a data frame. In chapter 1, I explained that Spark distributes a large amount of data\\nin multiple partitions and orchestrates transformations and actions through a master-workers\\nsplit. In our model, a pandas UDF is just like treating each chunk of data like an independent\\npandas data frame.\\nFor this section, I use the National Oceanic and Atmospheric Administration (NOAA) Global\\nSurface Summary of the Day (GSOD) dataset. This data is available from multiple sources, but\\none of the easiest to access is Google pubic data set repository, made available through\\nBigQuery. I use the BigQuery connector to Spark to ingest the data (\\n). The instructions on their Github github.com/GoogleCloudDataproc/spark-bigquery-connector\\nmight change over time, so refer to their README as necessary. For Spark 2.4.5, you will need\\nto download the . If you are using Spark 3.0, you will need the spark_bigquery_latest.jar\\njar for your Scala version (2.11 or 2.12, depending on your Spark installation).\\nTo access the data, you also need a Google Cloud Platform (GCP) account. Once your account is\\ncreated, you need to create a service account and a service account key to tell BigQuery to give\\nyou access to the public data programmatically. To do so, select \"Service Account\" (under \"IAM\\n& Admin\") and click \"+ Create Service Account\". Give a funny name to your service account\\nname. In the service account permissions menu, select \"BigQuery → BigQuery admin\" and click8.3 Big data is just a lot of small data: using pandas UDF\\n8.3.1 Setting our environment: connectors and libraries\\n190\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 193}), Document(page_content='\"continue\". In the last step, click \"+ CREATE KEY\" and select JSON. Download the key and\\nstore it somewhere safe.\\nWARNING Treat this key like any other password. If a malicious person steals your\\nkey, go back to the \"Service Accounts\" menu and delete this key,\\nrecreating a new one.\\nThe last step before analyzing our data using vectorized UDF is to install PyArrow. PyArrow is\\nthe python bindings to the Apache Arrow project ( ), an in-memory data arrow.apache.org/\\nserialization library. It provides a bridge between the PySpark data frame and the pandas data\\nframe. If you are using Spark 2.3 or 2.4, you also need to set a flag in the conf/spark-env.sh\\nfile of your Spark root directory. In the  directory, you should find a conf/\\n file. Make a copy, name it  and add this line in the file. spark-env.sh.template spark-env.sh\\nThis will tell PyArrow to use a serialization format compatible with Spark 2.X, instead of the\\nnewer one only compatible with Spark 3.0. The Spark JIRA ticket contains more information\\nabout this ( ). You can also use PyArrow version issues.apache.org/jira/browse/SPARK-29367\\n0.14 and avoid the problem altogether.\\nTIP If you are using PySpark in the cloud, refer to your provider documentation.\\nEach cloud provider has a different way of managing Spark dependencies\\nand libraries. For a quick review of the most popular Spark cloud offerings,\\nsee Appendix C.\\nFinally, we can (re-)start our PySpark shell, with the new library installed. The  and pyspark\\n commands take an optional  parameter that loads external dependencies spark-submit --jars\\non your Spark installation.\\nListing 8.11 Starting a PySpark shell with the BigQuery connector installed\\nAlternatively, if you use PySpark through your Python/IPython shell, you can load the library\\ndirectly from Maven (Java/Scala’s equivalent of PyPI) when creating your . SparkSession\\nARROW_PRE_0_15_IPC_FORMAT=1\\npyspark --jars spark-bigquery-latest.jar\\n191\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 194}), Document(page_content='1.  \\n2.  Listing 8.12 Initializing PySpark withing your python shell with the BigQuery\\nconnector enabled.\\nI took the package version recommended for the most recent Spark/Scala version\\n(2.4.5/2.11)\\nWARNING If you have a SparkSession already in progress, it is  to just not enough\\n and try to restart. You need to stop the JVM process spark.stop()\\naltogether. Trying to do this without restarting your PySpark/Python REPL\\nis an exercise in frustration, so just kill and start fresh. If you use the\\nmethod in listing 8.12 and you don’t see similar jar verbiage, it will not work.\\nBefore we can start working on our pandas UDF, we need to extract the data from BigQuery and\\nassemble the multiple tables in a cohesive data frame. Reading data from BigQuery is\\nstraightforward. I use the  specialized  — provided by the connector bigquery SparkReader\\nlibrary we embedded to our PySpark shell — providing two options:\\nThe  parameter, pointing to the table we want to ingest. The format is table\\n: the  is a project available to all. project.dataset.table bigquery-public-data\\nThe  is the JSON key downloaded in 8.3.1. You need to adjust thecredentialsFile\\npath and file name accordingly to the location of the file.\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.config(\\n    \"spark.jars.packages\",\\n    \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta\",  \\n).getOrCreate()\\n# Ivy Default Cache set to: /Users/jonathan_rioux/.ivy2/cache\\n# The jars for the packages stored in: /Users/jonathan_rioux/.ivy2/jars\\n# :: loading settings :: url = jar:file:/usr/local/Cellar/apache-spark/2.4.5/libexec/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\\n# com.google.cloud.spark#spark-bigquery-with-dependencies_2.11 added as a dependency\\n# :: resolving dependencies :: org.apache.spark#spark-submit-parent-035f1392-cda4-4935-a62b-969bda5449d5;1.0\\n#     confs: [default]\\n#     found com.google.cloud.spark#spark-bigquery-with-dependencies_2.11;0.15.1-beta in central\\n# :: resolution report :: resolve 134ms :: artifacts dl 2ms\\n#     :: modules in use:\\n#     com.google.cloud.spark#spark-bigquery-with-dependencies_2.11;0.15.1-beta from central in [default]\\n#     ---------------------------------------------------------------------\\n#     |                  |            modules            ||   artifacts   |\\n#     |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\\n#     ---------------------------------------------------------------------\\n#     |      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\\n#     ---------------------------------------------------------------------\\n# :: retrieving :: org.apache.spark#spark-submit-parent-035f1392-cda4-4935-a62b-969bda5449d5\\n#     confs: [default]\\n#     0 artifacts copied, 1 already retrieved (0kB/4ms)\\n# [...]\\n8.3.2 Preparing our data\\n192\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 195}), Document(page_content='TIP If you are using Google DataProc, you do not have to provide a \\n since the permissions will be granted through your GCP credentialsFile\\naccount. The documentation for the BigQuery connector will provide the\\nmost up-to-date instructions. Appendix C covers Spark in the cloud,\\nincluding Google DataProc.\\nThe code is available in listing 8.13. For my  table, I need to union the tables together in a gsod\\nsingle cohesive data frame. While I can chain multiple  like in chapter 7, I went a more union()\\nelegant route using the  operator, this time by applying it to my list comprehension. reduce\\nListing 8.13 Reading the  and  tables for 2010 to 2020. stations gsod\\nSince all the tables are read the same way, I abstract my reading routine in a\\nre-usable function, returning the resulting data frame.\\nI use the  specialized reader via the  method. bigquery format()\\nThe stations table is available in BigQuery under \\n, where  is the four-digit bigquery-public-data.noaa_gsod.gsodXXXX XXXX\\nyear.\\nI pass my JSON service account key to the  option, to tell credentialsFile\\nGoogle I am allowed to use the BigQuery service.\\n can be passed as a parameter to , where it’ll union all DataFrame.union reduce\\nthe tables in my list pair-wise into a single table.\\nIt’s easier to understand the reduce operation if we break it down into discrete steps.\\nI start with a  (in my example 2010 to 2020, including 2010 but excluding 2020). range of years\\nfrom functools import reduce\\nfrom pyspark.sql import DataFrame\\ndef read_df_from_bq(year):  \\n    return (\\n        spark.read.format(\"bigquery\")  \\n        .option(\"table\", f\"bigquery-public-data.noaa_gsod.gsod{year}\")  \\n        .option(\"credentialsFile\", \"bq-key.json\")  \\n        .load()\\n    )\\ngsod = (\\n    reduce(\\n        DataFrame.union, [read_df_from_bq(year) for year in range(2010, 2020)]  \\n    )\\n    .dropna(subset=[\"year\", \"mo\", \"da\", \"temp\"])\\n    .where(F.col(\"temp\") != 9999.9)\\n)\\n193\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 196}), Document(page_content='For this, I use the  function. range()\\nI apply my helper function  to each year via a list comprehension, yielding read_df_from_bq()\\na list of data frames. I don’t have to worry about memory consumption as the list contains only a\\nreference to the data frame ( ). DataFrame[…]\\nAs a reducing function, I use the  function. This method, when applied to a DataFrame.union\\ndata frame ( ), takes a single parameter, since there is an implicit  that maps to df.union() self\\nthe data frame calling the method. If we apply the function , using it from the generic statically\\n object, then it’ll take  and union the data frames in a DataFrame two data frames as parameter\\nsingle one.\\nWe could do this iteratively, using a  loop. In listing 8.14, I show how to accomplish the for\\nsame goal without using . Since higher-order functions usually yield cleaner code, I reduce()\\nprefer using them to looping constructs where it make sense.\\nListing 8.14 Reading the gsod data from 2010 to 2020 using an iterative/looping\\napproach. We need to load a first table to initialize the process.\\nWhen using a looping approach to union tables, you need an explicit starting seed.\\nI use the table from 2010.\\nThe reduce approach only works if all the tables union without any problem; same schema, from\\ncolumn name, order, and types. Google is doing us a solid here by having the data all pre-cleaned\\nfor us\\nTIP If you are using a local Spark, loading 2010-2019 will make the examples\\nin this chapter rather slow. I use 2018 only when working on my local\\ninstance so I don’t have to wait too long for code execution. Inversely, if\\nyou are working with a more powerful setup, you can add years to the\\nrange. The  tables go back to 1929. gsod\\nScalar UDF are the most common type of pandas UDF. As their name indicates, they work on\\nscalar values: for each record passed in, it needs to return one record. They behave just like\\nregular Python UDF, with one key difference. Python UDF work on one record at a time and you\\nexpress your logic through regular Python code. Scalar UDF work on one  at a time and series\\nyou express your logic through  code. The difference is subtle and it’s easier to explain pandas\\nvisually.\\ngsod_alt = read_df_from_bq(2010)  \\nfor year in range(2011, 2020):\\n    gsod_alt = gsod_alt.union(read_df_from_bq(year))\\n8.3.3 Scalar UDF\\n194\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 197}), Document(page_content='In a Python UDF, when you pass column objects to you UDF, PySpark will unpack each value,\\nperform the computation, and then return the value for each record in a  object. In a Column\\nScalar UDF, depicted in figure 8.5, PySpark will serialize (through PyArrow) each partitioned\\ncolumn into a pandas  object ( Series\\n). You then perform the pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\\noperations on the Series object directly, returning a Series of the same dimension from your\\nUDF. From an end-user perspective, they are the same functionally In Chapter 9, I discuss the\\nperformance implications of Python UDF vs. a scalar UDF.\\nFigure 8.5 Comparing a Python UDF to a pandas scalar UDF. The former splits a column\\nin individual records, where the latter breaks them in Series.\\nNOTE PySpark makes no guarantees about how the columns you pass to your\\nscalar UDF will be split in Series, so you need to make sure your UDF\\ndoesn’t depend on a specific breakdown. For more control over the breaks,\\nsee listing 8.17.\\nNow armed with the  of scalar UDF, let’s create one ourselves. I chose to create a how it works?\\nsimple function that will transform Fahrenheit degrees to Celcius. In Canada, we use both scales\\n195\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 198}), Document(page_content='1.  \\n2.  depending on the usage: F for cooking, C for body or outside temperature. I have no idea of if 95\\ndegrees F is hot or cold, but I know how to dress when it’s 10 degrees C, yet my dinner cooks at\\n350 F.\\nMy function is depicted in listing 8.15. The building blocks are eerily similar; there are two main\\ndifferences noticeable.\\nInstead of , I use , again, from the udf() pandas_udf() pyspark.sql.functions\\nmodule. The first parameter is the return type of the function ( ) and the DoubleType()\\nsecond is an indicator of the type of pandas UDF I am creating, here a \\n. PandasUDFType.SCALAR\\nMy code itself could be used as-is for a regular python UDF. I am (ab)using the fact that\\nyou can do arithmetic operations with pandas Series. You can use any Series method\\nshould you need to.\\nListing 8.15 Creating a pandas scalar UDF that transforms Fahrenheit into\\nCelcius. I use the  decorator with a UDF type of . pandas_udf PandasUDFType.SCALAR\\nFor scalar UDF, the biggest change happens in the decorator used. I could use the \\n function directly too. pandas_udf\\nimport pandas as pd\\n@F.pandas_udf(T.DoubleType(), F.PandasUDFType.SCALAR)  \\ndef f_to_c(degrees):\\n    \"\"\"Transforms Farhenheit to Celcius.\"\"\"\\n    return (degrees - 32) * 5 / 9\\nf_to_c.func(pd.Series(range(32, 213)))  \\n# 0        0.000000\\n# 1        0.555556\\n# 2        1.111111\\n# 3        1.666667\\n# 4        2.222222\\n#           ...\\n# 176     97.777778\\n# 177     98.333333\\n# 178     98.888889\\n# 179     99.444444\\n# 180    100.000000\\n# Length: 181, dtype: float64\\ngsod = gsod.withColumn(\"temp_c\", f_to_c(F.col(\"temp\")))\\ngsod.select(\"temp\", \"temp_c\").distinct().show(5)\\n# +-----+-------------------+\\n# | temp|             temp_c|\\n# +-----+-------------------+\\n# | 37.2| 2.8888888888888906|\\n# | 85.9| 29.944444444444443|\\n# | 53.5| 11.944444444444445|\\n# | 71.6| 21.999999999999996|\\n# |-27.6|-33.111111111111114|\\n# +-----+-------------------+\\n# only showing top 5 rows\\n196\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 199}), Document(page_content='1.  \\n2.  \\n3.  To test my function, I apply it to every Fahrenheit value from 32 to 212 inclusively\\n(0 to 100 Celcius), using the  attribute that returns the local version of the func\\nUDF.\\nScalar UDF, just like Python regular UDF, are very convenient when the record-wise\\ntransformation (or \"mapping\") you want to apply to your data frame is not available within the\\nstock PySpark functions ( ). Creating a \"Fahrenheit to Celcius\" pyspark.sql.functions\\nconverter as part of core Spark would be a little intense, so using PySpark or (pandas) scalar\\nUDF is a way to extend the core functionality with a minimum of fuss. Next, we see how to gain\\nmore control over the split and use the split-apply-combine pattern in PySpark.\\nSIDEBAR Vocabulary matters: partitions vs. chunks\\nIt can be tempting to use the word  when talking about how partitions\\nPySpark splits the data for a pandas UDF. Spark already has a concept of\\npartitions, though: they refer to the physical data contained on the worker\\nnodes. When working with pandas UDF, Spark can use the partitions as\\nchunks, but can also decide to split them or move some data around.\\nBecause of this, I use  or  (for grouped pandas UDF) instead. chunks groups\\nLess confusion, more data fun.\\nGrouped map UDF are PySpark’s answer to the split-apply-combine pattern. At the core,\\nsplit-apply-combine is just a series of three steps that are frequently used in data analysis.\\nFirst, you  your data set into logical chunks. split\\nYou then  a function to each chunk independently. apply\\nFinally, you  the chunks into a unified data set. combine\\nTo be perfectly honest, I did not know this pattern’s name until somebody pointed at my code\\none day and said \"this is some nice split-apply-combine work you did there\". You probably use it\\nintuitively as well. In PySpark’s world, I see it more as a  move, as illustrated divide and process\\nin figure 8.6.\\n8.3.4 Grouped map UDF\\n197\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 200}), Document(page_content='Figure 8.6 Split-apply-combine depicted visually. We chunk/group the data frame, process\\neach one with pandas, before unioning them into a (Spark) data frame again.\\nBefore looking at the PySpark plumbing, we focus on the pandas side of the equation. Where\\nscalar UDF were relying on pandas Series, grouped map UDF are using pandas DataFrame. Each\\nlogical chunk from step 1 in figure 8.6 becomes a DataFrame ready for action. Our UDF must\\nalso return a DataFrame.\\nGrouped map UDF also take  as a decorator, this time with a type of pandas_udf()\\n. The return type is also more verbose: since we have multiple PandasUDFtype.GROUPED_MAP\\ncolumns in the pandas DataFrame, we have to provide the schema in a . For a StructType()\\ndeeper dive into schemas, head to chapter 6.\\n198\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 201}), Document(page_content='Listing 8.16 A grouped map UDF that normalizes (min-max) the temperature given\\na set of values. The code in the  function is regular pandas scale_temperature()\\ncode.\\nTIP pandas_udf()  will take a SQL-like schema as a string parameter too. For\\nlisting 8.16, we could have used stn string, name string, country\\nstring, year string, mo string, da string, temp double, temp_norm\\n as the return type. double\\nCompared with the UDF seen so far in this chapter, the main difference is the return type of the\\nUDF. In both the Python and the scalar UDF, we returned a single column. Here, we return a \\n. In listing 8.16, our return DataFrame contains six columns. My complete (pandas) DataFrame\\nUDF only adds one new column, , which scales the temperature column received temp_norm\\nfrom a scale from zero to one. Since I have a division in my UDF, I am giving a reasonable value\\nof 0.5 if the minimum temperature in my chunk equals the maximum temperature. By default,\\npandas will give an infinite value for division by zero: PySpark will interpret this as null.\\nNow with the \"apply\" step done, the rest is a piece of cake. I broke the punch in chapter 5: we\\nuse  to split a data frame in manageable chunks and then pass our function to the groupby()\\n method. You can see the result in listing 8.17. apply()\\n@F.pandas_udf(\\n    T.StructType(\\n        [\\n            T.StructField(\"stn\", T.StringType()),\\n            T.StructField(\"year\", T.StringType()),\\n            T.StructField(\"mo\", T.StringType()),\\n            T.StructField(\"da\", T.StringType()),\\n            T.StructField(\"temp\", T.DoubleType()),\\n            T.StructField(\"temp_norm\", T.DoubleType()),\\n        ]\\n    ),\\n    F.PandasUDFType.GROUPED_MAP,\\n)\\ndef scale_temperature(temp_by_day):\\n    \"\"\"Returns a simple normalization of the temperature for a site.\\n    If the temperature is constant for the whole window, defaults to 0.5.\"\"\"\\n    temp = temp_by_day.temp\\n    answer = temp_by_day[[\"stn\", \"year\", \"mo\", \"da\", \"temp\"]]\\n    if temp.min() == temp.max():\\n        return answer.assign(temp_norm=0.5)\\n    return answer.assign(temp_norm=(temp - temp.min()) / (temp.max() - temp.min()))\\n199\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 202}), Document(page_content='Listing 8.17 Split-apply-combing in PySpark: we  records in a groupby()\\n object and  our UDF to each group.GroupedData apply()\\nIf you are working locally, keeping a single year worth of data will make sure you\\ndon’t wait too long to get your results.\\nI group by three fields, , , and . Unlike the  combo seen in chapter stn year mo groupby()/agg()\\n5, where the keys are implicitly present in the resulting data frame, the UDF applied needs to\\nreturn any column that we want in our resulting data frame. My UDF has six columns in its\\nreturn value, the data frame after  has the same six, following the same type equivalence apply()\\nseen in table 8.1. In practice, you create a grouped map UDF with a  pattern in mind, groupby()\\nso there is a low risk of a mismatch.\\nWARNING With great power comes great responsibility: when grouping by your data\\nframe, make sure each chunk is \"pandas-size\", i.e. it can be loaded\\ncomfortably in memory. If one or more chunks is too big, you’ll get an\\nout-of-memory exception.\\nGrouped map UDF shine when you have distinct groups of data that you can process\\nindependently. In the case of listing 8.17, we scale the temperature for each combination of \\n. The moment you feel like your code can process some distinct (station, year, month)\\ngroups in your data frame, a grouped map UDF is a great choice.\\nWe finish our tour of pandas user-defined functions with the grouped aggregate UDF. They can\\nbe thought of a combination of the ones we saw so far, as they take pandas Series as parameters\\nbut return a simple scalar value. In that sense, they are akin to Spark aggregate functions: each\\ngroup is summarized by a single value.\\nFor grouped aggregate UDF, we still rely on the \"split\" step provided by  — which groupby()\\nmakes them like grouped map UDF — but this time, we apply our UDF to the  method, agg()\\ngsod = gsod.where(F.col(\"year\") == \"2018\")  \\ngsod = gsod.groupby(\"stn\", \"year\", \"mo\").apply(scale_temperature)\\ngsod.show(5, False)\\n# +------+----+---+---+-------------------+-------------------+\\n# |stn   |year|mo |da |temp_c             |temp_norm          |\\n# +------+----+---+---+-------------------+-------------------+\\n# |010250|2018|12 |08 |-5.666666666666667 |0.06282722513088991|\\n# |010250|2018|12 |27 |-2.0555555555555554|0.40314136125654443|\\n# |010250|2018|12 |31 |-1.6111111111111103|0.4450261780104712 |\\n# |010250|2018|12 |19 |-2.4444444444444438|0.3664921465968586 |\\n# |010250|2018|12 |04 |2.5555555555555562 |0.8376963350785341 |\\n# +------+----+---+---+-------------------+-------------------+\\n# only showing top 5 rows\\n8.3.5 Grouped aggregate UDF\\n200\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 203}), Document(page_content='just aggregate functions. For my grouped aggregate UDF, I wanted to do something a little more\\ncomplex than reproducing the common offenders (count, min, max, average). In listing 8.18, I\\ncompute the linear slope of the (scaled) temperature for a given period, using scikit-learn’s \\n object. You do not need to know scikit-learn or machine learning to follow LinearRegression\\nalong: I’m using basic functionality and explain each step.\\nNOTE This is not a machine learning exercise: I am just using scikit-learn’s\\nplumbing to create a feature. Machine learning in Spark is covered in part 3\\nof this book. Don’t take this code as a robust model training exercise!\\nListing 8.18 Creating a grouped aggregate UDF that computes the slope of a set\\nof temperature\\nI import the linear regression object from sklearn.linear_model\\nI initialize the  object. LinearRegression\\nThe  method trains the model, using the  Series as a feature and the fit day temp\\nseries as the prediction.\\nSince I have only one feature, I select the first value of the  attribute as my coef_\\nslope.\\nTo train a model in scikit-learn, we start by initializing the model object. In this case, I use \\n without any other parameters. I then  the model, providing , my LinearRegression() fit X\\nfeature matrix, and , my prediction vector. In this case, since I have a single feature, I need to y\\n my  matrix or scikit-learn will complain about a shape mismatch. reshape X\\nAt the end of the fit method, our  object has trained a model and, in the case LinearRegression\\nof a linear regression, keeps its coefficient in a  vector. Since I really just care about the coef_\\ncoefficient, I just extract and return it.\\nIt’s easy to apply a grouped aggregate UDF to our data frame. In listing 8.19, I  the groupby()\\nstation code, name, and country, as well as the year and the month. I pass my newly created\\ngrouped aggregate function as a parameter to , passing my  objects as parameter to agg() Column\\nthe UDF.\\nfrom sklearn.linear_model import LinearRegression  \\n@F.pandas_udf(T.DoubleType(), F.PandasUDFType.GROUPED_AGG)\\ndef rate_of_change_temperature(day, temp):\\n    \"\"\"Returns the slope of the daily temperature for a given period of time.\"\"\"\\n    return (\\n        LinearRegression()  \\n        .fit(X=day.astype(\"int\").values.reshape(-1, 1), y=temp)  \\n        .coef_[0]  \\n    )\\n201\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 204}), Document(page_content='Listing 8.19 Applying our grouped aggregate UDF using . Our UDF behaves agg()\\njust like a Spark aggregate function.\\nApplying a grouped aggregate UDF is the same as using a Spark aggregating\\nfunction: you add it as an argument to the  method of the agg() GroupedData\\nobject.\\nI am looking at the stations having the lowest proportion of increasing temperature\\nfor a month, given that they have data for at least 6 distinct months. That can\\nsuggest data quality issues or skew in the data collection.\\nPandas UDF are quite useful to extend PySpark with transformations that are not included in the \\n module. I find that they’re also quite easy to understand but pretty hard to get pyspark.sql\\n. I finish this chapter with a few pointers on testing out and debugging your pandas UDF. right\\nThe most important aspect of an pandas UDF (and any UDF) is that it need to work on the\\nresult = gsod.groupby(\"stn\", \"year\", \"mo\").agg(\\n    rate_of_change_temperature(gsod[\"da\"], gsod[\"temp_norm\"]).alias(  \\n        \"rt_chg_temp\"\\n    )\\n)\\nresult.show(5, False)\\n# +------+----+---+---------------------+\\n# |stn   |year|mo |rt_chg_temp          |\\n# +------+----+---+---------------------+\\n# |010250|2018|12 |-0.01014397905759162 |\\n# |011120|2018|11 |-0.01704736746691528 |\\n# |011150|2018|10 |-0.013510329829648423|\\n# |011510|2018|03 |0.020159116598556657 |\\n# |011800|2018|06 |0.012645501680677372 |\\n# +------+----+---+---------------------+\\n# only showing top 5 rows\\nresult.groupby(\"stn\").agg(\\n    F.sum(F.when(F.col(\"rt_chg_temp\") > 0, 1).otherwise(0)).alias(\"temp_increasing\"),\\n    F.count(\"rt_chg_temp\").alias(\"count\"),\\n).where(F.col(\"count\") > 6).select(\\n    F.col(\"stn\"),\\n    (F.col(\"temp_increasing\") / F.col(\"count\")).alias(\"temp_increasing_ratio\"),\\n).orderBy(\\n    \"temp_increasing_ratio\"\\n).show(\\n    5, False\\n)\\n# +------+---------------------+  \\n# |stn   |temp_increasing_ratio|\\n# +------+---------------------+\\n# |681115|0.0                  |\\n# |384572|0.0                  |\\n# |682720|0.0                  |\\n# |672310|0.0                  |\\n# |654530|0.08333333333333333  |\\n# +------+---------------------+\\n# only showing top 5 rows\\n8.3.6 Going local to troubleshoot pandas UDF\\n202\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 205}), Document(page_content='non-distributed version of your data. For regular UDF, this means passing any argument of the\\n should yield an answer. As an example, our function in 8.2 took an type of values you expect\\narray of two integers: it needs to work for any arrays of two integers, including a potential zero\\nas a denominator. The same is true for any pandas UDF: you need to be lenient with the input\\nyou accept and strict with the output you provide.\\nTo test your pandas UDF, my favorite strategy is always to bring a sample of the data locally\\n(one chunk) and test my function. This way, I can play around in the REPL until I get it just\\nright, then promote it to my script. I show an example of the rate_of_change_temperature()\\nUDF, applied locally, in listing 8.20.\\nListing 8.20 Moving one station, one month worth of data in a local pandas\\nDataFrame to test my  function. rate_of_change_temperature()\\nWhen bringing a sample of your data frame into a pandas data frame for a grouped map or\\ngrouped aggregate UDF, you need to ensure you’re getting a full chunk to reproduce the results.\\nIn our specific case, since we grouped by , I brought one \"station\", \"year\", \"month\"\\nstation, one month (one specific year/month, to be precise) of data. Since the grouping of the\\ndata happens at PySpark’s level (via ), you need to think the filters for your sample groupby()\\ndata in the same fashion.\\nThis is a very quick overview of a basic strategy to confirm your code is doing what you’re\\nexpecting. In Chapter 14, I cover PySpark code testing, which includes testing UDF, both\\nPySpark and pandas.\\nUser-defined functions are probably the most powerful feature PySpark offers for data\\nmanipulation. While the standard data manipulation API provided a lot of functionality out of the\\nbox, you have the option to outgrow what’s provided and write your own functions, using\\nPython and pandas. Once written, scaling them to PySpark is as easy as decorating them. Python,\\npandas, Spark, they all work together now.\\ngsod_local = gsod.where(\"year = \\'2018\\' and mo = \\'08\\' and stn = \\'710920\\'\").toPandas()\\nprint(rate_of_change_temperature.func(gsod_local[\"da\"], gsod_local[\"temp_norm\"]))\\n# -0.007830974115511494\\n203\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 206}), Document(page_content='The most low level and flexible way of running Python code within the distributed Spark\\nenvironment is to use the resilient distributed dataset (RDD). With an RDD, you have no\\nstructure imposed on your data and need to manage type information into your program,\\nand defensively code against potential exceptions.\\nThe API for data processing on the RDD is heavily inspired by the MapReduce\\nframework. The same ideas are permeating to the data frame abstraction, which can be\\nseen as a specialized and structured RDD.\\nThe data frame’s most basic Python code promotion functionality, called the (PySpark)\\nUDF, emulates the \"map\" part of the RDD. You use it as a scalar function, taking Column\\nobjects as parameters and returning a single . Column\\nPySpark provides three UDF that leverages pandas serialization and processing (hence\\ntheir name, \"pandas UDF\"): the scalar version, which provides similar functionality to\\nthe Python UDF, the grouped map, that splits the data frame into chunks and processes\\nthem using pandas code on a DataFrame, and the grouped aggregate, which takes \\n and processes them like pandas Series, returning a scalar value. Columns\\nUsing the following definitions, create a  that takes a temp_to_temp(value, from, to)\\nnumerical  in  degrees and converts it to  degrees. value from to\\nC = (F - 32) * 5 / 9  (Celcius)\\nK = C + 273.15  (Kelvin)\\nR = F + 459.67  (Rankine)\\nCorrect the following UDF so it doesn’t generate an error.\\nModify listing 8.16 to use Celcius degrees instead of Fahrenheit. How is the result of the UDF\\ndifferent if applied to the same data frame?\\nTaking listing 8.17, what happens if we apply our grouped map UDF like so instead?8.4 Summary\\n8.5 Exercises\\n8.5.1 Exercise 8.1\\n8.5.2 Exercise 8.2\\n@F.udf(T.IntegerType())\\ndef naive_udf(t: str) -> str:\\n    ...\\n    return answer * 3.14159\\n8.5.3 Exercise 8.3\\n8.5.4 Exercise 8.4\\ngsod_exo = gsod.groupby(\"year\", \"mo\").apply(scale_temperature)\\n204\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 207}), Document(page_content='Modify listing 8.18 to return both the intercept of the linear regression, as well as the slope, in an\\n. (Hint: the intercept is in the  attribute of the fitted model.) ArrayType intercept_\\n8.5.5 Exercise 8.5\\n205\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 208}), Document(page_content='A\\nThis appendix contains the solutions to the exercises present in the book.\\nAnswer:\\nExplanation:\\nsample.csv  is the name of the file we want to ingest.\\nThe record delimiter is the comma. Since we are asked to provide a value there, I pass the\\ncomma character  explicitly, knowing it is the default one. ,\\nThe file has a header row, so I input header=True\\nThe quoting character is the dollar sign character, , so I pass it as an argument to . $ quote\\nFinally, since inferring the schema is nice, I pass  to . True inferSchema\\nAnswer:Appendix A: Solutions to the exercices\\nCHAPTER 4\\nEXERCISE 4.1\\nsample = spark.read.csv(\"sample.csv\",\\n                           sep=\",\",\\n                           header=True\",\\n                           quote=\"$\",\\n                           inferSchema=True)\\nEXERCISE 4.2\\nDIRECTORY = \"../../data/Ch04\"\\nlogs_raw = spark.read.csv(os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8.CSV\"),)\\nlogs.printSchema()\\n# root\\n#  |-- _c0: string (nullable = true)\\nlogs.show(5)\\n# +--------------------+\\n# |                 _c0|\\n# +--------------------+\\n# |BroadcastLogID|Lo...|\\n# |1196192316|3157|2...|\\n# |1196192317|3157|2...|\\n# |1196192318|3157|2...|\\n206\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 209}), Document(page_content='1.  \\n2.  Two major differences:\\nPySpark globbed everything into a single string column, since it did not encounter the\\ndefault delimiter ( ) consistently in the records. ,\\nIt named the record , the default convention when it has no information about column _c0\\nnames.\\nAnswer:\\nExplanation:\\nI use the list comprehension trick on the data frame’s columns, using the filtering clause if not\\n to keep only the columns that do not end with \"ID\". x.endswith(\"ID\")\\nAnswer:\\nc\\nExplanation:\\nBoth  and  match a columns, while  doesn’t. PySpark will ignore the item UPC prices\\nnon-existent columns passed to . drop()\\nAnswer:\\nleft\\nExplanation:\\nA  join only keep the records in left where the  value is also present in the left_semi my_column\\n column in  A  join is the opposite: it keep the recorts not present. my_column right. left_anti\\nUnioning those two together results in the original data frame, . left\\nAnswer:\\nExplanation:\\nWhen performing an inner join, all the records from the left data frame are kept in the joined data\\nframe. If the predicate in unsuccessful, then the column values from the right table are all set to\\nnull for the affected records. We just have to filter to keep only the unmatched records and then\\n# |1196192319|3157|2...|\\n# +--------------------+\\n# only showing top 5 rows\\nEXERCISE 4.3\\nlogs_clean = logs.select(*[x for x in logs.columns if not x.endswith(\"ID\")])\\nEXERCISE 4.4\\nCHAPTER 5\\nEXERCISE 5.1\\nEXERCISE 5.2\\nleft.join(right, how=\"left\",\\n          on=left[\"my_column\"] == right[\"my_column\"]).where(\\n          right[\"my_column\"].isnull()\\n          ).select(left[\"my_column\"]).\\n207\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 210}), Document(page_content='select the  column. left[\"my_column\"]\\nAnswer:\\nExplanation:\\nThis follows the same pattern as the left anti join, but with a few more tricks.\\nA left semi join is equivalent to keep only the left records that resolve the predicate. Because a\\nleft join would duplicate left records if they are matched more than once in the right table, we\\nhave to remove the potentially duplicate values.\\nWe can’t  at the end, since this would remove the duplicate values in the left table distinct()\\nthat we want to keep.\\nAnswer:\\nI will create a sample data frame before providing the answer, to see it in action.\\nExplanation:\\nI start by mapping each element to the value 1, regardless of the input. The  in the lambda _\\nfunction doesn’t bind the elements because we don’t process the element: we just care that they\\nexits. After the map operation, we have a RDD containing only the value 1. We can \\n to get the sum of all the ones, which yields the number of elements in the RDD. reduce(sum)\\nAnswer:\\na\\nExplanation:\\nFilter will drop any values when the predicate (the function passed as an argument) returns a\\nfalsey value. In Python, 0, , and empty collections are falsey. Since the predicate returns the None\\nvalue unchanged, , ,  and  are falsey and filtered out, leaving only  as the answer. 0 None [] 0.0 [1]\\nEXERCISE 5.3\\nleft.alias(\"l\").join(right.select(\"my_column\").distinct().alias(\"r\"), how=\"left\",\\n          on=F.col(\"l.my_column\") == F.col(\"r.my_column\")).where(\\n          ~F.col(\"r.my_column\").isnull()\\n          ).select(F.col(\"l.my_column\"))\\nCHAPTER 8\\nEXERCISE 8.1\\n# I assume spark is initialized and that `spark` exists as a variable\\nfrom operator import add\\nexo_rdd = spark.sparkContext.parallelize(list(range(100)))\\n# Answer below\\nexo_rdd.map(lambda _: 1).reduce(add)\\nEXERCISE 8.2\\n208\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 211}), Document(page_content='B\\nSIDEBAR Last update: 2020-06-21\\nPython 2 is now officially unsupported as of January 1st, 2020. At the\\nmoment, most OSes are in the transitional period between Python 2 and 3,\\nwhich is why I spend a little time discussing how to install Python 3. I\\nexpect this guide to become simpler as time goes.\\nI am currently targeting the most recent version of each OS as of time\\nof last update.\\nWindows 10\\nOS.X Catalina\\nGNU/Linux Ubuntu 20.04 LTS\\nThis appendix covers the installation of standalone Spark and PySpark on your own computer,\\nwhether it’s running Windows, Os.X or Linux.\\nHaving a local PySpark cluster means that you’ll be able to experiment with the syntax, using\\nsmaller data sets. You don’t have to acquire multiple computers or spend any money on managed\\nPySpark on the cloud until you’re ready to scale your programs.\\nSpark is a complex piece of software and most guides out there are over-complicating the\\ninstallation proces. We’ll take a much simpler approach by installing the bare minimum to start,\\nand building from there. Our goals are as follow:\\nInstall Java (Spark is written in Scala, which runs on the Java Virtual Machine, or JVM).\\nInstall Spark\\nInstall Python 3 and IPython\\nLaunch a PySpark shell using IPython\\n(Optional): Install Jupyter and use it with PySpark.Appendix B: Installing PySpark locally\\n209\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 212}), Document(page_content='When working on Windows, you either have the option to install Spark directly on Windows, or\\nto use WSL (Windows Subsystem for Linux). If you want to use WSL, follow the instructions at \\n and then follow the instructions for GNU/Linux. If you want to install on plain aka.ms/wslinstall\\nWindows, follow the rest of this section.\\nThe easiest way to install Java on Windows is to go on www.java.com and follow the download\\nand installation instructions for downloading Java 11. Make sure to read the installer steps to\\navoid installing non-useful software!\\nWARNING If you are installing Spark 2.4.6 or before, you will need to install Java 8\\ninstead of Java 11.\\nSpark is available as a GZIP archive ( ) file on their website. By default, windows doesn’t .tgz\\nprovide a native way to extract those files. The most popular option is 7-zip . Simply go on the8\\nwebsite, download the program and follow the installation instructions.\\nGo on the Apache website and download the latest Spark release. You shouldn’t have to change\\nthe default options, but Figure-B.1 displays the ones I see when I navigate to the download page.\\nMake sure to download the signatures and checksums if you want to validate the download (step\\n4 on the page).\\nFigure B.1 The options to download SparkB.1 Windows\\nB.1.1 Install Java\\nB.1.2 Install 7-zip\\nB.1.3 Download and install Apache Spark\\n210\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 213}), Document(page_content='Once you have downloaded the file, unzip the file using 7-zip. I recommend putting the directory\\nunder  .C:\\\\Users\\\\[YOUR_USER_NAME]\\\\spark\\nNext, we need to download a  to prevent some cryptic Hadoop errors. Go on the winutils.exe\\n repository and download the  file in the github.com/cdarlint/winutils winutils.exe\\n directory where X is the highest number. As of the time of writing, it is hadoop-2.7.X/bin\\n2.7.7. Keep the  of the repository handy. README.md\\nTIP Spark is also available with a more recent Hadoop, but it is not the default\\noption. If you select \"Pre-built for Apache Hadoop 3.2 and later\" in step 2 of\\nthe download form, take the most recent version of winutils.\\nPlace the  in the  directory of your Spark inrtallation. Then, set the winutils.exe bin\\nenvironment variables as listed on the winutils repository’s  To do so, open the start README.md\\nmenu and search for \"Edit the system environment variables\". Click on the \"Environment\\nvariables button\" (see Figure-B.2) and then add them there. You will also need to set \\n to the directory of your Spark installation ( SPARK_HOME\\n). Finally, add the  directory to C:\\\\Users\\\\[YOUR-USER-NAME]\\\\spark\\\\bin %SPARK_HOME%\\\\bin\\nyour  environment variable.PATH\\nNOTE For the  variable, you might already have some in there. If this is the PATH\\ncase, double click on the variable and append  to the %HADOOP_HOME%\\\\bin\\nlist, as well as . %SPARK_HOME%\\\\bin\\n211\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 214}), Document(page_content='Figure B.2 Setting environment variables for Hadoop on Windows\\nThe easiest way to get Python 3 is to use the Anaconda Distribution. Go to \\n and follow the installation instructions, making sure you’re www.anaconda.com/distribution\\ngetting the 64-bits Graphical installer for Python 3.X for your OS.\\nOnce Anaconda is installed, we can activate the Python 3 environment by selecting the\\n\"Anaconda Powershell Prompt\" in the start menu. If you want to create a dedicated virtual\\nenvironment for PySpark, use the following command.\\nWARNING Python 3.8 is supported only using Spark 3.0. If you use Spark 2.4.X or\\nbefore, be sure to specify Python 3.7 in you environment creation.\\nThen, to select your newly created environment, just input  in the conda activate pyspark\\nAnaconda Prompt.\\nIf you have configured the  and  variables, your Python REPL will have access SPARK_HOME PATH\\nto a local instance of pyspark. Follow the next code block to launch iPython.\\nB.1.4 Install Python\\n$ conda create -n pyspark python=3.8 pandas pyspark=3.0.0\\nB.1.5 Launching an iPython REPL and starting PySpark\\n212\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 215}), Document(page_content='TIP If you aren’t comfortable with the Command Line and Powershell, I’ve\\npersonally learned to use it using Learn Powershell in a Month of Lunches\\nby Don Jones and Jeffery D. Hicks (Manning, 2016).\\nOnly if you have created a  virtual environment. pypark\\nThen, within the REPL, you can import pyspark and get rolling.\\nNOTE Spark provides a  helper command through the  directory pyspark.cmd bin\\nof your Spark installation. I prefer accessing PySpark through a regular\\nPython REPL when working locally as I find it easier to install libraries and\\nknow exactly which Python you’re using. It also interfaces well with your\\nfavorite editor.\\nSince we have configured PySpark to be discovered from a regular Python process, we don’t\\nhave any further configuration to do to use it with a notebook. In your Anaconda Powershell\\nwindow, install jupyter using the following command.\\nYou can now run a Jupyter notebook server using the following command.\\nWith macOS, the easiest option — by far — is to use the HomeBrew  package. It apache-spark\\ntakes care of all dependencies (I still recommend using Anaconda for managing Python\\nenvironments, for simplicity).\\nHomeBrew is a package manager for OS.X. It provides a simple command line interface to\\ninstall many popular software packages and keep them up to date. While you can follow the\\nmanual \"download and install\" steps you’ll find on the Windows OS with little change,\\nHomebrew will simplify our installation process to a few commands.B.2 macOS\\nconda activate pyspark \\nipython\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\nB.1.6 (Optional) Install and run Jupyter to use Jupyter notebook\\nconda install -c conda-forge notebook\\njupyter notebook\\nB.2.1 Install Homebrew\\n213\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 216}), Document(page_content='To install Homebrew, go to  and follow the installation instructions. You’ll be able to brew.sh\\ninteract with Homebrew through the  command. brew\\nInput the following command in a terminal.\\nYou can specify the version you want; I recommend getting the latest by passing no parameters.\\nThe easiest way to get Python 3 is to use the Anaconda Distribution. Go to \\n and follow the installation instructions, making sure you’re www.anaconda.com/distribution\\ngetting the 64-bits Graphical installer for Python 3.X for your OS.\\nIf it’s your first time using Anaconda, follow the instructions to register your shell.\\nWARNING Python 3.8 is supported only using Spark 3.0. If you use Spark 2.4.X or\\nbefore, be sure to specify Python 3.7 in you environment creation.\\nThen, to select your newly created environment, just input  in the conda activate pyspark\\nAnaconda Prompt.\\nHomebrew should have the  and  environment variables, so your Python REPL SPARK_HOME PATH\\nwill have access to a local instance of pyspark. You just have to type the following.\\nOnly if you have created a  virtual environment. pypark\\nThen, within the REPL, you can import pyspark and get rolling.\\nB.2.2 Install Java and Spark\\n$ brew install apache-spark\\nB.2.3 Install Anaconda/Python\\n$ conda create -n pyspark python=3.8 pandas pyspark=3.0.0\\nB.2.4 Launching a iPython REPL and starting PySpark\\nconda activate pyspark \\nipython\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\n214\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 217}), Document(page_content='Since we have configured PySpark to be discovered from a regular Python process, we don’t\\nhave any further configuration to do to use it with a notebook. In your Anaconda Powershell\\nwindow, install jupyter using the following command.\\nYou can now run a Jupyter notebook server using the following command.\\nMost GNU/Linux distributions provide a package manager. OpenJDK version 11 is available\\nthrough the software repository.\\nWARNING If you want to install a version of Spark prior to 3.0.0, install openjdk-8-jre\\ninstead.\\nGo on the Apache website and download the latest Spark release. You shouldn’t have to change\\nthe default options, but Figure-B.1 displays the ones I see when I navigate to the download page.\\nMake sure to download the signatures and checksums if you want to validate the download (step\\n4 on the page).\\nTIP On WSL (and sometimes Linux), you don’t have a graphical user interface\\nreally available. The easiest way to download Spark is to go on the\\nwebsite, follow the line, copy the link of the nearest mirror and past it along\\nwith  command.wget\\nIf you want to know more about using the command line on Linux (and\\nOs.X) proficiently, a good free reference is  by The Linux Command Line\\nWilliam Shotts . It is also available as a paper or e-book (No Starch Press,9\\n2019).\\nOnce you have downloaded the file, unzip the file (using 7-zip on Windows). If you are using theB.3 GNU/Linux and WSL\\nB.2.5 (Optional) Install and run Jupyter to use Jupyter notebook\\nconda install -c conda-forge notebook\\njupyter notebook\\nB.3.1 Install Java\\n`sudo apt-get install openjdk-11-jre`\\nB.3.2 Installing Spark\\nwget [YOUR_PASTED_DOWNLOAD_URL]\\n215\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 218}), Document(page_content='command line, the following command will do the trick. Make sure you’re replacing the \\n by the name of the file you just downloaded. spark-[…].gz\\nThis will unzip the content of the archive into a directory. You can now rename and move the\\ndirectory to your liking. I usually put it under /home/[MY-USER-NAME]/bin/spark-3.0.0/\\n(and rename if the name is not identical) and the instructions will use that directory.\\nSet the following environment variables.\\nPython 3 is already provided, you just have to install IPython. Input the following command in a\\nterminal.\\nTIP You can also use Anaconda on GNU/Linux! Follow the instructions on the\\nmacOS section.\\nLaunch an iPython shell.\\nThen, within the REPL, you can import pyspark and get rolling.\\nSince we have configured PySpark to be discovered from a regular Python process, we don’t\\nhave any further configuration to do to use it with a notebook. In your terminal, input the\\nfollowing to install jupyter.\\nYou can now run a Jupyter notebook server using the following command.\\ntar -xvzf spark-[...].gz\\necho \\'export SPARK_HOME=\"$HOME/bin/spark-3.0.0\"\\' >> ~/.bashrc\\nB.3.3 Install Python 3 and IPython\\nsudo apt-get install ipython3\\nB.3.4 Launch PySpark with IPython\\nipython\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\nB.3.5 (Optional) Install and run Jupyter to use Jupyter notebook\\npip install notebook\\njupyter notebook\\n216\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 219}), Document(page_content='1.mIt can be a fun probability exercise to compute by how much, but I will try to keep the math stuff at a\\nminimum.\\n2.mApplication Programming Interface, which is basically the set of functions, classes and variables provided\\nfor you to interact with\\n3.mJava Virtual Machine, which is like an emulator running on your computer. Both Java and Scala targets the\\nJVM.\\n4.mIt does actually a whole lot more, but we will cover other aspects in Chapter 7.\\n5.mwriting a program using the lowest possible number of characters.\\n6.mIn Quebecois, we say \"s’enfarger dans les fleurs du tapis\" to talk of someone who’s too bogged down on the\\ndetails. Transliterated, it would be \"to trip over the rug’s flowers\".\\n7.mOn windows, you might sometimes have issues with the pip wheels. If this is your case, refer to the PyArrow\\ndocumentation page for installing: arrow.apache.org/docs/python/install.html\\n8.mwww.7-zip.org/\\n9.mlinuxcommand.org/Notes\\n217\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion', metadata={'source': 'pdfs\\\\Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf', 'page': 220}), Document(page_content='Ong Aun Jie \\nNo 1, Jalan 14/36, 46100 Petaling Jaya, Selangor \\nMobile: 018- 235 -3383  \\nlinkedin.com/in/ongaunjie  •  ongaunjie1@gmail.com  •  github.com/ongaunjie1 •  ongaunjie.com  \\n \\nAs a former manufacturing engineer, I am currently transitioning into data s cience through self-directed learning. With a solid                         \\nfoundation in lean manufacturing and kaizen, I bring two years of experienc e, showcasing strong problem-solving abilities                       \\nand a continuous improvement mindset. Through honed analytical skills and a rob ust foundation in data science, I am eager to \\nleverage my engineering background to derive data-driven insights and contribute  meaningfully to the field. I have successfully \\ncompleted data science projects demonstrating my ability to extract valuable ins ights from complex datasets. My unique blend                  \\nof engineering expertise and data science skills positions me as a valuab le asset for driving data-driven decision making in both    \\ndiverse and dynamic environments.   \\nTECHNICAL SKILLS                                                                                                                                                                                         \\n• Programming Languages:  Python , SQL  (Postgres, Oracle SQL), HTML, Tailwind CSS, React.js \\n• Data Analysis and Manipulation:  Pandas, NumPy, Regex , Microsoft Excel (Pivot table, XLOOKUP, Power Query and etc.) \\n• Data Visualization: PowerBI (including DAX) , Matplotlib, Seaborn, Plotly \\n• Machine learning and Deep Learning:  Scikit-learn, PyTorch, OpenCV, HuggingFace, YOLOv8, RoboFlow , MLflow, PyCaret \\n• Natural Language Processing:  NLTK, BERT models (Text Classification, Sentiment Analysis, Question Answering) \\n• Data Collection:  BeautifulSoup, Selenium, Scrapy, API Data Retrieval, Requests \\n• Version Control and Tracking:  Git, GitHub ,  \\n• Cloud Knowledge:  AWS(Athena, SageMaker, S3, Redshift) , Microsoft Azure (Blob storage, AzureML), Datab ricks \\n• Others:  Docker, RESTful API using Flask, Streamlit \\n               PROFESSIONAL  EXPERIENCE                                                                                                                                                               \\n               Career Transition into Data Science   • Present                                                                                                Peta ling Jaya , Malaysia \\n               Self-taught                                                                                                                                                                                Mar  2023 – Present    \\n               During this period, I’ve proactively transition ed into data science through self-study, online courses and hands-on projects.  \\n• Developed a strong foundation in Python, emphasizing data structures, algorithms, and eff icient problem-solving  \\ntechniques. \\n• Attained data analytics certifications covering data cleaning, manipulation, visual ization, exploratory data analysis, A/B  \\ntesting, statistical analysis, and machine learning. \\n• Explored machine learning algorithms such as XGBoost, Random Forest and more for classification and regression tasks,  \\nwith a focus on hyperparameter tuning using RandomizedSearchCV and GridSearchC V. \\n• Gained proficiency in deep learning applications using PyTorch for tasks like  object detection and image classification. \\n• Created web applications for machine learning and deep learning models us ing Streamlit. \\n• Strengthened skills in PowerBI and SQL through practical application. \\n• Learned diverse data collection techniques including Web Scraping and API data retrieval. \\n• Applied AWS and Microsoft Azure for ML model deployment, SQL queries, and Cloud S torage. \\n• Explored Natural Language Processing, covering text classification, sentiment an alysis, and chatbot development.', metadata={'source': 'pdfs\\\\OngAJ resume.pdf', 'page': 0}), Document(page_content='• Explored Natural Language Processing, covering text classification, sentiment an alysis, and chatbot development. \\n• Developed a personal portfolio website using HTML, TailwindCSS, and React.js, integrated wi th a fully functional chatbot \\ncreated with Deep learning and Natural Language Processing Techniques. \\n• Please check out my portfolio website where I share my data science projects : ongaunjie.com   \\n              NXP Malaysia Sdn Bhd  •  6 months                                                                                                                       Petaling Jaya , Malaysia  \\n               Wire-Bond Equipment Engineer (Front-end BGA)                                                                                                       Sept 2022 – Feb 2023  \\n• Responsible for managing K&S Wire Bonder machines and Post Wire Bond Inspection (AOI) machines, overseeing their \\noperation, maintenance, and performance optimization. \\n• Skilled in investigating quality incidents, implementing corrective and preventive ac tions (CAPA), and ensuring adherence  \\nto equipment-related documentation standards. \\n• Experienced in utilizing root-cause analysis methods like Fishbone diagram, 5 Whys, F ault-Tree Analysis, Pareto Chart, \\nand Failure Mode Effect Analysis (FMEA) to address machine-related issues. \\n• Collaborates with machine vendors to enhance machine performance and efficie ncy. \\n• Utilizes data analytics tools such as Excel, PowerBI and SQL to monitor machine performa nce, analyze data, and create  \\ndashboards for real-time monitoring and performance improvements. \\n✓ Improved Mean Time Between Assists (MTBA) by 8% after implementing track ing through dashboards,  \\nenabling proactive maintenance and reducing machine downtime. \\n✓ Successfully reduced overall in-line quality issues by utilizing a PowerBI dashboard for monitoring machine  \\nperformance and promptly address any deviations or anomalies. \\n✓ Effectively employed the dashboard to track and manage Critical Defect Managemen t System (CDMS), resulting  \\nin a 13% reduction in CDMS and improved product quality. \\n• Presented data analysis findings to the quality team and department head to drive in formed decision making.', metadata={'source': 'pdfs\\\\OngAJ resume.pdf', 'page': 0}), Document(page_content=\"Daikin Malaysia Sdn Bhd  •  1 year 6 months                                                                                                       Sungai Buloh, Malaysia                       \\n                Production Engineer  (Light commercial AC – Indoor unit)                                                                                       Oct 20 20 – Mar 2022  \\n• Planned production capacity, schedule, and oversaw execution to meet customer delivery r equirements. \\n• Managed a team of supervisors, foremen, lead operators, and operators to su pport production flow. \\n• Analysed and planned workforce utilization, workflow, space requirements,  and equipment layout to improve  \\nmanufacturing efficiency and capacity. \\n• Conducted root cause analysis and provided corrective & preventive actions for pr oduct quality issues. \\n• Collaborated with R&D, QC, Material Control, Maintenance, and Production Enginee ring departments to enhance  \\nproduct quality, optimize facilities, and improve production processes. \\n• Implemented Poka Yoke (Mistake Proofing) measures to minimize product defects and imp rove process control. \\n• Led continuous production and process improvement (KAIZEN) projects to achieve optimal performance in yield \\nand productivity. \\n• Executed a KAIZEN project which resulted in a reduction of one manpower, save d cost by eliminating the need for an  \\nadditional employee. \\n• Involved in various New Product Introduction (NPI) projects, including the implementation of Daikin M alaysia's First  \\nEver Heat Reclaim Ventilator (VAM) into existing production lines. \\n• Created and maintained documents for work indication, safety work indication, an d process checkpoints to  \\nminimize errors and accidents. \\n                Soon Ngai Engineering Sdn Bhd  •  3 months                                                                                                    Seri Kembangan, Malaysia \\n                Engineering Intern                                                                                                                                                                  May 2019 – Aug 2019  \\n• Involved in the design and implementation of water treatment systems, including DI (Deionize d) system,  \\nRO (Reverse Osmosis) system, and media filtration system. \\n• Participated in documentation works for various manufacturing industries, including Texas Instrument Malaysia,  \\nMewah Oil Sdn Bhd, and Sunpower Malaysia. \\n• Collaborated with project engineers in preparing and submitting environmen tal quality documents to the  \\nDepartment of Environmental (DOE). \\n• Worked closely with purchasing and site personnel to prepare quotations for projects. \\n• Utilized 2D AutoCAD to create multiple drawings for clients. \\n• Attended an engineering workshop on the Fundamentals of Online Oxygen Measure ment provided by Mettler Toledo. \\nEDUCATION                                                                                                                                                                               \\nHeriot-Watt University                                                                                                                                                     Putrajaya,  Malaysia  \\nMaster of Engineering in Chemical Engineering with Oil and Gas  (MEng)                                                               Jan 2017 – July 2020                        \\n• 2:1 Upper Second-Class Honors with CGPA – 3.5/4.0 \\n• Thesis title: Recovery of Starch Residues from Sago effluent using Natural Coagulants \\nSOFT SKILLS                                                                                                                                                                                                                 \\nAnalytical Thinking:  Strong analytical skills, critical thinking, and p roblem-solving abilities\", metadata={'source': 'pdfs\\\\OngAJ resume.pdf', 'page': 1}), Document(page_content='Analytical Thinking:  Strong analytical skills, critical thinking, and p roblem-solving abilities  \\nAttention to Detail : Ability to pay close attention to details, ensuring accuracy and precision in data ana lysis. \\nCuriosity and Continuous Learning : Eagerness to learn and stay updated with the latest industry trends and te chnologies,  \\nCommunication: Effective communicator, ability to present complex ideas to both technical and non-technical audience \\nCollaboration: Teamwork and collaboration skills, ability to work effectively in cross-functional teams \\nTime Management: Strong organizational and time management skills to  handle multiple tasks and meet deadlines \\nCERTIFICATIONS & AWARDS            \\nGoogle Advanced Data Analytics Professional Certificate  | Coursera                                                                 May 2023   \\nDP-100: A-Z Machine Learning using Azure Machine Learning  | Udemy                                                           July 2023 \\nPractical Data Science on the AWS Cloud Specialization  |   Coursera                                                                   Aug 2023 \\nComplete Machine Learning & Data Science Bootcamp 2023  |  Udemy                                                             Sept 2023 \\nThe Complete SQL Bootcamp: Go from Zero to Hero | Udemy                                                                               Sept  2023 \\nYOLOv8: Object Detection, Tracking & Web App in Python | Udemy                                                                    Oct  2023 \\nMicrosoft PowerBI Desktop for Business Intelligence  | Udemy                                                                            Nov 2023 \\nPython Data Structures & Algorithms | Udemy                                                                                                              Dec 2023 \\nDuke of Edinburgh’s Award (Bronze, Silver, Gold)  | DofE                                                                                    2013- 2015  \\nADDITIONAL  INFORMATION                                                                                                                                                                          \\nLanguages:  Fluent  in English,  Mandarin, Bahasa Melayu | Limited working proficien cy in Hokkien, Cantonese \\nInterests:  Hiking, Competitive sports & Competitive gaming', metadata={'source': 'pdfs\\\\OngAJ resume.pdf', 'page': 1})]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "©Manning Publications Co.  To comment go to  liveBook welcome  \n",
      "Thank you for purchasing the MEAP for PySpark in Action: Python data analysis at scale.  \n",
      "It is a lot of fun (and work!) and I hope you’ll enjoy reading it as much as I am enjoying \n",
      "writing the book. \n",
      "My journey with PySpark is pretty typical: the company I used to work for migrated their \n",
      "data infrastructure to a data lake and realized along the way that their usual warehouse-type \n",
      "jobs didn’t work so well anymore. I spent most of my first months there figuring out how to \n",
      "make PySpark work for my colleagues and myself, starting from zero. This book is very \n",
      "influenced by the questions I got from my colleagues and students (and sometimes myself). \n",
      "I’ve found that combining practical experience through real examples with a little bit of \n",
      "theory brings not only proficiency in using PySpark, but also how to build better data \n",
      "programs. This book walks the line between the two by explaining important theoretical \n",
      "concepts without being too laborious. \n",
      "This book covers a wide range of subjects, since PySpark is itself a very versatile \n",
      "platform. I divided the book into three parts. \n",
      "• Part 1: Walk teaches how PySpark works and how to get started and perform basic \n",
      "data manipulation. \n",
      "• Part 2: Jog builds on the material contained in Part 1 and goes through more advanced \n",
      "subjects. It covers more exotic operations and data formats and explains more what \n",
      "goes on under the hood. \n",
      "• Part 3: Run tackles the cooler  stuff: building machine learning models at scale, \n",
      "squeezing more performance out of your cluster, and adding functionality to PySpark. \n",
      "To have the best time possible with the book, you should be at least comfortable using \n",
      "Python. It isn’t enough to have learned another language and transfer your knowledge into \n",
      "Python. I cover more niche corners of the language when appropriate, but you’ll need to \n",
      "do some research on your own if you are new to Python. \n",
      "Furthermore, this book covers how PySpark can interact with other data manipulation \n",
      "frameworks (such as Pandas), and those specific sections assume basic knowledge of \n",
      "Pandas.  \n",
      "Finally, for some subjects in Part 3, such as machine learning, having prior exposure will \n",
      "help you breeze through. It’s hard to strike a balance between “not enough explanation” and \n",
      "“too much explanation”; I do my best to make the right choices. \n",
      "Your feedback is key in making this book its best version possible. I welcome your \n",
      "comments and thoughts in the liveBook discussion forum . \n",
      "Thank you again for your interest and in purchasing the MEAP! \n",
      " \n",
      "—Jonathan Rioux\n"
     ]
    }
   ],
   "source": [
    "print(data[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words in the context: 451293\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\".join(str(p.page_content) for p in data)\n",
    "print(\"The total number of words in the context:\", len(context))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Extracted Data into Text Chunks\n",
    "\n",
    "* Chunk Size (e.g., 10000 letters): The maximum length of each small piece you cut from the string.\n",
    "\n",
    "* Chunk Overlap (e.g., 200 letters): Some of the letters at the end of one piece are also at the beginning of the next piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'©Manning Publications Co.  To comment go to  liveBook \\nMEAP Edition \\nManning Early Access Program \\nPySpark in Action \\nPython data analysis at scale \\nVersion 6 \\nCopyright 2020 Manning Publications \\nFor more information on this and other Manning titles go to \\nmanning.com\\n\\n©Manning Publications Co.  To comment go to  liveBook welcome  \\nThank you for purchasing the MEAP for PySpark in Action: Python data analysis at scale.  \\nIt is a lot of fun (and work!) and I hope you’ll enjoy reading it as much as I am enjoying \\nwriting the book. \\nMy journey with PySpark is pretty typical: the company I used to work for migrated their \\ndata infrastructure to a data lake and realized along the way that their usual warehouse-type \\njobs didn’t work so well anymore. I spent most of my first months there figuring out how to \\nmake PySpark work for my colleagues and myself, starting from zero. This book is very \\ninfluenced by the questions I got from my colleagues and students (and sometimes myself). \\nI’ve found that combining practical experience through real examples with a little bit of \\ntheory brings not only proficiency in using PySpark, but also how to build better data \\nprograms. This book walks the line between the two by explaining important theoretical \\nconcepts without being too laborious. \\nThis book covers a wide range of subjects, since PySpark is itself a very versatile \\nplatform. I divided the book into three parts. \\n• Part 1: Walk teaches how PySpark works and how to get started and perform basic \\ndata manipulation. \\n• Part 2: Jog builds on the material contained in Part 1 and goes through more advanced \\nsubjects. It covers more exotic operations and data formats and explains more what \\ngoes on under the hood. \\n• Part 3: Run tackles the cooler  stuff: building machine learning models at scale, \\nsqueezing more performance out of your cluster, and adding functionality to PySpark. \\nTo have the best time possible with the book, you should be at least comfortable using \\nPython. It isn’t enough to have learned another language and transfer your knowledge into \\nPython. I cover more niche corners of the language when appropriate, but you’ll need to \\ndo some research on your own if you are new to Python. \\nFurthermore, this book covers how PySpark can interact with other data manipulation \\nframeworks (such as Pandas), and those specific sections assume basic knowledge of \\nPandas.  \\nFinally, for some subjects in Part 3, such as machine learning, having prior exposure will \\nhelp you breeze through. It’s hard to strike a balance between “not enough explanation” and \\n“too much explanation”; I do my best to make the right choices. \\nYour feedback is key in making this book its best version possible. I welcome your \\ncomments and thoughts in the liveBook discussion forum . \\nThank you again for your interest and in purchasing the MEAP! \\n \\n—Jonathan Rioux\\n\\n©Manning Publications Co.  To comment go to  liveBook brief contents \\nPART 1:  WALK \\n  1  Introduction \\n  2  Your first data program in PySpark \\n  3  Submitting and scaling your first PySpark program \\n  4  Analyzing tabular data with pyspark.sql \\n  5  The data frame through a new lens: joining and grouping \\nPART 2:  JOG \\n  6  Multi-dimentional data frames: using PySpark with JSON data \\n  7  Bilingual PySpark: blending Python and SQL code \\n  8  Extending PySpark with Python: RDD and user-defined-functions \\n  9  Faster big data processing: a primer \\nPART 3:  RUN \\n10  A foray into machine learning: logistic regression with PySpark \\n11  Simplifying your experiments with machine learning pipelines \\n12  Machine learning for unstructured data \\n13  PySpark for graphs: GraphFrames \\n14  Testing PySpark code \\n15  Going full circle: structuring end-to-end PySpark code \\nAPPENDIXES : \\n A  Solutions to the exercices \\n B  Installing PySpark locally \\n C  Using PySpark with a cloud provider \\n D  Python essentials \\n E  PySpark data types \\n F  Efficiently using PySpark’s API documentation\\n\\n1\\nIn this chapter, you will learn\\nAccording to pretty much every news outlet, data is everything, everywhere. It’s the new oil, the\\nnew electricity, the new gold, plutonium, even bacon! We call it powerful, intangible, precious,\\ndangerous. I prefer calling it . After all, for a computer, any piece of data useful in capable hands\\nis a collection of zeroes and ones, and it is our responsibility, as users, to make sense of how it\\ntranslates to something useful.\\nJust like oil, electricity, gold, plutonium and bacon (especially bacon!), our appetite for data is\\ngrowing. So much, in fact, that computers aren’t following. Data is growing in size and\\ncomplexity, yet consumer hardware has been stalling a little. RAM is hovering for most laptops\\nat around 8 to 16 GB, and SSD are getting prohibitively expensive past a few terabytes. Is the\\nsolution for the burgeoning data analyst to triple-mortgage his life to afford top of the line\\nhardware to tackle Big Data problems?\\nIntroducing Spark, and its companion PySpark, the unsung heroes of large-scale analytical\\nworkloads. They take a few pages of the supercomputer playbook— powerful, but manageable,\\ncompute units meshed in a network of machines— and bring it to the masses. Add on top a\\npowerful set of data structures ready for any work you’re willing to throw at them, and you have\\na tool that will  (pun intended) with you. grow\\nThis book is great introduction to data manipulation and analysis using PySpark. It tries to coverIntroduction\\nWhat is PySpark\\nWhy PySpark is a useful tool for analytics\\nThe versatility of the Spark platform and its limitations\\nPySpark’s way of processing data\\n1\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion\\n\\njust enough theory to get you comfortable, while giving enough opportunities to practice. Each\\nchapter except this one contains a few exercices to anchor what you just learned. The exercises\\nare all solved and explained in Appendix A.\\nWhat’s in a name?  Actually, quite a lot. Just by separating PySpark in two, one can already\\ndeduce that this will be related to Spark and Python. And it would be right!\\nAt the core, PySpark can be summarized as being the Python API to Spark. While this is an\\naccurate definition, it doesn’t give much unless you know the meaning of Python and Spark. If\\nwe were in a video game, I certainly wouldn’t win any prize for being the most useful NPC.\\nLet’s continue our quest to understand what is PySpark by first answering . What is Spark?\\nSpark, according to their authors, is a . unified analytics engine for large-scale data processing\\nThis is a very accurate definition, if a little dry.\\nDigging a little deeper, we can compare Spark to an . The raw material— here, analytics factory\\ndata— comes in, and data, insights, visualizations, models, you name it! comes out.\\nJust like a factory will often gain more capacity by increasing its footprint, Spark can process an\\nincreasingly vast amount of data by  instead of . This means that, instead of scaling out scaling up\\nbuying thousand of dollars of RAM to accommodate your data set, you’ll rely instead of multiple\\ncomputers, splitting the job between them. In a world where two modest computers are less\\ncostly than one large one, it means that scaling out is less expensive than up, keeping more\\nmoney in your pockets.\\nThe problem with computers is that they crash or behave unpredictably once in a while. If\\ninstead of one, you have a hundred, the chance that at least one of them go down is now much\\nhigher.  Spark goes therefore through a lot of hoops to manage, scale, and babysit those poor1\\nlittle sometimes unstable computers so you can focus on what you want, which is to work with\\ndata.\\nThis is, in fact, one of the weird thing about Spark: it’s a good tool because of what you can do\\nwith it, but especially because of what you  with it. Spark provides a powerful don’t have to do\\nAPI  that makes it look like you’re working with a cohesive, non-distributed source of data,2\\nwhile working hard in the background to optimize your program to use all the power available.\\nYou therefore don’t have to be an expert at the arcane art of distributed computing: you just need\\nto be familiar with the language you’ll use to build your program. This leads us to…\\u200b1.1 What is PySpark?\\n1.1.1 You saw it coming: What is Spark?\\n2\\n©Manning Publications Co.  To comment go to liveBook \\nhttps://livebook.manning.com/#!/book/pyspark-in-action/discussion'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download google embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings for each of the text chunks and save them into a vector store (chromadb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Chroma.from_texts(texts, embeddings).as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "  Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "  provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "  Context:\\n {context}?\\n\n",
    "  Question: \\n{question}\\n\n",
    "\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The stuff documents chain (“stuff” as in “to stuff” or “to fill”) is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes    that prompt to an LLM.\n",
    "\n",
    "* This chain is well-suited for applications where documents are small and only a few are passed in for most calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is his education background\"\n",
    "docs = vector_index.get_relevant_documents(question)\n",
    "response = chain(\n",
    "    {\"input_documents\":docs, \"question\": question}\n",
    "    , return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'Master of Engineering in Chemical Engineering with Oil and Gas (MEng) from Heriot-Watt University, Putrajaya, Malaysia'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
